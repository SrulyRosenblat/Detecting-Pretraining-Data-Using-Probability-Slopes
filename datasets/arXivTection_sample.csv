,ID,Example_A,Example_B,Example_C,Example_D,Answer,Label
536,Non-autoregressive Text Editing with Copy-aware Latent Alignments,"To overcome the above deficiency, recently, there is a growing interest in an alternative approach, referred to as Seq2Edit (Awasthi et al., 2019; Omelianchuk et al., 2020; Mallinson et al., 2020), which, in contrast, proposes to reconstruct the target sentence by applying a set of edit operations, e.g., keep, deletion and insertion, to the input. Drawing on the insight that the input/output tokens are heavily shared, Seq2Edit favors copying most of the source text directly via the keep operation, which eases the reliance for an autoregressive decoder (Malmi et al., 2019; Mallinson et al., 2022). Among others, the best-performing GECToR (Omelianchuk et al., 2020, 2021) directly formulates text-editing as a non-autoregressive sequence tagging task, thus enabling more efficient parallelizable inference. ","Recently, there has been increased interest in a new way of handling this shortcoming, called Seq2Edit (Awasthi et al., 2019; Omelianchuk et al., 2020; Mallinson et al., 2020). Unlike previous methods, it aims to recreate the target sentence by making edits like keeping, deleting or inserting words from the original input. Since the input and output share many of the same tokens, Seq2Edit prefers directly copying most of the source text through keeping words, reducing the need for a slow step-by-step decoder (Malmi et al., 2019; Mallinson et al., 2022). One top approach, GECToR (Omelianchuk et al., 2020, 2021), models text editing directly as a parallelizable sequence labeling task, enabling faster inference.","Recently, an alternative approach called Seq2Edit (Awasthi et al., 2019; Omelianchuk et al., 2020; Mallinson et al., 2020) has garnered interest to address the above issue. In contrast to previous methods, it reconstructs the target sentence by performing edit operations like keeping, removing or adding words from the input. Seq2Edit capitalizes on the insight that input and output tokens heavily overlap by directly copying most source words through keeping, reducing reliance on slow autoregressive decoding (Malmi et al., 2019; Mallinson et al., 2022). GECToR (Omelianchuk et al., 2020, 2021), a top model, formulates text editing as a parallel sequence tagging task, allowing faster inference.","There has been growing interest lately in an alternative technique called Seq2Edit (Awasthi et al., 2019; Omelianchuk et al., 2020; Mallinson et al., 2020) to mitigate the aforementioned deficiency. Unlike previous approaches, it aims to recreate the target sentence by applying edits like retaining, omitting or inserting words from the input. Seq2Edit leverages the observation that input and output tokens substantially coincide by directly duplicating most source text via retaining words, lessening the need for slow step-by-step decoding (Malmi et al., 2019; Mallinson et al., 2022). One leading model, GECToR (Omelianchuk et al., 2020, 2021), poses text editing as a parallelizable sequence tagging problem, enabling quicker inference.",A,0
39,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Finally, to verify the effectiveness of the proposed method, we carry out extensive experiments on different benchmark datasets. The empirical studies show the superiority of our method, which achieves effective performance gains compared to several competitive baseline methods. Overall, the main contributions can be summarized as follows: (1) Compared with existing metric-based methods, we propose a more efficient and effective end-to-end scheme for zero-shot slot filling, and show our soft label embeddings perform much better than previous commonly-used static label representations. (2) We investigate the slot level contrastive learning to effectively improve generalization capacity for zero-shot slot filling. (3) By extensive experiments, we demonstrate the benefits of our model in comparison to the existing metric-based methods, and provide an insightful quantitative and qualitative analysis.","Finally, to confirm the effectiveness of the suggested approach, we conduct comprehensive experiments on various benchmark datasets. The empirical studies demonstrate the superiority of our method, which accomplishes effective performance improvements compared to several competitive baseline methods. Overall, the main contributions can be summarized as follows: (1) Compared to existing metric-based methods, we propose a more efficient and effective end-to-end scheme for zero-shot slot filling, and show our soft label embeddings perform much better than previous commonly-used static label representations. (2) We investigate the slot level contrastive learning to effectively enhance generalization capacity for zero-shot slot filling. (3) Through extensive experiments, we exhibit the benefits of our model in comparison to the existing metric-based methods, and provide an insightful quantitative and qualitative analysis.","In conclusion, to validate the efficacy of the proposed approach, we perform extensive experiments on various benchmark datasets. The empirical evaluations exhibit the superiority of our method, which achieves effective performance gains compared to several competitive baseline methods. In summary, the main contributions can be outlined as follows: (1) Compared to existing metric-based methods, we propose a more efficient and effective end-to-end framework for zero-shot slot filling, and demonstrate our soft label embeddings perform much better than previous commonly-used static label representations. (2) We explore the slot level contrastive learning to effectively boost generalization capacity for zero-shot slot filling. (3) Through comprehensive experiments, we showcase the advantages of our model compared to the existing metric-based methods, and provide an insightful quantitative and qualitative analysis.","To summarize, to confirm the effectiveness of the proposed approach, we undertake extensive experiments on various benchmark datasets. The empirical evaluations exhibit the superiority of our method, which accomplishes effective performance improvements compared to several competitive baseline methods. In essence, the main contributions can be outlined as follows: (1) In contrast to existing metric-based methods, we propose a more efficient and effective end-to-end framework for zero-shot slot filling, and demonstrate our soft label embeddings perform much better than previous commonly-used static label representations. (2) We investigate the slot level contrastive learning to effectively enhance generalization capacity for zero-shot slot filling. (3) Via extensive experiments, we highlight the benefits of our model compared to the existing metric-based methods, and provide an insightful quantitative and qualitative analysis.",A,0
211,Counting the Bugs in ChatGPTs Wugs A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,"The morphological generator was set up to generate only Turkish word forms that corresponded to the selected inflectional morpheme combinations we selected, for all applicable roots. For testing, we expected the baseline systems to generate the word forms with the selected inflectional feature combinations, but for 10 nonce roots. The nonce roots were chosen so that they would force the inflected forms to orthogonally adhere to surface morphographemic constraints and rules such as various types of vowel harmony, consonant elision, or assimilation at morpheme boundaries.","The word form generator was configured to produce only Turkish word forms matching the inflectional affix combinations we chose, for all relevant stems. For evaluation, we anticipated the baseline systems to generate the word forms having the selected inflectional traits, but for 10 made-up roots. The made-up roots were selected so they would obligate the inflected forms to orthogonally follow surface morphographic limitations and principles like various vowel harmony types, consonant omission, or assimilation at morpheme borders.","The morphological generator was set up to create only Turkish word forms that matched the inflectional affix mixes we picked, for all suitable bases. For testing, we expected the baseline systems to build the word forms with the chosen inflectional qualities, but for 10 nonsense roots. The nonsense roots were selected so they would force the inflected forms to perpendicularly follow surface spelling constraints and guidelines like various vowel harmony kinds, consonant deletion, or assimilation at morpheme junctions. ","The word generator was configured to produce only Turkish word forms corresponding to the inflectional suffix combinations we selected, for all relevant roots. For evaluation, we anticipated the baseline systems to form the word forms having the chosen inflectional attributes, but for 10 artificial roots. The artificial roots were chosen so they would compel the inflected forms to orthogonally conform to surface orthographic limits and principles like various vowel harmony types, consonant omission, or assimilation at morpheme boundaries.",A,0
199,Counting the Bugs in ChatGPTs Wugs A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,"The English past tense has a long and storied history in computational studies of morphology (Rumelhart and McClelland, 1986a; Pinker and Prince, 1988; Ullman et al., 1997; Plunkett and Juola, 1999; Albright and Hayes, 2002, 2003; Kirov and Cotterell, 2018; Ma and Gao, 2022). English displays a handful of conjugation classes as well as frequent morphographemic alternations— consonant doubling and e-deletion, for example— affecting past forms of verbs. To create the English data, 50 two- to five-letter irregular verbs (defined as verbs that do not form the past tense simply by adding -ed) were sampled from the UniMorph 4.0 dataset (Batsuren et al., 2022).","The history of the English past tense has been extensively studied in computational morphology over the years (Rumelhart and McClelland, 1986a; Pinker and Prince, 1988; Ullman et al., 1997; Plunkett and Juola, 1999; Albright and Hayes, 2002, 2003; Kirov and Cotterell, 2018; Ma and Gao, 2022). English exhibits a few conjugation patterns and frequent morphographemic changes - like consonant doubling and e deletion - that affect past tense forms of verbs. To generate the English data, 50 irregular two- to five-letter verbs (verbs that don't form the past by just adding -ed) were taken from the UniMorph 4.0 dataset (Batsuren et al., 2022).","Researchers have long been fascinated by the English past tense and its complexities, as evidenced by the many computational morphology studies over the past decades (Rumelhart and McClelland, 1986a; Pinker and Prince, 1988; Ullman et al., 1997; Plunkett and Juola, 1999; Albright and Hayes, 2002, 2003; Kirov and Cotterell, 2018; Ma and Gao, 2022). The English past tense system displays a small number of conjugation patterns, as well as common morphographemic changes like consonant doubling and e deletion that affect past tense verb forms. To create the dataset, 50 irregular two- to five-letter verbs (verbs that form the past tense in ways other than just adding -ed) were selected from the UniMorph 4.0 resource (Batsuren et al., 2022).","The English past tense has been extensively analyzed by computational linguists over the years, with many studies investigating its complex morphology (Rumelhart and McClelland, 1986a; Pinker and Prince, 1988; Ullman et al., 1997; Plunkett and Juola, 1999; Albright and Hayes, 2002, 2003; Kirov and Cotterell, 2018; Ma and Gao, 2022). English past tense formation displays a small set of verb conjugation classes, along with common morphographemic alternations like consonant doubling and e deletion that affect past tense forms. To generate the data, 50 irregular two- to five-letter verbs (verbs not forming the past simply by adding -ed) were extracted from the UniMorph 4.0 dataset (Batsuren et al., 2022).",A,0
235,Cultural Concept Adaptation on Multimodal Reasoning,"It not only spans five typologically diverse languages (Chinese, Tamil, Swahili, Indonesian, and Turkish) but also adopts different basic-level concepts across different cultures. Thus, the challenges are multi-faceted, including cross-modal, cross-lingual, and cross-cultural aspects. In this task, given two images (Ilef t and Iright) and a description D, the model needs to assess the validity of the description given the images, which can be cast as a classification problem. See the appendix C for sampled examples and detailed descriptions.","This task encompasses 5 languages with different types and structures (Chinese, Tamil, Swahili, Indonesian, and Turkish), and also uses different fundamental concepts across various cultures. So there are many complex challenges, including relating different modes, languages, and cultures. For this task, given 2 images (Ileft and Iright) and a description D, the model must evaluate if the description matches the images, which can be framed as a classification problem. See appendix C for example samples and more details.","The task includes 5 typologically diverse languages (Chinese, Tamil, Swahili, Indonesian, and Turkish) and utilizes different basic concepts in different cultures. Thus, there are multifaceted difficulties, spanning cross-modal, cross-lingual, and cross-cultural facets. For this task, with 2 given images (Ileft and Iright) and a description D, the model needs to judge if the description is valid for the images, which can be viewed as a classification challenge. Refer to appendix C for representative examples and in-depth explanations.","This task encompasses 5 typologically varied languages (Chinese, Tamil, Swahili, Indonesian, and Turkish) and also employs different fundamental concepts across various cultures. Hence, the challenges have multiple aspects, including cross-modal, cross-lingual, and cross-cultural facets. For this task, given 2 images (Ileft and Iright) and a description D, the model must assess if the description is accurate for the images, which can be considered a classification problem. See appendix C for sample examples and thorough descriptions.",A,0
215,Cultural Concept Adaptation on Multimodal Reasoning,"Developing cultural adaptation methods is important, which can improve the model performance on the low-resource ones and provide more equitable opportunities for everyone to benefit from advanced technology. Past methods primarily focused on multilingual and multimodal capabilities, and the improvement of multicultural competence is still an unexplored problem. This is largely due to the difficulty of data scarcity and expensive annotation. In this paper, we navigate this uncharted territory by leveraging high-resource cultures to facilitate comprehension of low-resource ones.","Creating ways for technology to adapt across cultures is vital, as it can make models work better for communities with limited resources and ensure everyone has equal access to advanced systems. Previous techniques largely focused on handling multiple languages and data types, while enhancing cross-cultural understanding remains an unsolved issue. This stems from the scarcity of annotated data. Here we break new ground by utilizing abundant data from some cultures to promote understanding of those with less data.","Developing methods for AI to adapt to different cultures is crucial, since it can improve performance for marginalized groups and enable broad access to cutting-edge tools. Earlier work concentrated mostly on multilingual and multimedia skills, with cross-cultural competence still an open challenge. Sparse labeled data makes this hard. Our research pioneers new territory by leveraging data-rich cultures to comprehend data-poor ones. ","Inventing techniques for AI to be culturally flexible is important, because it can make systems work better for underserved communities and let everyone benefit from high-tech advances. Past research stressed abilities with multiple languages and data types, while cross-cultural aptitude remains an unsolved issue. This results from minimal annotated data. We chart new ground by exploiting abundant cultural data to understand cultures with sparse data.",A,0
548,Non-autoregressive Text Editing with Copy-aware Latent Alignments,"We in this work propose to bridge this gap by introducing a special token K to denote the KEEP operation. Concretely, we interpret generating K at aiT`j , the jth upsampled position for ith source token, as directly copying the source token xi. In this way, the final output space of each ai is V Ť tKu Ť t∅u. Training objective Our final objective is to minimize the negative log-likelihood of all possible alignments with the three kinds of edit operations. Glancing training Previous works have shown that the glancing training strategy (Qian et al., 2021) can give a boost to the performance of nonautoregressive generation. ","In this work, we put forward a plan to connect this opening by bringing in a unique symbol K to represent the KEEP action. Specifically, we understand creating K at aiT`j, the jth oversampled location for ith origin token, as straightforwardly duplicating the source token xi. By doing this, the final production space of each ai is V Ť tKu Ť t∅u. Preparation objective Our final goal is to decrease the negative log-likelihood of all potential alignments with the three kinds of edit maneuvers. Quick training Past works have exhibited that the glancing training plan (Qian et al., 2021) can give a lift to the presentation of non-autoregressive generation.","We in this paper propose to bridge this difference by introducing a special token K to denote the RETAIN operation. In particular, we interpret generating K at aiT`j, the jth upsampled position for ith source token, as directly copying the source token xi. By this means, the final output space of each ai is V Ť tKu Ť t∅u. Learning objective Our final aim is to minimize the negative log-likelihood of all possible alignments with the three types of edit operations. Fast training Previous works have shown that the glancing training strategy (Qian et al., 2021) can provide a boost to the performance of non-autoregressive generation.","In this work, we suggest closing this gap by bringing in a unique sign K to represent the MAINTAIN action. Specifically, we understand producing K at aiT`j, the jth oversampled spot for ith source sign, as straight copying the source token xi. Through this, the final yield space of each ai is V Ť tKu Ť t∅u. Learning goal Our final purpose is to decrease the negative log-likelihood of all feasible alignments with the three sorts of edit actions. Quick learning Earlier works have displayed that the glancing training plan (Qian et al., 2021) can provide an improvement to the act of non-autoregressive generation.",A,0
350,Improved Techniques for Training Consistency Models,"Previous experiments with consistency models in Song et al. (2023) always employ zero dropout, motivated by the fact that consistency models generate samples in a single step, unlike diffusion models that do so in multiple steps. Therefore, it is intuitive that consistency models, facing a more challenging task, would be less prone to overfitting and need less regularization than their diffusion counterparts. Contrary to our expectations, we discovered that using larger dropout than diffusion models improves the sample quality of consistency models. Specifically, as shown in Fig. 1c, a dropout rate of 0.3 for consistency models on CIFAR-10 obtains better FID scores.","Earlier tests using consistency models from Song et al. (2023) constantly apply zero dropout, driven by the observation that consistency models make samples in one step, not like diffusion models which use multiple steps. So it makes sense that consistency models, with a harder job, would be less likely to overfit and require less regularization versus diffusion models. Surprisingly, we found that more dropout than diffusion models enhances sample quality for consistency models. Precisely, as Fig. 1c displays, a 0.3 dropout rate for consistency models on CIFAR-10 achieves superior FID scores.","Past experiments utilizing consistency models in the work of Song et al. (2023) always employ no dropout, justified by the fact that consistency models generate examples in a single pass, rather than diffusion models that do so over multiple passes. Therefore, it is intuitive that consistency models, undertaking a more difficult task, would be more robust to overfitting and necessitate less regularization compared to diffusion models. Contrary to expectations, we discovered that applying greater dropout than diffusion models improves the sample quality of consistency models. In particular, as illustrated in Fig. 1c, a 0.3 dropout rate for consistency models on CIFAR-10 obtains lower FID scores.","Earlier trials leveraging consistency models in Song et al. (2023) consistently apply zero dropout, motivated by consistency models synthesizing samples in one go, unlike diffusion models generating samples across multiple iterations. Thus, it is reasonable that consistency models, confronting a more challenging problem, would be more immune to overfitting and require less regularization versus diffusion counterparts. Surprisingly, we found larger dropout than diffusion models enhances sample quality for consistency models. Specifically, as shown in Fig. 1c, a 0.3 dropout level for consistency models on CIFAR-10 achieves superior FID metrics.",A,0
300,"Explain, Edit, Generate","Table 2 shows the effects of the data generated by RACE and baselines on the OOD generalization. We can observe that, (I) RACE significantly improves model performance on PolitiHop, SCIFACT and PubHealth compared to the results without data augmentation, and outperforms baselines on almost all OOD datasets, demonstrating the effectiveness of our augmentation strategy for multi-hop fact verification task. (II) RACE significantly outperforms POLYJUICE, showing that the general-purpose CDA method, designed for tasks without requiring complex reasoning on the input, fails to achieve acceptable results on multi-hop fact verification task, and even impairs the OOD generalization. (III) The counterfactual data generated by LLMs provides little improvement in OOD generalization, demonstrating that CDA for multi-hop fact verification task remains challenging for LLMs by using the incontext learning alone. (IV) The incorporation of (c, E′ , REF) further improves the model generalization to a certain extent on PolitiHop, indicating that the edited evidence still remains multi-hop correlated and reasonable.","The statistics in Table 2 exhibit the consequences of the information created by RACE and baseline models on out-of-distribution generalization. We notice that, (I) RACE substantially enhances model accuracy on PolitiHop, SCIFACT and PubHealth compared to the outcomes without data augmentation, and surpasses baselines on nearly all OOD datasets, proving the effectiveness of our augmentation approach for multi-hop fact verification tasks. (II) RACE significantly outdoes POLYJUICE, indicating that the general-purpose CDA technique, intended for tasks without requiring intricate reasoning on the input, fails to accomplish acceptable outcomes on multi-hop fact verification tasks, and even impairs OOD generalization. (III) The counterfactual information produced by LLMs provides minimal improvement in OOD generalization, demonstrating that CDA for multi-hop fact verification remains challenging for LLMs utilizing in-context learning alone. (IV) The inclusion of (c, E', REF) additionally improves model generalization to some degree on PolitiHop, signifying that the edited evidence still maintains multi-hop correlation and sensibility.","The numbers in Table 2 display the effects of the data generated by RACE and baseline systems on out-of-sample generalization. We discern that, (I) RACE substantially boosts model performance on PolitiHop, SCIFACT and PubHealth compared to the results without data augmentation, and exceeds baselines on nearly all OOD datasets, validating the efficacy of our augmentation methodology for multi-hop fact verification tasks. (II) RACE significantly surpasses POLYJUICE, indicating that the general-purpose CDA technique, intended for tasks without requiring complex reasoning on the input, fails to achieve satisfactory outcomes on multi-hop fact verification tasks, and even degrades OOD generalization. (III) The counterfactual data produced by LLMs provides minimal improvement in OOD generalization, demonstrating that CDA for multi-hop fact verification remains difficult for LLMs using in-context learning alone. (IV) The inclusion of (c, E', REF) additionally enhances model generalization to some extent on PolitiHop, signifying that the edited evidence still maintains multi-hop correlation and sensibility.  ","The data in Table 2 illustrate the impacts of the information generated by RACE and baseline models on out-of-distribution generalization. We note that, (I) RACE substantially improves model accuracy on PolitiHop, SCIFACT and PubHealth compared to the results without data augmentation, and exceeds baselines on nearly all OOD datasets, confirming the effectiveness of our augmentation methodology for multi-hop fact verification tasks. (II) RACE significantly outperforms POLYJUICE, indicating that the general-purpose CDA method, intended for tasks without requiring intricate reasoning on the input, fails to achieve satisfactory results on multi-hop fact verification tasks, and even degrades OOD generalization. (III) The counterfactual data produced by LLMs provides minimal improvement in OOD generalization, demonstrating that CDA for multi-hop fact verification remains challenging for LLMs utilizing in-context learning alone. (IV) The inclusion of (c, E', REF) additionally enhances model generalization to some extent on PolitiHop, signifying that the edited evidence still maintains multi-hop correlation and sensibility.",A,0
137,BOOOOKSCORE,"Before discussing our evaluation protocol, we first outline two strategies—hierarchical merging and incremental updating—for prompting an LLM to summarize book-length documents that exceed its maximum context size. In both strategies, the length of the input document necessitates first dividing it into smaller chunks and then repeatedly merging, updating, and/or compressing chunk-level partial summaries (Figure 1). While neither strategy is well-explored by published research, hierarchical merging essentially adapts the strategy proposed by Wu et al. (2021) to zero-shot prompting, while incremental updating resembles chain-of-density prompting proposed for short-document summarization (Adams et al., 2023). Both are implemented in widely-used open-source LLM libraries such as LangChain,1 but the relative merits of each method remain unexplored.","Prior to examining our assessment approach, we first describe two techniques—stepwise combining and progressive refining—for prompting a large language model to summarize documents longer than its maximum context capacity. For both techniques, the input document's length necessitates first splitting it into smaller segments and then iteratively combining, modifying, and/or condensing segment-level incomplete summaries (Figure 1). Although neither technique has been thoroughly studied in published work, stepwise combining essentially tailors the approach proposed by Wu et al. (2021) to zero-shot prompting, while progressive refining is similar to chain-of-thought prompting suggested for short-document summarization (Adams et al., 2023). Both are implemented in widely-used open-source large language model libraries like LangChain,1 but the relative benefits of each method are still unknown.","Before examining our evaluation methodology, we first outline two approaches—hierarchical integration and gradual enhancement—for instructing a large language model to summarize texts longer than its maximum context size. For both approaches, the length of the input text requires first dividing it into smaller portions and then repeatedly merging, updating, and/or compressing portion-level incomplete summaries (Figure 1). While neither approach has been extensively explored in published research, hierarchical integration essentially adapts the strategy proposed by Wu et al. (2021) to zero-shot prompting, while gradual enhancement is similar to chain-of-thought prompting proposed for short-text summarization (Adams et al., 2023). Both are implemented in widely-used open-source large language model libraries such as LangChain,1 but the relative advantages of each method remain unexplored.  ","Prior to discussing our assessment protocol, we first describe two techniques—tiered consolidation and stepwise refinement—for instructing a large language model to summarize texts exceeding its maximum context length. For both techniques, the input text's length necessitates first partitioning it into smaller segments and then iteratively combining, updating, and/or condensing segment-level partial summaries (Figure 1). Although neither technique has been thoroughly investigated in published research, tiered consolidation essentially adapts the approach proposed by Wu et al. (2021) to zero-shot prompting, while stepwise refinement resembles chain-of-thought prompting suggested for short-text summarization (Adams et al., 2023). Both are implemented in widely-used open-source large language model libraries like LangChain,1 but the relative merits of each method remain unexamined.",A,0
324,Fifty Shades of Bias,"A detailed analysis of the datasets used for gender bias was conducted by Stanczak and Augenstein (2021). Two more recently created datasets not introduced in the survey are the BUG dataset (Levy et al., 2021) and the CORGI-PM dataset (Zhang et al., 2023). Levy et al. (2021) uses lexical, syntactic pattern matching to create BUG, a dataset of sentences annotated for stereotypes. CORGIPM is a Chinese corpus of 32.9K sentences that were human-annotated for gender bias.","A thorough examination of the data collections utilized for gender prejudice was performed by Stanczak and Augenstein (2021). Two more recently formed data collections not presented in the review are the BUG data collection (Levy et al., 2021) and the CORGI-PM data collection (Zhang et al., 2023). Levy et al. (2021) employs lexical, syntactic pattern matching to generate BUG, a data collection of sentences annotated for stereotypes. CORGI-PM is a Chinese corpus of 32.9K sentences that were manually-annotated for gender bias.","An in-depth study of the information sets leveraged for gender inclination was undertaken by Stanczak and Augenstein (2021). Two additional freshly created information sets not brought in the survey are the BUG information set (Levy et al., 2021) and the CORGI-PM information set (Zhang et al., 2023). Levy et al. (2021) utilizes lexical, syntactic pattern correlating to produce BUG, an information set of sentences classified for stereotypes. CORGI-PM is a Chinese corpus of 32.9K sentences that were human-classified for gender inclination.  ","A meticulous review of the data compilations used for gender leaning was performed by Stanczak and Augenstein (2021). Two more recently assembled data compilations not showcased in the review are the BUG data compilation (Levy et al., 2021) and the CORGI-PM data compilation (Zhang et al., 2023). Levy et al. (2021) employs lexical, syntactic pattern complementing to form BUG, a data compilation of sentences annotated for stereotypes. CORGI-PM is a Chinese corpus of 32.9K sentences that were manually-annotated for gender leaning.",A,0
218,Cultural Concept Adaptation on Multimodal Reasoning,"A number of previous works (Vilares and Gómez-Rodríguez, 2018; Acharya et al., 2020; Yin et al., 2021; Liu et al., 2021a; Cao et al., 2023) have delved into cultural topics, largely focusing on cultural differences or evaluating the cross-cultural competency of computational models instead of enhancing them. The primary reason is the complexity of improving cross-cultural abilities, as low resource languages and their cultural concepts are inherently scarce, exacerbating the data scarcity issue. Moreover, annotating cross-cultural data and establishing links between concepts across cultures is an expensive process given the limited number of annotators well-versed in various countries’ cultures.","Several prior studies (Vilares and Gómez-Rodríguez, 2018; Acharya et al., 2020; Yin et al., 2021; Liu et al., 2021a; Cao et al., 2023) have explored cultural subjects, mostly concentrating on cultural discrepancies or judging the cross-cultural proficiency of computational systems rather than refining them. The key rationale is the intricacy of boosting cross-cultural skills, as low resource languages and their cultural ideas are naturally sparse, intensifying the data deficiency problem. Furthermore, labeling cross-cultural information and forming connections between concepts across cultures is a costly undertaking given the small number of annotators well-versed in the cultures of various countries.","A number of earlier works (Vilares and Gómez-Rodríguez, 2018; Acharya et al., 2020; Yin et al., 2021; Liu et al., 2021a; Cao et al., 2023) have investigated cultural topics, largely focusing on differences between cultures or evaluating the ability of computational models to work across cultures instead of improving them. The main reason is the complexity of enhancing cross-cultural capabilities, as languages with limited resources and their cultural concepts are inherently rare, worsening the data scarcity issue. In addition, annotating cross-cultural data and establishing links between concepts across cultures is an expensive process given the limited number of annotators knowledgeable about the cultures of various countries.","Several previous studies (Vilares and Gómez-Rodríguez, 2018; Acharya et al., 2020; Yin et al., 2021; Liu et al., 2021a; Cao et al., 2023) have examined cultural subjects, mostly centering on variances between cultures or assessing the cross-cultural competence of computational models rather than augmenting them. The primary rationale is the intricacy of boosting cross-cultural abilities, as languages with scarce resources and their cultural concepts are naturally uncommon, exacerbating the data deficiency problem. Moreover, labeling cross-cultural information and creating connections between concepts across cultures is a costly undertaking given the small number of annotators well-versed in the cultures of different countries.",A,0
174,Copyright Violations and Large Language Models,"Regarding the closed source models, GPT-3.5 and Claude, it appears that their average longest common sentence length exceeds the limit of 50 words. Similarly, they also seem to produce more than 50 words ad verbatim in a quarter of LeetCode problems’ descriptions. See the right part of Figure 2 for the average LCS length per book. Books such as Lolita, Harry Potter and the Sorcerer’s Stone, and Gone with the Wind, appear to be highly memorized, even with our simple probing strategies, leading the models to output very long chunks of text raising copyright concerns.","With respect to the proprietary models, GPT-3.5 and Claude, it seems their typical longest shared sentence length goes beyond the restriction of 50 words. Likewise, they also look to generate over 50 words verbatim in a quarter of LeetCode problems' outlines. Refer to the right section of Figure 2 for the mean LCS length per publication. Publications like Lolita, Harry Potter and the Sorcerer's Stone, and Gone with the Wind, seem to be very memorized, even with our simple probing tactics, resulting in the models producing very long segments of text raising copyright issues.","In regard to the closed-source models, GPT-3.5 and Claude, their median longest common sentence length appears to surpass the limit of 50 words. Furthermore, they also appear to produce more than 50 words word-for-word in one quarter of LeetCode problems' descriptions. See the right portion of Figure 2 for the average LCS length per book. Books such as Lolita, Harry Potter and the Sorcerer's Stone, and Gone with the Wind, seem to be highly remembered, even with our simple probing methods, leading the models to output very lengthy chunks of text raising copyright concerns.","Concerning the proprietary models, GPT-3.5 and Claude, their typical maximum shared sentence length seems to exceed the threshold of 50 words. Additionally, they also seem to generate over 50 words verbatim in one quarter of LeetCode problems' outlines. Refer to the right side of Figure 2 for the mean LCS length per text. Texts like Lolita, Harry Potter and the Sorcerer's Stone, and Gone with the Wind, appear highly memorized, even with our simple probing techniques, resulting in the models producing very long segments of text raising copyright issues.",A,0
444,Eliminating Lipschitz Singularities in Diffusion Models,"Latent diffusion models (LDM) (Rombach et al., 2022) is one of the most renowned variants of diffusion models. In this section, we will investigate the Lipschitz singularities in LDM (Rombach et al., 2022), and apply E-TSDM to address this problem. LDM (Rombach et al., 2022) shares a resemblance with DDPM (Rombach et al., 2022) but has an additional auto-encoder to encode images into the latent space. As LDM typically employs the quadratic schedule, it is also susceptible to Lipschitz singularities, as confirmed in Figure A10.","Latent diffusion models (LDM) (Rombach et al., 2022) is one of the most well-known versions of diffusion models. In this part, we will examine the Lipschitz singularities present in LDM (Rombach et al., 2022), and use E-TSDM to tackle this issue. LDM (Rombach et al., 2022) has similarities to DDPM (Rombach et al., 2022) but utilizes an extra autoencoder to encode images into the latent space. Since LDM commonly uses the quadratic schedule, it is also vulnerable to Lipschitz singularities, as shown in Figure A10.","One of the most acclaimed variants of diffusion models is latent diffusion models (LDM) (Rombach et al., 2022). We will inspect the Lipschitz singularities in LDM (Rombach et al., 2022) in this section, and apply E-TSDM to address this problem. LDM (Rombach et al., 2022) is comparable to DDPM (Rombach et al., 2022) but employs an additional autoencoder to encode images into the latent space. As the quadratic schedule is typically used in LDM, it is also prone to Lipschitz singularities, verified in Figure A10.","Latent diffusion models (LDM) (Rombach et al., 2022) is one of the most highly regarded versions of diffusion models. We will analyze the Lipschitz singularities present in LDM (Rombach et al., 2022) here, and use E-TSDM to resolve this issue. LDM (Rombach et al., 2022) resembles DDPM (Rombach et al., 2022) but utilizes an extra autoencoder to encode images into the latent space. Since LDM commonly adopts the quadratic schedule, it is also susceptible to Lipschitz singularities, as shown in Figure A10.",A,0
168,Copyright Violations and Large Language Models,"Language models generating full citations could be a good practice to avoid copyright violations. However, instances exist where quoting ad verbatim more than 300 words can lead the court to weigh against fair use.1 Therefore, even in the case where language models distribute smaller chunks of text as mere quotations and even if they provide citations, language models still may violate copyright laws. Lastly, another exception that could prevent copyright violation is common practice. Here, there is some variation. For book-length material, some say a quotation limit of 300 words is common practice, but others have argued for anything from 25 words to 1000 words .","Language models that produce complete references could help prevent copyright violations. But courts may rule against fair use if more than 300 words are copied word-for-word, even with a citation.1 So language models might still break copyright law by quoting smaller sections of text, even with citations. One exception is if short quotes are standard practice. But there is disagreement on what's acceptable - from 25 to 1000 words of a book.","AI systems that generate full bibliographic citations may aim to avoid copyright infringement. However, verbatim copying over 300 words, even with a citation,1 could lead a court to rule against fair use protections. Thus, language models might still violate copyright by quoting smaller passages and citing sources. One defense is if brief quotes are conventional. Yet there is variation - from 25 to 1000 words of a book is deemed acceptable.","Language models that produce complete citations could reduce copyright violations. Though courts may determine copying over 300 consecutive words verbatim, even with attribution,1 exceeds fair use. So language models may still infringe copyrights by excerpting smaller text segments, even with citations. One exception is if brief quotes are standard practice. But acceptable lengths range from 25 to 1000 words of a book.",A,0
67,ALDi Quantifying the Arabic Level of Dialectness of Text,"Arabic is spoken by more than 420 million people all over the world (Bergman and Diab, 2022), and exists in a state of Diglossia, in which two variants of the language co-exist in Arabic-speaking communities (Ferguson, 1959). Modern Standard Arabic (MSA) is the standardized variant, which is taught in schools and used in formal communications and as a common language across all Arab countries. However, many local variants of Dialectal Arabic (DA) are used for daily communication— mainly in speech and speech-like text such as social media. These differences between MSA and DA, and the fact that speakers commonly code-switch between the two, are a major challenge for Arabic NLP systems.","The Arabic language has over 420 million speakers worldwide (Bergman and Diab, 2022). It exhibits Diglossia, where two forms of the language exist together in Arabic-speaking groups (Ferguson, 1959). Modern Standard Arabic (MSA) is the standardized form, taught in schools and used officially across Arab nations. However, many local Dialectal Arabic (DA) variants are used daily, especially in speech and informal writing like social media. The differences and code-switching between MSA and DA pose major difficulties for Arabic natural language processing.","More than 420 million people speak Arabic globally (Bergman and Diab, 2022). The language displays Diglossia, with two versions used concurrently in Arabic communities (Ferguson, 1959). Modern Standard Arabic (MSA) is the standardized form, learned in education and used formally across Arab countries. But many local Dialectal Arabic (DA) varieties are used informally, particularly in speech and casual writing like social media. The variations and code-switching between MSA and DA present major obstacles for natural language processing of Arabic.  ","Arabic has over 420 million speakers worldwide (Bergman and Diab, 2022) and exhibits Diglossia, where two forms coexist in Arabic-speaking populations (Ferguson, 1959). Modern Standard Arabic (MSA) is the standardized variety, taught in school and used officially across the Arab world. However, many local Dialectal Arabic (DA) versions are used in daily life, especially speech and informal writing such as social media. The differences and code-switching between MSA and DA pose significant challenges for natural language processing systems dealing with Arabic.",A,0
518,Neural Fine-Tuning Search for Few-Shot Learning,"The results on Meta-Album are shown in Table 4 as a function of number of shots within the 5-way setting, following Ullah et al. (2022). We can see that across the whole range of support set sizes, our NFTS dominates all of the well-tuned baselines from Ullah et al. (2022). The margins are substantial, greater than 5% at 5-way/5-shot operating point, for example. This result confirms that our framework scales to even more diverse datasets and domains than those considered previously in Meta-Dataset.","The findings from Meta-Album are displayed in Table 4 based on the quantity of attempts within the 5-way configuration, as done by Ullah et al. (2022). We observe that over the full array of support set dimensions, our NFTS surpasses all of the well-calibrated baseline models from Ullah et al. (2022). The differences are considerable, more than 5% at the 5-way/5-shot data point, for instance. This outcome supports that our structure generalizes to even more varied datasets and fields than those previously examined in Meta-Dataset.","The Meta-Album results are shown in Table 4 as a function of the number of shots in the 5-way setting, following the work of Ullah et al. (2022). Across all support set sizes, we see that our NFTS outperforms all of the well-tuned baseline models from Ullah et al. (2022). The margins are large, over 5% at the 5-way/5-shot operating point, for example. This confirms that our framework scales to even more diverse datasets and areas than those considered in Meta-Dataset.  ","The Meta-Album outputs are presented in Table 4 based on the quantity of attempts within the 5-way arrangement, as per Ullah et al. (2022). We notice that over the entire range of support set magnitudes, our NFTS is superior to all of the well-calibrated baseline versions from Ullah et al. (2022). The differences are significant, above 5% at the 5-way/5-shot data point, for instance. This indicates that our system generalizes to even more varied datasets and topics than those previously covered in Meta-Dataset.",A,0
344,Improved Techniques for Training Consistency Models," We can train consistency models using either consistency distillation (CD) or consistency training (CT). The former requires pre-training a diffusion model and distilling the knowledge therein into a consistency model. The latter allows us to train consistency models directly from data, establishing them as an independent family of generative models. Previous work (Song et al., 2023) demonstrates that CD significantly outperforms CT. However, CD adds computational overhead to the training process since it requires learning a separate diffusion model. Additionally, distillation limits the sample quality of the consistency model to that of the diffusion model.","We have two options for teaching consistency models - consistency distillation (CD) or consistency training (CT). CD means first pre-educating a diffusion model and moving that knowledge into a consistency model. CT lets us educate consistency models straight from information, making them their own type of generative model. Earlier research (Song et al., 2023) shows CD is much better than CT. However, CD makes training more complex since we have to build a separate diffusion model. Also, distillation restricts the consistency model's sample quality to the diffusion model's level.","There are two ways to develop consistency models - consistency distillation (CD) or consistency training (CT). CD requires first constructing a diffusion model and transferring its knowledge to a consistency model. CT enables directly training consistency models from scratch as their own generative model family. Past work (Song et al., 2023) proves CD greatly surpasses CT in performance. But CD adds computational burden during training by needing a distinct diffusion model. And distillation limits the consistency model's sample quality to that of the diffusion model.  ","We have two approaches for instilling consistency models - consistency distillation (CD) or consistency training (CT). The former necessitates first forming a diffusion model and infusing its expertise into a consistency model. The latter permits directly cultivating consistency models from the ground up as their own generative model type. Earlier research (Song et al., 2023) validates CD substantially excels over CT. However, CD increases computational load during training through requiring a separate diffusion model. Additionally, distillation confines the consistency model's sample quality to the diffusion model's benchmark.",A,0
445,LLM-enhanced Self-training for Cross-domain Constituency Parsing,"Self-training has proven to be an effective approach for cross-domain tasks, and in this study, we explore its application to cross-domain constituency parsing. Traditional self-training methods rely on limited and potentially low quality raw corpora. To overcome this limitation, we propose enhancing self-training with the large language model (LLM) to generate domain-specific raw corpora iteratively. For the constituency parsing, we introduce grammar rules that guide the LLM in generating raw corpora and establish criteria for selecting pseudo instances. Our experimental results demonstrate that self-training for constituency parsing, equipped with an LLM, outperforms traditional methods regardless of the LLM’s performance. Moreover, the combination of grammar rules and confidence criteria for pseudo-data selection yields the highest performance in the cross domain constituency parsing. ","Self-guided learning has shown to be a useful approach for tasks across different areas, and in this research, we look at using it for parsing sentences into their grammatical structures across domains. Standard self-guided learning ways depend on limited and potentially low-quality unlabeled text. To get around this restriction, we suggest boosting self-training with the large language model (LLM) to iteratively generate domain-specific unlabeled text. For the parsing, we present grammar guidelines that direct the LLM in generating unlabeled text and set up standards for choosing pseudo examples. Our experimental outcomes show that self-training for parsing, equipped with an LLM, exceeds traditional methods regardless of the LLM's performance. Moreover, the combination of grammar guidelines and confidence criteria for pseudo-data selection produces the highest performance in parsing sentences across domains.","Self-teaching has been shown to be an effective technique for tasks spanning different fields, and in this study, we explore applying it to analyzing the grammatical structure of sentences across domains. Conventional self-teaching approaches rely on limited and potentially low-quality raw text data. To overcome this constraint, we propose enhancing self-training with the large language model (LLM) to iteratively generate domain-specific raw text data. For the grammatical analysis, we introduce syntactic rules that guide the LLM in generating raw text data and establish criteria for selecting pseudo samples. Our experimental findings demonstrate that self-training for grammatical analysis, equipped with an LLM, outperforms traditional methods regardless of the LLM's capabilities. Furthermore, the combination of syntactic rules and confidence criteria for pseudo-data selection yields the highest performance in analyzing grammar across domains.  ","Self-directed learning has proven to be an effective strategy for tasks spanning different areas, and in this research, we explore applying it to analyzing sentence structure across domains. Standard self-directed learning techniques depend on limited and potentially inadequate raw text. To overcome this constraint, we propose augmenting self-training with the large language model (LLM) to iteratively generate domain-specific raw text. For the grammatical analysis, we introduce syntax guidelines that direct the LLM in generating raw text and establish standards for selecting pseudo examples. Our experimental results show that self-training for grammatical analysis, equipped with an LLM, exceeds traditional methods regardless of the LLM's capabilities. Additionally, the combination of syntax guidelines and confidence criteria for pseudo-data selection produces the highest performance in analyzing sentence structure across domains.",A,0
783,VECHR,"The underlying data is based exclusively on the publicly available datasets of ECtHR documents available on HUDOC10. The documents are not anonymized and contain the real names of the individuals involved. We do not consider the dataset to be harmful, given that the judgments are already publicly available. We are conscious that, by adapting pre-trained encoders, our models inherit any biases they contain. The results we observed do not substantially relate to such encoded bias. Nonetheless, attention should be paid to how models on vulnerability are employed practically. In light of the aforementioned limitations and the high stakes in a human rights court, we have evaluated the potential for misuse of the vulnerability classification models. ","The foundational information derives solely from the publicly accessible collections of ECtHR paperwork found on HUDOC10. The paperwork is not made anonymous and includes the genuine names of the people involved. We do not see the collection as detrimental, since the rulings are already accessible to the public. We are aware that, by tailoring pre-trained encoders, our models gain any biases they hold. The results we saw do not considerably connect to such encoded bias. However, care should be given to how vulnerability classification models are utilized in practice. Considering the aforementioned constraints and the high risks in a human rights court, we have assessed the potential for misuse of the vulnerability classification models.","The underlying material stems entirely from the publicly available ECtHR document troves on HUDOC10. The documents are not anonymized and have the real identities of the concerned parties. We do not deem the trove harmful, as the judgments are already public knowledge. We recognize that, by adapting pretrained encoders, our models absorb any biases they harbor. The results we observed do not substantially relate to such ingrained bias. Still, attention should be paid to how vulnerability classification models are applied in reality. Given the aforementioned limitations and high stakes in a human rights court, we have gauged the potential for misuse of the vulnerability classification models.  ","The foundational content is wholly extracted from the publicly obtainable collections of ECtHR paperwork on HUDOC10. The paperwork contains the genuine names of the involved people and is not anonymized. We do not consider the collection harmful, since the rulings are already public. We acknowledge that, by tailoring pretrained encoders, our models assimilate any biases they have. The results we saw do not meaningfully connect to such encoded bias. However, care should be exercised in how vulnerability classification models are used practically. Considering the stated constraints and high risks in a human rights court, we have evaluated the potential for misuse of the vulnerability classification models.",A,0
765,VECHR,"We divided the dataset chronologically into three subsets: training (–05/2015, 590 cases), validation (05/2015–09/2016, 90 cases) and test (09/2016– 02/2019, 108 cases). VECHRexplain: We selected 40 cases (20 each) from the val and test splits for the explanation dataset. Within each split, our sampling procedure involved two steps. First, we ensured coverage of all seven types by sampling one case for each type. Subsequently, we randomly selected an additional 13 cases to supplement the initial selection. VECHRchallenge: To test the model’s ability to generalize across distribution shifts, we extend VECHR by collecting and annotating additional cases not related to Article 3. ","We split the data into three groups based on time: one for training (-05/2015, 590 examples), one for validation (05/2015-09/2016, 90 examples), and one for testing (09/2016-02/2019, 108 examples). To create the explanation dataset, we picked 40 cases (20 from validation and 20 from testing). First we made sure to get one case of each of the seven types. Then we randomly added 13 more cases. To see how well the model could apply to new situations, we added more cases not about Article 3.","The data was separated into three chronological subsets: one for training (-05/2015, containing 590 instances), one for validation (05/2015-09/2016, containing 90 instances), and one for testing (09/2016-02/2019, containing 108 instances). For the explanation dataset, we selected 40 cases (20 from validation and 20 from testing). We first ensured we had one case of each of the seven types, then randomly picked 13 additional cases. To test generalization across distribution shifts, we supplemented VECHR with more cases unrelated to Article 3.","We organized the data temporally into three groups: a training set (-05/2015, 590 examples), a validation set (05/2015-09/2016, 90 examples), and a test set (09/2016-02/2019, 108 examples). The explanation dataset was created by choosing 40 cases (20 from validation and 20 from testing). We first picked one case per each of the seven types, then randomly selected 13 more cases. To evaluate how well the model could generalize to new distributions, we added extra cases not pertaining to Article 3 to VECHR.",A,0
629,SOUL,"We evaluate these LLMs under a zero-shot setting. To reduce variance, we report the average results with three random seeds. The detailed setup can be found in Appendix A.1. Evaluation Metrics For the RC task, we report f1 scores for each class and the overall accuracy. For the JG task, we use different evaluation metrics for predictions l and justifications j. We measure statement predictions l using overall accuracy. For justifications j, we employ commonly used text generation metrics, including BLEU (Papineni et al., 2002), ROUGE(1/2/L) (Lin, 2004), and BERTScore (Zhang et al., 2020) to calculate their similarity with the annotated justifications. ","We assess these large language models in a zero-shot manner. To decrease variability, we present the mean outcomes using three arbitrary seeds. The precise configuration is located in Appendix A.1. Evaluation Procedures For the reading comprehension task, we document F1 scores for each category and the total accuracy. For the judgment generation task, we utilize distinct assessment metrics for predictions l and rationales j. We quantify statement predictions l using total accuracy. For rationales j, we employ commonly utilized text generation metrics, like BLEU (Papineni et al., 2002), ROUGE(1/2/L) (Lin, 2004), and BERTScore (Zhang et al., 2020) to calculate their closeness to the annotated rationales.","We evaluate these large language models without fine-tuning. To minimize randomness, we show the average performance using 3 random initializations. The specifics are in Appendix A.1. Performance Metrics For reading comprehension, we present F1 per class and overall accuracy. For judgment generation, we use different metrics for predictions l and explanations j. We measure statement predictions l by overall accuracy. For explanations j, we use common text generation metrics like BLEU (Papineni et al., 2002), ROUGE(1/2/L) (Lin, 2004), and BERTScore (Zhang et al., 2020) to assess their similarity to the annotated explanations.  ","We appraise these large language models in a zero-shot fashion. To decrease variability, we present the mean outputs using 3 arbitrary seeds. The particulars are in Appendix A.1. Evaluation Measures For reading comprehension, we report F1 per group and total accuracy. For judgment generation, we utilize distinct gauges for predictions l and justifications j. We quantify statement predictions l by overall accuracy. For justifications j, we use prevalent text generation gauges like BLEU (Papineni et al., 2002), ROUGE(1/2/L) (Lin, 2004), and BERTScore (Zhang et al., 2020) to calculate their resemblance to the annotated justifications.",A,0
417,INTERFAIR Debiasing with Natural Language Feedback for Fair Interpretable Predictions,"The rationale extractor and task predictor are not connected parametrically, another reason why we can only use heuristic-based methods to update the task rationales. The final accuracy and Bias F1 were not significantly different than what was achieved in our LSTM-based setup despite GPT-3 based INTERFAIR-base having significantly better performance (acc. 84.0). This suggests the choice of the underlying base model may not be significant if the output can be fixed through iterative debiasing.","The reasoning extractor and job forecaster are not linked in parameters, which is another justification for why we can only utilize heuristic-grounded techniques to refresh the job rationales. The concluding precision and Bias F1 were not notably divergent than what was attained in our LSTM-founded configuration despite GPT-3 founded INTERFAIR-base having extensively superior execution (acc. 84.0). This intimates the determination of the fundamental base archetype may not be noteworthy if the yield can be rectified through repetitive de-biasing.","The justification extractor and assignment predictor do not share parameters, so we must use heuristic methods to update the task justifications. The final accuracy and Bias F1 were similar to our LSTM setup, even though GPT-3 based INTERFAIR-base performed much better (84.0 acc.). This implies the base model choice may not matter if iterative debiasing can fix the output. ","The reason extractor and job anticipator have no common parameters, so heuristic techniques are required to refresh the job rationales. Despite INTERFAIR-base with GPT-3 having far superior performance (84.0 acc.), the final precision and Bias F1 were comparable to our LSTM configuration. This suggests the base model selection may be insignificant if repetitive bias elimination can correct the production.",A,0
336,Fifty Shades of Bias,"Therefore, in our study using a snowball sampling approach, we recruited 20 annotators from within Microsoft Research India who had some basic understanding of gender bias to perform the annotation task. All annotators were from India and had native proficiency in English. The annotators had at least an undergraduate degree as their minimum educational qualification. Out of the 20 annotators, 12 were male, and 8 were female.","As such, in our research utilizing a snowball sampling technique, we enlisted 20 labelers from within Microsoft Research India who had fundamental knowledge of gender bias to carry out the annotation assignment. All labelers were from India and had native fluency in English. The labelers had at least an undergraduate degree as their minimum educational credential. Out of the 20 labelers, 12 were men, and 8 were women.","Hence, in our investigation employing a snowball sampling approach, we recruited 20 coders from Microsoft Research India who possessed elementary comprehension of gender bias to execute the coding task. All coders were Indian natives and had native eloquence in English. The coders had a minimum of an undergraduate qualification as their least educational accreditation. Of the 20 coders, 12 were male, and 8 were female. ","Thus, in our examination utilizing a snowball sampling method, we engaged 20 reviewers from within Microsoft Research India who had basic understanding of gender bias to perform the review task. All reviewers were Indian residents and had native proficiency in English. The reviewers had at least an undergraduate degree as their minimum educational achievement. Out of the 20 reviewers, 12 were men, and 8 were women.",A,0
23,A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"In six out of eight (model, language, and stereotype) groups, apron,prof is higher for females than males. Regarding stereotypical vs. anti-stereotypical occupations, apron,prof is higher for the latter on three out of four model-language pairs. This statistic supports the intuition that anti-stereotypical cases are where the model is most challenged, particularly for female professions, which consistently have the lowest accuracy. These findings, taken together, reveal a concerning bias in the way professions are portrayed in the models. Even after making an extra effort to consider pronouns, professions are frequently translated into their male inflection, even when they would be stereotypically associated with the female gender.","Across six of the eight groups split by model, language, and stereotype, the apron,prof score is greater for women compared to men. Looking at stereotypical versus anti-stereotypical occupations, apron,prof is higher for the latter in three out of four model-language pairs. This measurement aligns with the idea that anti-stereotypical situations are where the model struggles the most, especially for female professions, which consistently have the lowest precision. These results, taken together, uncover a troubling predisposition in how the models portray professions. Even after making a concerted effort to consider pronouns, professions are often translated into the male form, even when they would stereotypically be linked with the female gender.","In six of the eight groups separated by model, language, and stereotype, the apron,prof score is higher for females versus males. When looking at stereotypical compared to anti-stereotypical occupations, apron,prof is greater for the latter in three out of four model-language pairs. This data point supports the notion that anti-stereotypical cases are where the model faces the biggest challenges, especially for female professions, which consistently have the lowest accuracy. These findings, collectively, expose a disturbing bias in how the models depict professions. Even after making a dedicated effort to account for pronouns, professions are frequently translated into the masculine form, even when they would stereotypically be associated with the feminine gender.  ","Across six of the eight categories divided by model, language, and stereotype, the apron,prof metric is elevated for women in contrast to men. In terms of stereotypical versus anti-stereotypical occupations, apron,prof is higher for the latter in three out of four model-language combinations. This measurement reinforces the idea that anti-stereotypical instances are where the model struggles the most, particularly for female professions, which consistently have the poorest performance. These results, together, uncover an alarming prejudice in how the models portray professions. Even after making a concerted attempt to factor in pronouns, professions are often translated into the male version, even when they would stereotypically be linked to the female gender.",A,0
247,Cultural Concept Adaptation on Multimodal Reasoning,"Liu et al. (2021a) points that most of the synsets employed by NLVR2 (Suhr et al., 2019) and ImageNet (Deng et al., 2009) are only present in 30 or fewer languages and they contain overly specific concepts that belong to leaf nodes in WordNet. Given the biases in ImageNet-derived or inspired datasets, they define a protocol to collect data that is driven by native speakers of a language, consisting of concepts arising from their lived experiences. As a consequence, the descriptions are written in five languages: Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish, and the concepts are selected to be culturally relevant. Both multilingual and monolingual models perform comparably well in English (NLVR2).","Liu and colleagues (2021a) indicate that the majority of synsets used in NLVR2 (Suhr et al., 2019) and ImageNet (Deng et al., 2009) exist in only 30 or fewer languages and contain overly narrow concepts belonging to the outermost nodes in WordNet. Given the biases present in ImageNet-based or related datasets, they outline a method to gather data driven by native speakers of a language, made up of concepts from their real-world encounters. Thus, the descriptions are authored in five tongues: Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish, with concepts picked for cultural importance. Both multilingual and single-language models have similar strong performance in English (on NLVR2).","The study by Liu and coauthors (2021a) finds that most of the synsets used in NLVR2 (Suhr et al., 2019) and ImageNet (Deng et al., 2009) are present in only 30 or fewer languages and comprise excessively specific concepts situated at the leaf nodes in WordNet. Considering the biases in datasets derived from or inspired by ImageNet, they develop a methodology to collect data guided by native speakers of a language, made up of concepts stemming from their lived experiences. Consequently, the descriptions are written in five languages: Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish, with concepts chosen for cultural relevance. Both multilingual and monolingual models have comparable high performance in English (on NLVR2).  ","The research by Liu and colleagues (2021a) indicates that the majority of synsets employed in NLVR2 (Suhr et al., 2019) and ImageNet (Deng et al., 2009) exist in only 30 or fewer languages and contain overly narrow concepts located at the terminal nodes in WordNet. In light of the biases present in datasets derived from or modeled after ImageNet, they design a protocol to gather data directed by native speakers of a language, comprising concepts arising from their real-world experiences. As a result, the descriptions are authored in five tongues: Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish, with concepts selected for cultural significance. Both multilingual and monolingual models exhibit similar strong performance in English (on NLVR2).",A,0
734,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"Since the initial fixer is far from optimal, the pseudo-labels assigned by the initial fixer may have low precision. To address this problem, we analyze the relation between the confidence of ˆy(i) and the precision of z(i). In Figure 4, we observe that highconfidence predictions (i.e., ˆy(i) predicted with a high probability) are associated with more accurate grammaticality labels. Therefore, we propose to select a highly confident subset Dsub from D′m such that for every x(i) ∈ Dsub, the fixer predicts ˆy(i) with probability greater than 0.9. ","The original fixer is not very good, so the initial pseudo-labels it assigns may be inaccurate. We looked at how the confidence of the predictions relates to their precision. As shown in Figure 4, predictions made with high confidence (i.e. high probability) tend to have more correct grammaticality labels. So we suggest choosing a subset Dsub from D'm where for every x(i) in Dsub, the prediction ˆy(i) has a probability over 0.9.","Since the first fixer is suboptimal, the pseudo-labels it provides can lack precision. We studied the connection between the certainty of ˆy(i) and the accuracy of z(i). Figure 4 shows that predictions made very confidently (with high probability) tend to have more precise grammaticality tags. Thus, we recommend selecting a very confident subset Dsub from D′m where for all x(i) in Dsub, the fixer predicts ˆy(i) with a probability above 0.9.","The initial fixer has room for improvement, so its pseudo-labels may not be very accurate. We examined the relationship between the model's confidence in ˆy(i) and the precision of z(i). As evident in Figure 4, highly confident predictions (those with high probability) correspond to more precise grammaticality labels. Therefore, we suggest extracting a highly confident subset Dsub from D′m such that for every x(i) in Dsub, the fixer predicts ˆy(i) with a probability greater than 0.9.",A,0
677,Standardizing Distress Analysis,"This may have invited some undesired sampling bias while constructing the dataset.  Additionally, emoticons and other non-standard symbols like $ are often used in current online interactions.  One potential research direction is to use these neglected visual features of text information to adapt to more realistic settings.  Ethical Consideration We created our resource using publicly accessible social media postings.  We adhered to the data use guidelines and did not infringe on any copyright problems.  Our Institutional Review Board also reviewed and approved this research.  We make the code and data accessible for research purposes through an appropriate data agreement mechanism.","This process might have introduced some unwanted bias in selecting the data samples when building the dataset. Also, emoticons and other symbols not in the standard alphabet like $ are frequently used in current online conversations. One possible future research avenue is utilizing these overlooked visual components of text data to adapt to more practical real-world situations. Ethical Considerations We assembled our resource utilizing publicly available social media posts. We followed the data usage guidelines and did not violate any copyright laws. Our Institutional Review Board also examined and sanctioned this research. We make the code and information available for research purposes through an appropriate data agreement system.","This could have brought in some undesirable skewing while gathering the examples for the dataset. Furthermore, emojis and other non-standard characters such as $ are often utilized in present online interactions. One prospective research focus is leveraging these neglected visual features of text to adjust to more realistic settings. Ethical Implications We compiled our resource from publicly posted social media content. We adhered to the data usage policies and did not infringe on any copyright issues. Our Institutional Review Board also evaluated and approved this research. We provide access to the code and data for research purposes through a suitable data access mechanism.","This might have introduced some unwanted bias during the data collection for the dataset. Also, emoticons and other unconventional symbols such as $ are commonly used in today's online conversations. One possible future research direction is harnessing these overlooked visual aspects of text to adapt to more practical real-life situations. Ethical Considerations We built our resource using publicly available social media posts. We followed the data usage terms and did not violate any copyright laws. Our Institutional Review Board also reviewed and permitted this research. We enable access to the code and information for research purposes under proper data access protocols.",A,0
779,VECHR,"As the court uses similar phrases in cases against the same respondent state or alleging the same violation, the model may learn that these are particularly relevant, even though this does not represent the legal reality. In this regard, it is questionable whether cases of the ECtHR can be considered “natural language”. Moreover, the wording of case documents is likely to be influenced by the decision or judgement of the Court. This is because the documents are composed by court staff after the verdict. Awareness of the case’s conclusion could potentially impact the way its facts are presented, leading to the removal of irrelevant information or the highlighting of facts that were discovered during an investigation and are pertinent to the result.","The model may incorrectly learn that certain phrases are especially relevant when the court uses similar wording in multiple cases against the same country or alleging the same violation. This does not reflect legal reality. Additionally, it is debatable whether ECtHR cases can be viewed as ""natural language"" since the wording of case documents is probably influenced by the Court's decision or judgment. This is because court staff write the documents after the verdict is reached. Knowing the conclusion could affect how they present the facts, causing them to omit irrelevant information or emphasize facts that were discovered during the investigation and are related to the outcome.","When the court utilizes comparable terminology in lawsuits against the same nation or claiming the same breach, the model might erroneously conclude that these phrases are particularly meaningful, even though this is not legally accurate. Furthermore, it is questionable if ECtHR cases can be regarded as ""natural language"" given that the phrasing of case records is likely shaped by the Court's decision or ruling. This is because court employees compose the documents after the verdict is handed down. Awareness of the case's end result could potentially impact how its details are conveyed, resulting in the omission of unimportant information or the highlighting of facts that were uncovered during an examination and pertain to the conclusion.  ","Since the court employs similar wording in multiple cases against the same respondent state or alleging the same violation, the model may incorrectly learn these phrases are especially relevant, when in fact this does not match legal reality. Additionally, it is debatable whether ECtHR cases qualify as ""natural language"" since the language used in case documents is probably influenced by the Court's decision or judgment, given that court staff write the documents after the verdict is reached. Knowing the case's conclusion could shape how they present the facts, leading them to leave out unimportant details or emphasize facts discovered during the investigation that relate to the outcome.",A,0
465,LLM-enhanced Self-training for Cross-domain Constituency Parsing,"For the five target domains under consideration, each comprising 1,000 test samples, the number of available outputs is 424 (Dialogue), 212 (Forum), 276 (Law), 114 (Literature), and 367 (Review), respectively. The LLM exhibits domain bias in the formatting errors of parse tree. It is important to highlight that the reported scores are likely higher than the actual performance, and the scores presented in the main table have been adjusted by multiplying the corresponding available probability. Furthermore, compared to the other domains, gpt- 3.5-turbo demonstrates a significantly better performance in constituency parsing for Law domain, just looking at the correctly formatted parsing results. Secondly, we investigated direct model transfer for cross-domain constituency parsing, a strong baseline method compared with LLMs’ parsing. ","For the five target areas being looked at, each having 1,000 test samples, the quantity of existing outputs is 424 (Dialogue), 212 (Forum), 276 (Law), 114 (Literature), and 367 (Review), respectively. The LLM shows bias towards certain domains in the formatting mistakes of parse tree. It is vital to emphasize that the documented scores are likely higher than the real performance, and the scores presented in the main table have been modified by increasing the corresponding available probability. Furthermore, compared to the other domains, gpt- 3.5-turbo shows a significantly better performance in constituency parsing for Law domain, just examining the correctly formatted parsing results. Secondly, we inspected direct model transfer for cross-domain constituency parsing, a strong baseline approach compared with LLMs’ parsing.","Across the five target domains under examination, where each contains 1,000 test examples, there are 424 (Dialogue), 212 (Forum), 276 (Law), 114 (Literature), and 367 (Review) outputs available, in that order. The LLM displays preference towards certain domains in the formatting errors of the parse tree. It is crucial to stress that the recorded scores are probably higher than the true performance, and the scores shown in the main table were adjusted by multiplying the matching available probability. Additionally, compared to the other domains, gpt-3.5-turbo shows a significantly superior performance in constituency parsing for the Law domain, looking solely at the accurately formatted parsing outputs. Secondly, we analyzed direct model transfer for cross-domain constituency parsing, a robust baseline approach compared to LLMs' parsing.  ","For the five target areas analyzed, with each containing 1000 test samples, there are 424 (Dialogue), 212 (Forum), 276 (Law), 114 (Literature), and 367 (Review) outputs present, respectively. The LLM exhibits inclination towards particular domains in the formatting mistakes of the parse tree. It is important to emphasize that the documented scores are likely inflated versus the true performance, and the scores in the main table were modified by increasing the relevant available probability. Furthermore, relative to the other domains, gpt-3.5-turbo displays a significantly better performance in constituency parsing for the Law domain, considering just the correctly formatted parsing results. Secondly, we inspected direct model transfer for cross-domain constituency parsing, a strong baseline methodology compared to LLMs' parsing.",A,0
652,Standardizing Distress Analysis,"These span annotations help us to delve further into the manifestations of hatred or offensive speech.  For the Distressed Identification task, the Krippendorff’s α for the inter-annotator agreement is 0.66 which is much higher than other hate speech datasets (Ousidhoum et al., 2019; Mathew et al., 2021).  Following the work in (Poria et al., 2021; Ghosh et al., 2022c), we marked at most 3 causal spans for a distressed post in the dataset.  The final causal span is marked using the span-level aggregation approach detailed in (Gui et al., 2016). ","These range markings assist us in examining the appearances of hate or offensive language more deeply. For the Upset Recognition task, the Krippendorff's α for the agreement between annotators is 0.66, which is much greater than other hate speech data sets (Ousidhoum et al., 2019; Mathew et al., 2021). Pursuing the work in (Poria et al., 2021; Ghosh et al., 2022c), we labeled at most 3 causal ranges for a distressed post in the data set. The final causal range is marked utilizing the span-level aggregation technique explained in (Gui et al., 2016).","These extent tags help us delve further into the occurrences of hateful or offensive speech. For the Distressed Identification task, the concordance between annotators as measured by Krippendorff's α is 0.66, much higher than for other hate speech data sets (Ousidhoum et al., 2019; Mathew et al., 2021). Following the approaches in (Poria et al., 2021; Ghosh et al., 2022c), we annotated up to 3 causal extents for each distressed post in the data set. The final causal extent is determined using the span aggregation method described in (Gui et al., 2016).  ","These range labels assist our examination of the manifestations of hateful or offensive language. For Distressed Identification, inter-annotator agreement via Krippendorff's α is 0.66, far exceeding that of other hate speech datasets (Ousidhoum et al., 2019; Mathew et al., 2021). As in (Poria et al., 2021; Ghosh et al., 2022c), we marked up to 3 causal ranges per distressed post. The final causal range follows the span aggregation approach of (Gui et al., 2016).",A,0
773,VECHR,"Evaluation Metrics: we report micro-F1 (mic-F1) and macro-F1 (macF1) scores for 7+1 labels, where 7 labels correspond to 7 vulnerability types under consideration and an additional augmented label during evaluation to indicate non-vulnerable. pre-training. However, BERT models still face the input limitation constraint. Both Longformer and Hierarchical models improved compared to truncated variants and are comparable to each other. Overall, we see low overall performance across models, highlighting the challenging task. We use Integrated Gradient (IG) (Sundararajan et al., 2017) to obtain token-level importance from the model with respect to each vulnerable type under consideration. We max pool over sub-words to convert token-level IG scores into word-level scores, followed by a threshold-based binarization. ","Evaluation Results: we present micro-F1 (mic-F1) and macro-F1 (macF1) results for 7+1 labels, where 7 labels match 7 vulnerability types we examined and an extra label during assessment to signify non-vulnerable. pre-training. Still, BERT models still have the limitation of input size. Both Longformer and Hierarchical models were better than truncated versions and are comparable to each other. Overall, we see low total performance across models, highlighting the tough task. We utilize Integrated Gradient (IG) (Sundararajan et al., 2017) to get token-level importance from the model for each vulnerable type we looked at. We max pool over sub-words to turn token-level IG scores into word-level scores, followed by a threshold-based binarization.","Evaluation Metrics: we show micro-F1 (mic-F1) and macro-F1 (macF1) scores for 7+1 labels, where 7 labels represent 7 vulnerability types we examined and an extra label during evaluation to indicate non-vulnerable. pre-training. However, BERT models still have the limitation of input size. Both Longformer and Hierarchical models performed better than truncated versions and are similar to each other. Overall, we see low performance across models, highlighting the difficult task. We use Integrated Gradient (IG) (Sundararajan et al., 2017) to obtain token-level importance from the model for each vulnerable type we considered. We max pool over sub-words to change token-level IG scores into word-level scores, followed by a threshold-based binarization.  ","Evaluation Measures: we report micro-F1 (mic-F1) and macro-F1 (macF1) scores for 7+1 labels, where 7 labels correspond to 7 vulnerability types under examination and an additional label during evaluation to indicate non-vulnerable. pre-training. However, BERT models still have the constraint of input size. Both Longformer and Hierarchical models were better than truncated versions and are similar to each other. Overall, we see low performance across models, highlighting the challenging task. We utilize Integrated Gradient (IG) (Sundararajan et al., 2017) to obtain token-level importance from the model with respect to each vulnerable type we considered. We max pool over sub-words to convert token-level IG scores into word-level scores, followed by a threshold-based binarization.",A,0
477,Model Tells you what to Discard - Adaptive KV Cache Compression for LLMs,"Based on the Transformer architecture, autoregressive language models have attracted extensive attention (OpenAI, 2023; Touvron et al., 2023b). Along with the increase of model size, these models present significant challenges in terms of computational complexity and GPU memory consumption (Shazeer et al., 2017). Since these models achieve remarkable success across diverse applications, there is a pressing need for serving these models in an economically feasible manner.","Drawing from the Transformer design, self-regressive language systems have garnered widespread interest (OpenAI, 2023; Touvron et al., 2023b). As these models grow in scale, they pose major difficulties regarding computational cost and GPU memory usage (Shazeer et al., 2017). Because these models have accomplished impressive results across many applications, it is imperative to deploy them in a financially viable way.","Leveraging the Transformer blueprint, autoregressive natural language models have attracted considerable attention (OpenAI, 2023; Touvron et al., 2023b). With increasing model size, these systems present substantial challenges regarding computing complexity and GPU memory demands (Shazeer et al., 2017). Since these systems have achieved remarkable success across various tasks, there is an urgent need to serve these models economically.  ","Building on the Transformer architecture, self-predicting language models have gained widespread notice (OpenAI, 2023; Touvron et al., 2023b). As the scale of these systems expands, they introduce major difficulties in computational cost and GPU memory capacity (Shazeer et al., 2017). Because these systems have realized impressive achievements across numerous applications, it is essential to deploy them in a cost-effective manner.",A,0
527,Neural Fine-Tuning Search for Few-Shot Learning,"While there exist some recent NAS works that try to address a similar “train once, search many times” problem efficiently (Cai et al., 2020; Li et al., 2020a; Molchanov et al., 2022; Moons et al., 2021), naively using these approaches has two serious shortcomings: i) They assume that after the initial supernet training, subsequent searches do not involve any training (e.g., a search is only performed to consider a different FLOPs constraint while accuracy of different configurations is assumed to stay the same) and thus can be done efficiently – this is not true in the FSL setting as explained earlier. ii) Even if naively searching for each dataset at test time were computationally feasible, the few-shot nature of our setting poses a significant risk of overfitting the architecture to the small support set considered in each episode.","While some recent neural architecture search works have attempted to tackle a similar issue of ""training once then searching multiple times"" in an efficient way (Cai et al., 2020; Li et al., 2020a; Molchanov et al., 2022; Moons et al., 2021), simply applying these approaches has two major flaws: i) They presume that after the initial supernet training, subsequent searches do not require any additional training (for instance, a search is only done to consider a different FLOPs limit while the accuracy of various configurations is assumed to remain constant) and can thus be performed efficiently - however, this is not the case in the few-shot learning scenario as previously explained. ii) Even if naively searching for each dataset at test time was computationally viable, the few-shot essence of our setting poses a significant risk of overfitting the architecture to the small support set considered in each episode.","While some recent neural architecture search works have tried to tackle an analogous issue of ""train once, then search many times"" efficiently (Cai et al., 2020; Li et al., 2020a; Molchanov et al., 2022; Moons et al., 2021), simply utilizing these methodologies has two major deficiencies: i) They assume that after the initial supernet training, subsequent searches do not need any additional training (for example, a search is only conducted to consider a different FLOPs constraint while the accuracy of different configurations is presumed to remain unchanged) and can thus be performed efficiently - however, this is not true in the few-shot learning case as previously explained. ii) Even if naively searching for each dataset at test time was computationally feasible, the few-shot nature of our setting poses a significant risk of overfitting the architecture to the small support set considered in each episode.  ","Although some recent neural architecture search works have attempted to address a similar problem of ""train once, search many times"" efficiently (Cai et al., 2020; Li et al., 2020a; Molchanov et al., 2022; Moons et al., 2021), simply applying these techniques has two major shortcomings: i) They presume that after the initial supernet training, subsequent searches do not require any further training (for example, a search is only done to consider a different FLOPs constraint while the accuracy of different configurations is assumed to remain unchanged) and can thus be performed efficiently - however, this is not the case in the few-shot learning scenario as explained earlier. ii) Even if naively searching for each dataset at test time was computationally viable, the few-shot essence of our setting poses a significant risk of overfitting the architecture to the small support set considered in each episode.",A,0
97,Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering," In this paper, we delve into why TTA causes model collapse and find that the imbalanced label distribution inherent in QA is the reason for it. To address this problem, we propose Anti-Collapse Fast test-time adaptation (Anti-CF), which utilizes the source model‘s output to regularize the update of the adapted model during test time. We further design an efficient side block to reduce its inference time. Extensive experiments on various distribution shift scenarios and pre-trained language models (e.g., XLM-RoBERTa, BLOOM) demonstrate that our method can achieve comparable or better results than previous TTA methods at a speed close to vanilla forward propagation, which is 1.8× to 4.4× speedup compared to previous TTA methods.","This paper investigates why test-time adaptation causes model collapse and finds that the skewed label distribution in question answering is the culprit. To tackle this, we introduce Anti-Collapse Fast test-time adaptation, which leverages the source model's predictions to regulate the adapted model's updates during inference. We also design an efficient side module to accelerate it. Comprehensive experiments on various distribution shift cases and pre-trained language models (like XLM-RoBERTa, BLOOM) show our method can match or surpass prior TTA approaches at a speed near vanilla forward pass, which is 1.8-4.4x faster than previous TTA methods.","In this work, we analyze the reasons behind test-time adaptation resulting in model collapse and determine the inherent imbalanced label distribution in QA tasks is the cause. As a solution, we put forward Anti-Collapse Fast test-time adaptation that uses the source model's outputs to constrain the adapted model's changes during test time. We also construct an efficient side block to speed it up. Extensive tests on multiple distribution shift scenarios and pre-trained language models (such as XLM-RoBERTa, BLOOM) exhibit that our approach can achieve equal or superior performance to past TTA techniques at a velocity close to plain forward propagation, which is 1.8-4.4 times quicker than previous TTA approaches.","This paper investigates why test-time adaptation leads to model collapse and finds the skewed label distribution intrinsic to QA is the reason. To address this, we present Anti-Collapse Fast test-time adaptation, which regulates the adapted model's updates during inference using the source model's predictions. We also design a fast side module to accelerate it. Comprehensive experiments on various distribution shift settings and pre-trained language models (like XLM-RoBERTa, BLOOM) show our method can match or outperform previous TTA methods at a speed near vanilla forward pass, which is 1.8-4.4x faster than prior TTA approaches.",A,0
365,INSTRUCTSCORE,"SEScore (Xu et al., 2022b,a) show a higher correlation with humans on text generation tasks. However, all these metrics produce a single numerical score. These learned metrics lack interpretation of predictions nor link the scores with individual defects in the candidate text. How can we devise a fine-grained explanation based text generation metric capable of pinpointing concrete error locations, identifying error types, assigning severity labels, and justifying the final score—all simultaneously without relying on human-annotated data. In this paper, we propose INSTRUCTSCORE, a method to learn an explainable text generation metric without using human annotated ratings.","Xu et al.'s SEScore (2022b,a) exhibits a stronger connection with human judgments on text creation assignments. However, these gauges only yield a solitary quantitative rating. The acquired measurements do not elucidate forecasts or associate the outcomes with particular blemishes in the possibility content. How might we plan an elaborate clarification based text age metric equipped for recognizing explicit mistake areas, recognizing slip-up writes, appointing gravity names, and defending the last score—all simultaneously without depending on human-commented information. In this paper, we recommend INSTRUCTSCORE, a technique to gain proficiency with an interpretable text age metric without utilizing human commented evaluations.","Xu et al.'s SEScore (2022b,a) shows higher agreement with people on language generation tasks. Though, these metrics just produce one number. The learned assessments don't explain predictions or relate the marks to specific problems in the candidate text. How can we create a detailed explanation based text generation measure that can identify precise error locations, recognize error types, assign severity labels, and justify the final score—all at the same time without needing human-annotated data. In this paper, we propose INSTRUCTSCORE, a method to learn an understandable text generation metric without using human rated judgments.","Xu et al.'s SEScore (2022b,a) exhibits greater correlation with human ratings on text creation tasks. However, these measures only generate a single numeric score. The learned evaluations do not elucidate forecasts or connect the results with particular defects in the candidate content. How might we develop a fine-grained clarification based text generation gauge capable of pinpointing specific mistake locations, identifying mistake varieties, designating severity marks, and validating the final result—all simultaneously without depending on human-commented information. In this paper, we put forward INSTRUCTSCORE, a technique to acquire an interpretable text creation metric without utilizing human evaluated judgments.",A,0
586,RESEE,"This may be explained as models with separate encoder-decoder explicitly divide the understanding process of multimodal information and the generation of textual responses using different model parameters.  This makes the model devote more to each learning phase.  (2) On both constructed datasets, RESEE (SEP.) with full visual knowledge achieves the best or competitive performance in terms of relevance metrics i.e., BLEU, Rouge-L, even comparing models with task-oriented pre-training (DIALOGPT) or external document knowledge (MSDP).  This observation demonstrates the effectiveness of our model leveraging representations from both text and vision. ","This can be clarified as models having distinct encoder-decoder components overtly split the process of comprehending multimodal data and producing textual responses using separate model parameters. This enables the model to focus more on each learning stage. (2) On the two constructed datasets, RESEE (SEP.) utilizing complete visual knowledge attains the best or competitive scores regarding relevance metrics like BLEU, Rouge-L, even comparing to models with task-oriented pre-training (DIALOGPT) or external document knowledge (MSDP). This shows the efficacy of our model harnessing representations from text and vision.","One explanation is that models with separate encoder-decoder modules explicitly divide understanding multimodal information and generating textual responses into distinct steps using different model parameters. This allows the model to concentrate more on each phase of learning. (2) On both synthetic datasets, RESEE (SEP.) with full visual knowledge achieves the optimal or competitive results on relevance metrics such as BLEU, Rouge-L, even compared to models with task-specific pre-training (DIALOGPT) or external document knowledge (MSDP). This demonstrates the effectiveness of our model utilizing representations from text and images.  ","This can be elucidated as models containing individual encoder-decoder components overtly separate the process of analyzing multimodal data and producing textual responses utilizing distinct model parameters. This enables the model to place more emphasis on each learning stage. (2) On the two artificial datasets, RESEE (SEP.) leveraging complete visual knowledge attains the best or competitive performance on relevance metrics including BLEU, Rouge-L, even compared to models with task-oriented pre-training (DIALOGPT) or external document knowledge (MSDP). This exhibits the efficacy of our model exploiting representations from text and visuals.",A,0
517,Neural Fine-Tuning Search for Few-Shot Learning,"We re-iterate that PMF, ETT, and TSA are special cases of our search space corresponding respectively to: (i) Fine-tune all and include no adapters, (ii) Include ETT adapters at every layer while freezing all backbone weights, and (iii) Include TSA adapters at every layer while freezing all backbone weights. We also share initial pre-trained backbones with ETT and TSA (but not PMF, as they use a stronger pre-trained model with additional data). Thus, the margins achieved over these competitors are attributable to our systematic approach to finding suitable architectures, in terms of where to fine-tune and where to insert new adapter parameters.","We want to stress that PMF, ETT, and TSA are specific instances of our search space that correspond to: (i) Fine-tuning the entire model and not using any adapters, (ii) Adding ETT adapters to every layer while keeping all backbone weights frozen, and (iii) Adding TSA adapters to every layer while keeping all backbone weights frozen. We also start with the same pre-trained backbones as ETT and TSA (but not PMF, since they use a stronger pre-trained model trained on more data). Therefore, the improvements we achieve over these other methods can be attributed to our systematic approach for finding the right architectures, in terms of where to fine-tune and where to insert new adapter parameters.","Let us restate that PMF, ETT, and TSA are particular cases within our search space that match up to: (i) Fine-tuning the whole model and not utilizing any adapters, (ii) Incorporating ETT adapters into every layer while fixing all backbone weights, and (iii) Incorporating TSA adapters into every layer while fixing all backbone weights. We also start with the same pre-trained backbones as ETT and TSA (unlike PMF, which uses a stronger pre-trained model trained with more data). As a result, the gains we obtain over these other techniques can be credited to our systematic methodology for identifying optimal architectures, regarding where to fine-tune and where to add new adapter parameters.  ","We want to reaffirm that PMF, ETT, and TSA are specific examples within our search space that correspond to: (i) Fine-tuning the entire model and using no adapters, (ii) Adding ETT adapters to every layer while keeping all backbone weights constant, and (iii) Adding TSA adapters to every layer while keeping all backbone weights constant. We also use the same initial pre-trained backbones as ETT and TSA (unlike PMF, which uses a stronger pre-trained model trained on additional data). Therefore, the improvements we achieve over these other approaches can be attributed to our systematic method for finding the right architectures, in terms of where to fine-tune and where to insert new adapter parameters.",A,0
693,"Unnatural Error Correction, GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text","More surprisingly, we found that only GPT-4 nearly flawlessly processes inputs with unnatural errors, a task that poses significant challenges for other LLMs and often even for humans.  Specifically, GPT-4 can almost perfectly reconstruct the original sentences from scrambled ones, decreasing the edit distance by 95%, even when all letters within each word are entirely scrambled.  It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text. Large language models (LLMs) demonstrate impressive proficiency across a range of tasks, with certain capabilities emerging as the models scale up in size—a phenomenon commonly known as emergent abilities. (Wei et al., 2022a). ","Even more astonishingly, our research discovered that only GPT-4 nearly flawlessly handles inputs with abnormal errors, an assignment that presents considerable obstacles for other LLMs and frequently even for people. Precisely, GPT-4 can almost perfectly rebuild the original sentences from jumbled ones, lowering the edit distance by 95%, even when all letters within each word are totally scrambled. It is counterintuitive that LLMs can exhibit such resilience despite extreme disruption to input tokenization produced by garbled text. Large language models (LLMs) demonstrate remarkable competence across a range of tasks, with certain capabilities surfacing as the models increase in size—a phenomenon commonly referred to as emergent abilities. (Wei et al., 2022a).","More astoundingly, we found that only GPT-4 almost perfectly processes inputs with unnatural mistakes, a job that poses big challenges for other LLMs and often even for humans. Specifically, GPT-4 can nearly perfectly reconstruct the original sentences from mixed up ones, reducing the edit distance by 95%, even when all letters within each word are completely scrambled. It is counterintuitive that LLMs can show such resilience despite severe disruption to input tokenization caused by garbled text. Large language models (LLMs) demonstrate impressive skill across a range of tasks, with certain capabilities arising as the models scale up in size—a phenomenon commonly known as emergent abilities. (Wei et al., 2022a).  ","Even more surprisingly, our research discovered that only GPT-4 almost flawlessly handles inputs with abnormal errors, a task that presents significant obstacles for other LLMs and frequently even for people. In particular, GPT-4 can nearly perfectly rebuild the original sentences from jumbled ones, lowering the edit distance by 95%, even when all letters within each word are entirely scrambled. It is counterintuitive that LLMs can display such resilience despite extreme disruption to input tokenization caused by scrambled text. Large language models (LLMs) demonstrate remarkable competence across a range of tasks, with certain capabilities coming up as the models increase in size—a phenomenon commonly referred to as emergent abilities. (Wei et al., 2022a).",A,0
296,"Explain, Edit, Generate","Claim generation can also be done by very large language models (LLMs) (e.g., ChatGPT (OpenAI, 2022)) with in-context learning (Brown et al., 2020; Wei et al., 2022). However, since our editing may introduce inconsistencies with common sense, we empirically find that the edited evidence E′ is more likely to conflict with the internal knowledge of LLMs, thus leading to the irrelevant content or even failure in generating the claim c ′ . Thus, we choose the fine-tuned generation models.","Idea formulation can also be accomplished by very large language models (LLMs) (for example, ChatGPT (OpenAI, 2022)) with contextual learning (Brown et al., 2020; Wei et al., 2022). However, since our editing may introduce inconsistencies with common sense, we find through experience that the edited evidence E′ is more likely to conflict with the internal knowledge of LLMs, thus resulting in irrelevant content or even failure in formulating the claim c′. Therefore, we opt for the fine-tuned generative models.","Proposition development can also be done by very sizable language models (LLMs) (like ChatGPT (OpenAI, 2022)) with in-context learning (Brown et al., 2020; Wei et al., 2022). But because our editing can introduce conflicts with common sense, we empirically see that the edited proof E′ is more probable to disagree with the intrinsic knowledge of LLMs, thus causing the irrelevant content or even inability in forming the proposition c′. Hence, we choose the fine-tuned generation models.  ","Thesis formulation can also be accomplished by very large language models (LLMs) (such as ChatGPT (OpenAI, 2022)) using in-context learning (Brown et al., 2020; Wei et al., 2022). However, since our editing may introduce inconsistencies with common sense, we find through experience that the edited evidence E′ is more likely to contradict the inherent knowledge of LLMs, thereby resulting in irrelevant content or even failure in generating the thesis c′. Therefore, we opt for the fine-tuned generative models.",A,0
557,Non-autoregressive Text Editing with Copy-aware Latent Alignments,"For consistency, we use Geva et al. (2019)’s implementation2 to compute SARI. Results are listed in Table 4. We can see that our model surpasses all non-autoregressive works significantly, especially EDIT5, by more than 1 point EM score. One key observation is that 10.5% out of 4.5M fusion examples require source reordering. LaserTagger (Malmi et al., 2019) deals with this by defining a SWAP operation while EDIT5 uses pointer networks instead. The results indicate that our model is capable of doing reordering implicitly and thus handles the fusion task skillfully. On the other hand, our model achieves final EM/SARI scores of 66.0/90.7, showing strong competitiveness with the best performing RoBERTashare (66.6/90.3). ","For consistency, we utilize Geva et al. (2019)'s implementation to calculate SARI. The results are shown in Table 4. We can observe that our model significantly surpasses all non-autoregressive approaches, especially EDIT5, by over 1 point in EM score. One key finding is that 10.5% of the 4.5M fusion examples need source reordering. LaserTagger (Malmi et al., 2019) handles this by defining a SWAP operation while EDIT5 uses pointer networks. The results demonstrate that our model is capable of doing reordering implicitly and thus adeptly handles the fusion task. Furthermore, our model achieves final EM/SARI scores of 66.0/90.7, exhibiting strong competitiveness with the top performing RoBERTashare (66.6/90.3).","To be consistent, we use Geva et al. (2019)'s implementation to compute SARI. The outcomes are presented in Table 4. We notice that our model substantially exceeds all non-autoregressive methods, especially EDIT5, by more than 1 point in EM score. One important observation is that 10.5% of the 4.5M fusion examples need source reordering. LaserTagger (Malmi et al., 2019) addresses this by defining a SWAP operation whereas EDIT5 uses pointer networks. The results show that our model can do reordering implicitly and thus skillfully handles the fusion task. Additionally, our model achieves final EM/SARI scores of 66.0/90.7, displaying strong competitiveness with the best performing RoBERTashare (66.6/90.3).  ","To maintain consistency, we utilize Geva et al. (2019)'s implementation to calculate SARI. The numbers are presented in Table 4. We notice that our model surpasses all non-autoregressive approaches significantly, especially EDIT5, by over 1 point in EM score. One important observation is that 10.5% of the 4.5M fusion examples require source reordering. LaserTagger (Malmi et al., 2019) tackles this by defining a SWAP operation while EDIT5 uses pointer networks. The results indicate that our model can do reordering implicitly and thus adeptly handles the fusion task. Furthermore, our model achieves final EM/SARI scores of 66.0/90.7, exhibiting strong competitiveness with the top performing RoBERTashare (66.6/90.3).",A,0
448,LLM-enhanced Self-training for Cross-domain Constituency Parsing,"Traditional methods struggle to construct fine-grained sentences that facilitate knowledge transfer. The Large Language Model (LLM), with its powerful generative capabilities, can serve as a potential solution to the challenge of the raw corpus quantity and quality for the target domain (as shown in Figure 1). It’s important to note that our experiments revealed that LLMs exhibit limited performance for constituency parsing. To tackle the challenges of LLMs’ flexibility and hallucination problems (Bang et al., 2023; Manakul et al., 2023) in generating sentences, we employ grammar rules as instructions for LLMs to generate target domain sentences. ","Conventional techniques have difficulty producing detailed sentences that enable knowledge transfer. The Large Language Model (LLM), with its strong generative abilities, could address the issue of insufficient high-quality data in the target area (as Figure 1 shows). Notably, our tests showed LLMs aren't very good at constituency parsing. To handle LLMs' issues with flexibility and making things up (Bang et al., 2023; Manakul et al., 2023) when generating sentences, we use grammar rules to guide LLMs in creating sentences for the target domain.","Traditional approaches struggle to build fine-grained sentences that help convey knowledge. The Large Language Model (LLM), with its powerful ability to generate content, may solve the problem of limited quantity and quality of raw data for the desired domain (Figure 1 illustrates this). Importantly, we found LLMs have limited constituency parsing performance. To address LLMs' problems with flexibility and fabrication (Bang et al., 2023; Manakul et al., 2023) when producing sentences, we utilize grammar rules as instructions for LLMs to generate sentences for the target domain.  ","Established techniques have difficulty constructing detailed sentences that enable knowledge transfer. The Large Language Model (LLM), with its robust generative capabilities, could be a solution to the challenges of insufficient and low-quality raw data for the desired domain (shown in Figure 1). Notably, our experiments showed LLMs have limited constituency parsing abilities. To tackle LLMs' issues with flexibility and making things up (Bang et al., 2023; Manakul et al., 2023) when generating sentences, we use grammar rules to guide LLMs to produce sentences for the target domain.",A,0
581,RESEE,"We take masked response prediction as the main training task to make the model aware of appropriate responses with multimodal dialogue context.  In detail, we first initialize it with UNILM (2019).  During training, 70% of the responses are replaced by a special token [MASK] or another token in the vocabulary.  The masked response is denoted as ˆR.  In detail, we use the unmasked dialogue information [X,R\ˆR] to predict ˆR.  Besides, we also follow Liang et al.  (2021) to consider entity knowledge bias when decoding.  Inspired by recent progress in language generative methods (Dong et al., 2019;Wang et al., 2021), for both types of models, we process the encoder input with bi-directional attention, while giving the decoder output causal attention masks.  This masking strategy makes sure our models fully understand dialogue contexts and autoregressively generate tokens with learned knowledge. ","We utilize masked response prediction as the primary training objective to make the model cognizant of suitable responses using multimodal dialogue context. Specifically, we first initialize it with UNILM (2019). During training, 70% of the responses are substituted with a special token [MASK] or another token in the vocabulary. The masked response is denoted as R̂. In particular, we employ the unmasked dialogue information [X,R\R̂] to predict R̂. Furthermore, we also follow Liang et al. (2021) to take into account entity knowledge bias during decoding. Inspired by recent advances in language generative methods (Dong et al., 2019; Wang et al., 2021), for both model types, we process the encoder input using bi-directional attention, while applying causal attention masks to the decoder output. This masking approach ensures our models fully comprehend dialogue contexts and generate tokens autoregressively with acquired knowledge.","We use masked response prediction as the primary training task to make the model cognizant of suitable responses given multimodal dialogue context. Specifically, we first initialize it with UNILM (2019). During training, 70% of the responses are substituted with a special token [MASK] or another token in the vocabulary. The masked response is denoted R̂. In detail, we utilize the unmasked dialogue information [X,R\R̂] to predict R̂. Additionally, we also follow Liang et al. (2021) to account for entity knowledge bias during decoding. Inspired by recent advances in language generative methods (Dong et al., 2019; Wang et al., 2021), for both model types, we process the encoder input bidirectionally while applying causal attention masks to the decoder output. This masking approach ensures our models fully comprehend dialogue contexts and generate tokens autoregressively using acquired knowledge.","We use masked response prediction as the main training objective to make the model aware of suitable responses using multimodal dialogue context. Specifically, we first initialize it with UNILM (2019). During training, 70% of the responses are substituted with a special token [MASK] or another token in the vocabulary. The masked response is denoted R̂. In particular, we use the unmasked dialogue information [X,R\R̂] to predict R̂. Additionally, we also follow Liang et al. (2021) to consider entity knowledge bias when decoding. Inspired by recent progress in language generative methods (Dong et al., 2019; Wang et al., 2021), for both model types, we process the encoder input bidirectionally while applying causal attention masks to the decoder output. This masking strategy ensures our models fully comprehend dialogue contexts and generate tokens autoregressively using learned knowledge.",A,0
684,Standardizing Distress Analysis,"A.4.1 BiRNN-Attention The only difference between this model and the BiRNN model is the addition of an attention layer (Liu and Lane, 2016) after the sequential layer.  In order to further train the attention layer outputs, we calculate the cross entropy loss between the attention layer output and the ground truth attention.  A.4.2 CNN-GRU Zhang et al. (2018) employed CNN-GRU to achieve state-of-the-art on several hate speech datasets.  We add convolutional 1D filters of window sizes 2, 3, and 4, with 100 filters per size, to the existing architecture.  We employ the GRU layer for the RNN component and max-pool the hidden layer output representation.  This hidden layer is routed via a fully connected layer to yield prediction logits. ","The sole distinction between this architecture and the BiRNN configuration is appending an attention mechanism (Liu and Lane, 2016) subsequent to the sequential stratum. For enhanced tuning of the attention layer outputs, we determine the cross entropy loss amid the attention layer emission and the factual attention.  ","This model is identical to the BiRNN except for adjoining an attention layer (Liu and Lane, 2016) succeeding the sequential layer. To further optimize the attention layer outputs, we compute the cross entropy loss between the attention layer yield and the authentic attention.","The only variation between this framework and the BiRNN design is introducing an attention component (Liu and Lane, 2016) after the sequential tier. To promote further learning of the attention layer outputs, we evaluate the cross entropy loss between the attention layer product and the genuine attention.",A,0
654,Standardizing Distress Analysis,"The system employs a zero-shot strategy to dynamically incorporate emotional information into training and presents a novel fusion mechanism to infuse the features from the multimodal inputs.  The overall architecture of the proposed method is shown in Figure 3a.  4.1 Problem Formulation Given a post P = [s1, ・ ・ ・ si ・ ・ ・ , sp] composed of a sequence of sentences (s), and each utterance can be further decomposed into a sequence of words.  p indicates the number of sentences in the post. ","The approach uses an unsupervised technique to dynamically integrate emotional data into learning and puts forward a new fusion process to combine the characteristics from the multisensory inputs. The overall design of the suggested technique is displayed in Figure 3a. Formulating the Issue With a post P = [s1, ・ ・ ・ si ・ ・ ・ , sp] made up of a sequence of sentences (s), where each statement can be further broken down into a series of words. p denotes the quantity of sentences in the post.","The framework employs a zero-training strategy to dynamically include affective clues into education and introduces a novel integration mechanism to infuse the attributes from the cross-modal entries. The full structure of the proposed approach is illustrated in Figure 3a. Defining the Problem Given a post P = [s1, ・ ・ ・ si ・ ・ ・ , sp] consisting of a series of sentences (s), and each expression can be further decomposed into a sequence of words. p indicates the total sentences in the post.","The model uses an unsupervised approach to dynamically incorporate emotional data into training and presents a new fusion process to combine the features from the multimodal inputs. The complete architecture of the suggested technique is shown in Figure 3a. Formulating the Issue With a post P = [s1, ・ ・ ・ si ・ ・ ・ , sp] composed of a sequence of sentences (s), where each statement can be further broken down into a series of words. p denotes the number of sentences in the post.",A,0
431,Eliminating Lipschitz Singularities in Diffusion Models,"To ensure the generalizability of E-TSDM beyond specific settings of DDPM (Ho et al., 2020), we conduct a thorough investigation of E-TSDM on other popular noise schedules, as well as implement a continuous-time version of E-TSDM. Specifically, we explore the three popular ones including linear, quadratic and cosine schedules (Nichol & Dhariwal, 2021), and two newly proposed ones, which are cosine-shift (Hoogeboom et al., 2023) and zero-terminal-SNR (Lin et al., 2023) schedules.","In order to demonstrate that E-TSDM can be applied more broadly beyond the particular settings of DDPM (Ho et al., 2020), we have undertaken a comprehensive examination of E-TSDM using other common noise schedules, and have also implemented a continuous version of E-TSDM. In particular, we have tested E-TSDM on three widely used schedules - linear, quadratic, and cosine (Nichol & Dhariwal, 2021) - as well as two newly proposed schedules, cosine-shift (Hoogeboom et al., 2023) and zero-terminal-SNR (Lin et al., 2023).","To show that E-TSDM is not limited only to the specific configurations of DDPM (Ho et al., 2020), we have conducted an exhaustive study applying E-TSDM to additional popular noise schedules, and created a continuous-time form of E-TSDM. We tested E-TSDM on three common schedules - linear, quadratic, and cosine (Nichol & Dhariwal, 2021) - and two newly created schedules - cosine-shift (Hoogeboom et al., 2023) and zero-terminal-SNR (Lin et al., 2023). ","In order to demonstrate the wide applicability of E-TSDM outside of the particular DDPM (Ho et al., 2020) settings, we have performed a comprehensive evaluation of E-TSDM using various other prevalent noise schedules, and also developed a continuous version of E-TSDM. Specifically, we evaluated E-TSDM on three widely used schedules: linear, quadratic, and cosine (Nichol & Dhariwal, 2021), as well as two newly introduced schedules: cosine-shift (Hoogeboom et al., 2023) and zero-terminal-SNR (Lin et al., 2023).",A,0
120,Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering,"We explore the impact of the amount of warmup data on Anti-CF. We use xlmr-large as the source model on the NoiseQA-syn dataset and conducted experiments with different amounts of warmup data. We randomly sample the warmup data from the validation set of SQuAD. As shown in Figure 6, even if we do not perform warmup, Anti-CF still achieves performance over direct inference, which may be because source constraints make the efficient side block learn the knowledge from the source model in the process of TTA.","We analyze the effect of varying quantities of warmup information on Anti-CF. We utilize xlmr-large as the base model on the NoiseQA-syn dataset and performed tests with assorted warmup data amounts. We arbitrarily choose the warmup data from the validation set of SQuAD. As depicted in Figure 6, even without warmup, Anti-CF still surpasses direct inference, possibly because source constraints enable the efficient side block to acquire knowledge from the source model during TTA.","We inspect the influence of the volume of warmup statistics on Anti-CF. We employ xlmr-large as the originating model on the NoiseQA-syn data and conducted trials with differing warmup data sums. We randomly take the warmup statistics from the validation collection of SQuAD. As exhibited in Figure 6, even lacking warmup, Anti-CF still achieves superior performance over direct deduction, potentially since source limitations make the efficient side unit gain understanding from the source model while TTA transpires. ","We analyze the impact of the amount of warmup information on Anti-CF. We use xlmr-large as the source model on the NoiseQA-syn data and performed experiments with various amounts of warmup data. We randomly select the warmup data from the validation set of SQuAD. As shown in Figure 6, even without warmup, Anti-CF still surpasses direct inference, possibly because source constraints enable the efficient side module to learn knowledge from the source model during TTA.",A,0
86,ALDi Quantifying the Arabic Level of Dialectness of Text,"We use an off-the-shelf DI model implemented in (Obeid et al., 2020) based on (Salameh et al., 2018). The model is based on Naive Bayes, trained on the MADAR corpus (Bouamor et al., 2018), and uses character and word n-grams to classify a sentence into 6 variants of DA in addition to MSA. A sentence is assigned an ALDi score of 0 if it is classified as MSA and a score of 1 otherwise.","We utilize a ready-made dialect identification model created as described in (Obeid et al., 2020) following the approach of (Salameh et al., 2018). The model employs Naive Bayes, trained on the MADAR dataset (Bouamor et al., 2018), and leverages character and word n-grams to categorize a sentence into 6 dialectal Arabic varieties besides Modern Standard Arabic. A sentence gets an ALDi score of 0 if labeled as Modern Standard Arabic and 1 otherwise.","We make use of a pre-built dialect identification model developed as per (Obeid et al., 2020) based on the work of (Salameh et al., 2018). This model uses Naive Bayes, is trained on the MADAR corpus (Bouamor et al., 2018), and utilizes character and word n-grams to classify a sentence into 6 dialects of Arabic in addition to Modern Standard Arabic. A sentence is given an ALDi score of 0 if identified as Modern Standard Arabic and an ALDi score of 1 if identified as dialectal Arabic.  ","We employ an existing dialect identification model implemented as described in (Obeid et al., 2020) following (Salameh et al., 2018). The model utilizes Naive Bayes, is trained on the MADAR data set (Bouamor et al., 2018), and makes use of character and word n-grams to categorize a sentence into 6 varieties of dialectal Arabic as well as Modern Standard Arabic. A sentence receives an ALDi score of 0 if determined to be Modern Standard Arabic and an ALDi score of 1 if determined to be dialectal Arabic.",A,0
655,Standardizing Distress Analysis,"The objective is to determine if the post is distressed or not (0 or 1) and to extract every plausible causal span that supports the prediction.  4.2 Proposed DICE Framework Textual Encoder.  Our textual encoder uses BERT followed by an ontology-based word graph.  BERT extracts local information from a text.  Ontology is the backbone of knowledge graphs (KGs) (Song et al., 2022), which give meta-data descriptions to guide the creation and completion of knowledge graphs.  Additionally, relation descriptions contain semantic information that can be used to represent relations.  During Graph Neural Network (GNN) message transmission, we embed text within ontology nodes. ","The goal is to figure out if the post conveys distress or not (0 or 1) and to extract every possible causal phrase that backs up the prediction. Our text encoder utilizes BERT followed by a word graph based on ontology. BERT pulls out local information from text. Ontology forms the foundation of knowledge graphs (KGs) (Song et al., 2022), which provide metadata descriptions to direct the building and completion of knowledge graphs. Also, relation descriptions have semantic data that can represent relations. When transmitting Graph Neural Network (GNN) messages, we embed text inside ontology nodes.","The aim is to determine if the post is troubled or not (0 or 1) and to pull out every plausible causal chunk that supports the forecast. Our textual encoder employs BERT and then an ontology-based word graph. BERT extracts localized information from text. Ontology is the backbone of knowledge graphs (KGs) (Song et al., 2022), which give metadata descriptions to guide the creation and filling out of knowledge graphs. Additionally, relation descriptions hold semantic data that can represent relations. During Graph Neural Network (GNN) message passing, we embed text within ontology nodes.","The purpose is to conclude if the post is anguished or not (0 or 1) and to extract every possible causal phrase that validates the prediction. Our text encoder utilizes BERT followed by a word graph based on ontology. BERT pulls out localized information from a text. Ontology forms the backbone of knowledge graphs (KGs) (Song et al., 2022), which provide meta-data descriptions to direct the building and completion of knowledge graphs. Also, relation descriptions contain semantic information that can represent relations. When transmitting Graph Neural Network (GNN) messages, we embed text inside ontology nodes.",A,0
78,ALDi Quantifying the Arabic Level of Dialectness of Text,"For comments labeled as Non-MSA, the annotators also chose the dialect in which the text is written: EGY, LEV, GLF, Maghrebi (MAG), Iraqi (IRQ), General (GEN: used when the text is DA, but could belong to multiple dialects), Unfamiliar, and Other. Each row of the released AOC dataset consists of 12 different sentences representing a Human Intelligence Task (HIT) on Amazon mTurk, with annotations provided by the same human judge. A HIT has 10 comments in addition to 2 control sentences sampled from the articles’ bodies, which are expected to be mostly written in MSA. As part of each HIT, annotators provided some personal information such as their place of residence, whether they are native Arabic speakers, and the Arabic dialect they understand the most.","For remarks labeled as not Modern Standard Arabic, the evaluators also selected the language variety in which the text was authored: Egyptian Arabic, Levantine Arabic, Gulf Arabic, Maghrebi Arabic, Iraqi Arabic, General Arabic (used when the text is dialectal Arabic, but could fit multiple varieties), Unfamiliar, and Other. Each line of the published AOC data set is made up of 12 distinct utterances representing a Human Intelligence Task (HIT) on Amazon Mechanical Turk, with assessments given by the same human rater. A HIT contains 10 comments as well as 2 control sentences taken from the articles' bodies, which are expected to largely be written in Modern Standard Arabic. As part of each HIT, evaluators gave some personal details like where they live, if they are native Arabic speakers, and the Arabic dialect they understand best.","For feedback tagged as not Modern Standard Arabic, the appraisers also selected the language variety in which the text was penned: Egyptian, Levantine, Gulf, Maghrebi, Iraqi, General (used when the text is dialectal Arabic, but could belong to multiple varieties), Unfamiliar, and Other. Each row of the published AOC data set consists of 12 separate statements representing a Human Intelligence Task (HIT) on Amazon Mechanical Turk, with ratings provided by the same human assessor. A HIT has 10 comments as well as 2 control sentences extracted from the articles' bodies, which are anticipated to be largely authored in Modern Standard Arabic. As part of each HIT, appraisers gave some personal information such as their residence, whether they are native Arabic speakers, and the Arabic dialect they comprehend best.  ","For remarks tagged as not Modern Standard Arabic, the evaluators also chose the language variety in which the text was written: Egyptian, Levantine, Gulf, Maghrebi, Iraqi, General (used when the text is dialectal Arabic, but could fit multiple varieties), Unfamiliar, and Other. Each line of the released AOC data set is composed of 12 distinct utterances representing a Human Intelligence Task (HIT) on Amazon Mechanical Turk, with ratings provided by the same human judge. A HIT contains 10 comments as well as 2 control sentences taken from the articles' bodies, which are expected to be largely penned in Modern Standard Arabic. As part of each HIT, evaluators provided some personal details like their place of living, if they are native Arabic speakers, and the Arabic dialect they understand the most.",A,0
377,INSTRUCTSCORE,"Then, we prompt GPT-4 to synthesize designated generation errors, as shown in Table 1. For each text, we specify the number of errors, error types, and severity labels, and ask GPT-4 to generate a candidate output with the specified error descriptions and 2) an explanation for this error annotation. If an evaluation task is multi-dimensional, error types will be separately assigned to each dimension (An example is included in the Appendix). Benefiting from the large-scale pre-training process, GPT-4 is able to generate diverse errors and meet the requirements with specified instructions.","Next, we instruct GPT-4 to produce particular creation mistakes, as displayed in Table 1. For each section of text, we indicate the quantity of errors, error kinds, and severity tags, and request GPT-4 to create a possible output with the given error explanations and 2) a clarification for this error notation. If an assessment task has multiple dimensions, error types will be individually allocated to each dimension (An illustration is incorporated in the Appendix). Capitalizing on the large-scale pre-training procedure, GPT-4 can generate varied errors and satisfy the prerequisites with particular guidelines.","Subsequently, we prompt GPT-4 to synthesize intended generation flaws, as exhibited in Table 1. For every passage, we establish the number of mistakes, error varieties, and severity labels, and direct GPT-4 to construct a candidate yield with the defined error descriptions and 2) an elucidation for this error annotation. If an evaluation task has multiple aspects, error types will be separately assigned to each aspect (An instance is contained in the Appendix). Benefiting from the extensive pre-training process, GPT-4 is capable of producing diverse errors and fulfill the requirements with specified instructions.","After that, we cue GPT-4 to produce planned creation defects, as shown in Table 1. For every excerpt, we stipulate the quantity of flaws, error kinds, and severity tags, and instruct GPT-4 to generate a possible output with the stated error explanations and 2) a clarification for this error notation. If an assessment task is multidimensional, error types will be individually allocated to each dimension (An example is incorporated in the Appendix). Capitalizing on the large-scale pre-training procedure, GPT-4 can create varied errors and satisfy the specifications with particular guidelines.",A,0
782,VECHR,"The assessment of the ethical implications of the dataset is based on the Data Statements by Bender and Friedman (2018). Through this, we aim to establish transparency and a more profound understanding of limitations and biases. The curation is limited to the Article 3 documents in English. The speaker and annotator demographic are legally trained scholars, proficient in the English language. “Speaker” here refers to the authors of the case documents, which are staff of the Court, rather than applicants. We do not believe that the labelling of vulnerable applicants is harmful because it is done from a legally theoretical perspective, intending to support applicants. ","The review of the moral issues of the data is founded on the Data Statements by Bender and Friedman (2018). Through this, we seek to build openness and a deeper grasp of constraints and prejudices. The curation is constrained to the Article 3 documents in English. The speaker and annotator people are legally educated academics, skilled in the English tongue. ""Speaker"" here mentions the writers of the case papers, who are employees of the Court, rather than petitioners. We do not think that the marking of susceptible petitioners is detrimental because it is done from a legally theoretical view, wanting to assist petitioners.","The evaluation of the ethical consequences of the data relies on the Data Statements by Bender and Friedman (2018). In doing so, we aim to create transparency and a more profound comprehension of limits and biases. The collection is limited to the Article 3 documents in English. The speaker and tagger population are people trained in law, fluent in English. ""Speaker"" here indicates the authors of the case documents, who are staff of the Court, not applicants. We do not believe the identification of vulnerable applicants is harmful because it is done from a legal theoretical lens, with the goal of aiding applicants.  ","The appraisal of the moral implications of the information depends on the Data Statements by Bender and Friedman (2018). Through this, we seek to build openness and a deeper understanding of constraints and prejudices. The gathering is constrained to the Article 3 documents in English. The speaker and labeler people are legally educated scholars, fluent in English. ""Speaker"" here refers to the writers of the case papers, who are employees of the Court, not petitioners. We do not think the identification of susceptible petitioners is detrimental because it is done from a legal theoretical perspective, with the intent of supporting petitioners.",A,0
650,Standardizing Distress Analysis,"Please refer to Appendix Section A.1 for details on data collection from Gab, including keywords used for the DCaM dataset (see Table 8).  To compile this data, we scoured Gab for posts between November and December 2022.  Posts that have been deleted and reposted are not considered.  We also remove links from posts to ensure that annotators can access all relevant information.  A number of distress datasets are compared in Table 1.  3.2 Data Annotation To ensure the dataset consists of only English posts, we used the TextBlob library for language detection and included only those identified as English. ","For information about how we gathered data from Gab for the DCaM dataset (refer to Table 8 for the keywords used), see Appendix A.1. We searched Gab for posts made between November and December 2022 to compile this, excluding deleted and reposted content. We also took out links from posts so annotators could access all pertinent info. Table 1 compares various distress datasets.  ","The details about how we collected data from Gab for the DCaM dataset (keywords in Table 8) are in Appendix Section A.1. We combed through Gab posts from November to December 2022 to put this together, ignoring posts that were deleted and reposted. We removed links from posts too, so annotators could see all important information. Table 1 shows how our distress dataset stacks up against others.","Refer to Appendix A.1 for specifics on how we obtained data from Gab for the DCaM dataset (keywords in Table 8). We searched Gab for posts between November and December 2022 to assemble this, not considering deleted and reposted content. We also stripped links from posts so annotators had access to all relevant details. Table 1 provides a comparison of multiple distress datasets.",A,0
49,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"In order to better understand our method, we further present some quantitative and qualitative analyses that provides some insights into why our method works and where future work could potentially improve it. One advantage of our framework is the efficient inference process benefiting from the well-parallelized design. We evaluate the speed by running the model one epoch on the BookRestaurant test data with batch size set to 32. Results in Table 3 show that our method achieves ×13.89 and ×7.06 speedup compared with the advanced metric-based method (i.e., LEONA) and QA-based method (i.e., SLMRC). This could be attributed to our batch-wise decoding in parallel.","To gain more insight into our approach, we conducted quantitative and qualitative analyses to elucidate why it is effective and where it could potentially be improved further. A key benefit of our framework is its efficient inference enabled by a highly parallelized architecture. We assessed the speed by running one epoch on the BookRestaurant test set with a batch size of 32. As shown in Table 3, our method achieved ×13.89 and ×7.06 speedup over the state-of-the-art metric-based (LEONA) and QA-based (SLMRC) methods, respectively. This substantial increase can be ascribed to our batch-wise decoding performed in parallel.","In order to better comprehend our technique, we present additional quantitative and qualitative studies that give perspective into the reasons for its efficacy and areas for future enhancement. One advantage of our framework is the fast inference process resulting from its well-parallelized design. We evaluate the velocity by executing the model for one cycle on the BookRestaurant test data using a batch dimension of 32. The outcomes in Table 3 exhibit that our approach attains ×13.89 and ×7.06 speedup versus the advanced metric-based technique (LEONA) and QA-based technique (SLMRC). This can be credited to our batch-wise decoding performed in parallel.","To gain deeper insight into our methodology, we provide further quantitative and qualitative analyses that shed light on why it is effective and where it could be improved moving forward. A benefit of our framework is its efficient inference enabled by a highly parallel architecture. We assess the speed by running the model for one epoch on the BookRestaurant test set with a batch size of 32. The results in Table 3 show our method achieves ×13.89 and ×7.06 speedup compared to state-of-the-art metric-based (LEONA) and QA-based (SLMRC) approaches. This considerable acceleration can be attributed to our batch-wise decoding in parallel.",A,0
77,ALDi Quantifying the Arabic Level of Dialectness of Text,"We define the Level of Dialectness of a sentence as the extent by which the sentence diverges from the standard language, which can be based on any of the cues described above. This definition is consistent with the crowd-sourced annotation of the Arabic Online Commentary (AOC) dataset (Zaidan and Callison-Burch, 2011), where annotators labeled user comments on Arabic newspaper articles by their dialect and their level of dialectness. However, the original and subsequent work only used the dialect labels, and the dialectness annotations have not previously been analyzed in detail.","We characterize the Level of Dialectness of a sentence as the degree to which the sentence differs from the standard language, which can draw on any of the indicators described previously. This characterization aligns with the crowdsourced tagging of the Arabic Online Commentary (AOC) dataset (Zaidan and Callison-Burch, 2011), where taggers classified user remarks on Arabic newspaper articles by their dialect and their level of dialectness. However, the initial and following work only utilized the dialect labels, and the dialectness annotations were not examined thoroughly before.","We define the Extent of Dialect Usage in a sentence as how far it strays from the formal language, based on the cues listed above. This matches the collaborative labeling of the Arabic Online Commentary (AOC) dataset (Zaidan and Callison-Burch, 2011), where volunteers categorized comments on Arabic news stories by their dialect and extent of dialect usage. But the early and later studies only employed the dialect categories, and did not closely analyze the dialect usage annotations.  ","We characterize the Degree of Colloquialism in a sentence as how much it diverges from the standard language, drawing on any of the markers described earlier. This aligns with the group-based tagging of the Arabic Online Commentary (AOC) dataset (Zaidan and Callison-Burch, 2011), where teams classified remarks on Arabic news articles based on their dialect and degree of colloquialism. However, the initial and follow-up studies only used the dialect tags, and did not thoroughly examine the colloquialism annotations.",A,0
231,Cultural Concept Adaptation on Multimodal Reasoning,"Pretrained multilingual multimodal models often demonstrate disparate performance across various languages and cultural contexts in test datasets, a discrepancy likely attributable to uneven resource distribution during pretraining. These imbalances pose challenges that are not readily addressed by self-supervised pretraining objectives. Our method offers a mechanism to ameliorate these language and cultural biases by manipulating the sampling distribution. Essentially, we can enhance model performance on specific language or cultural topics in a controllable manner. For instance, if the model is anticipated to be applied in a Turkish context, the sampling probability for Turkish can be increased during data augmentation.","Pre-trained models that use both language and visual inputs frequently show unequal abilities across different languages and cultures when evaluated, likely due to uneven use of resources when they were trained initially. These imbalances present problems not easily fixed by the self-supervised training methods used. Our approach provides a way to reduce these biases related to language and culture by changing how data is sampled. Basically, we can improve model performance for certain languages or cultures in a controlled way. For example, if the model will be used in Turkish contexts, the chance of sampling Turkish data can be increased during augmentation.","Models pretrained on multilingual and multimodal data often have inconsistent performance on different languages and cultures, probably because training data was unevenly distributed. These disparities are challenging to address just using self-supervised training. Our technique lets us mitigate language and cultural biases by modifying the data sampling. We can enhance model capabilities for specific languages or cultures in a targeted way. If the model will be applied in Turkish settings, we can increase the likelihood of sampling Turkish data during augmentation.","Models pre-trained on text and images in multiple languages frequently demonstrate unequal effectiveness on various languages and cultures when tested, likely due to imbalanced resource usage during pretraining. These inconsistencies present difficulties not easily resolved by self-supervised pretraining techniques. Our approach provides a means to reduce these language and cultural biases by changing the sampling distribution. We can improve model performance for particular languages or cultures in a controlled fashion. For example, if the model will be used in Turkish contexts, we can increase the probability of sampling Turkish data during augmentation.",A,0
785,VECHR,"Our research group is strongly committed to research on legal NLP models as a means to derive insight from legal data for purposes of increasing transparency, accountability, and explainability of data-driven systems in the legal domain. Here is the typology of vulnerability in ECtHR (Heri, 2021): • Dependency: dependency-based vulnerability, which concerns minors, the elderly, and those with physical, psychosocial and cognitive disabilities (i.e., mental illness and intellectual disability). • State Control: vulnerability due to state control, including vulnerabilities of detainees, military conscripts, and persons in state institutions. • Victimisation: vulnerability due to victimisation, including by domestic and sexual abuse, other violations, or because of a feeling of vulnerability. • Migration: vulnerability in the migration context, applies to detention and expulsion of asylum-seekers. ","Our research group strongly supports conducting legal natural language processing research to gain insights from legal data. This can improve the transparency, accountability, and explainability of data-driven systems in law. Here is a summary of vulnerable groups in ECtHR cases (Heri, 2021): • Reliance: those relying on others, like children, elderly, and people with disabilities. • Government Custody: vulnerable due to government control, like detainees, military draftees, and institutionalized persons. • Abuse: vulnerable due to victimization from domestic abuse, sexual abuse, or other violations. • Immigration: vulnerable in migration contexts like asylum detention and deportation.","Our research group is dedicated to using legal natural language processing to extract insights from legal data. Our goal is to increase the transparency, accountability, and explainability of data-driven legal systems. Here is a summary of types of vulnerability in ECtHR cases (Heri, 2021): • Dependence: vulnerability of those dependent on others, including minors, seniors, and people with disabilities. • State Authority: vulnerability of those under state control, like detainees, military recruits, and institutionalized persons. • Mistreatment: vulnerability due to mistreatment, including domestic abuse, sexual abuse, other violations, or feeling vulnerable. • Displacement: vulnerability in migration contexts like asylum seeker detention and removal.","Our research group strongly supports using legal natural language processing to gain insights from legal data. We aim to improve transparency, accountability, and explainability of data-driven legal systems. Here is a summary of vulnerable groups in ECtHR cases (Heri, 2021): • Reliance: vulnerability of dependents like children, elderly, and disabled. • State Control: vulnerability of those under state authority like detainees, conscripts, and institutionalized. • Abuse: vulnerability of abuse victims including domestic, sexual, other violations, or feeling vulnerable. • Migration: vulnerability in migration contexts including asylum seeker detention and deportation.",A,0
66,ALDi Quantifying the Arabic Level of Dialectness of Text,"We introduce the AOC-ALDi dataset (derived from the AOC dataset), containing 127,835 sentences (17% from news articles and 83% from user comments on those articles) which are manually labeled with their level of dialectness. We provide a detailed analysis of AOC-ALDi and show that a model trained on it can effectively identify levels of dialectness on a range of other corpora (including dialects and genres not included in AOC-ALDi), providing a more nuanced picture than traditional DI systems. Through case studies, we illustrate how ALDi can reveal Arabic speakers’ stylistic choices in different situations, a useful property for sociolinguistic analyses.","We present the AOC-ALDi dataset which has 127,835 sentences labeled with their degree of dialect usage. 17% of sentences are from news reports and 83% are from user remarks on those reports. We thoroughly analyze AOC-ALDi and demonstrate that a model trained on it can successfully identify degrees of dialect usage in other datasets, even for dialects and genres not in AOC-ALDi. This provides a more detailed view than traditional dialect identification systems. Through examples, we show how ALDi can uncover Arabic speakers' stylistic selections in various contexts, a helpful trait for sociolinguistic studies.","We introduce the AOC-ALDi dataset containing 127,835 sentences annotated with their level of dialect usage. 17% of the sentences originate from news articles while 83% are from user comments on the articles. We offer an in-depth analysis of AOC-ALDi and exhibit that a model trained on it can capably categorize levels of dialect usage across various other datasets, including unfamiliar dialects and genres absent from AOC-ALDi. This gives a more nuanced depiction than conventional dialect identification systems. Through case studies, we demonstrate how ALDi can uncover Arabic speakers' stylistic choices in different situations, a valuable capacity for sociolinguistic research.  ","We present the AOC-ALDi dataset with 127,835 sentences manually labeled with their degree of dialect usage. 17% of sentences are from news reports and 83% are from user comments on those reports. We provide a comprehensive analysis of AOC-ALDi and establish that a model trained on it can proficiently identify degrees of dialect usage across a variety of other datasets, even unfamiliar dialects and genres outside of AOC-ALDi. This gives a more subtle portrayal than traditional dialect ID systems. Through examples, we illustrate how ALDi can reveal Arabic speakers' stylistic selections in various contexts, a useful ability for sociolinguistic analysis.",A,0
260,DisCo Distilled Student Models Co-training for Semi-supervised Text Mining,"For an ablation comparison of DisCo, we introduce the variant of DisCo only equipped with the model view, shown in Figure 1(b). In this variant, labelled and unlabeled data are duplicated and would be fed to the students directly. DisCo and its variant ensure reciprocal collaboration among the distilled students and can enhance the generalization ability of the student cohort by the consistency constraint. In this section, we introduce DisCo from two aspects: knowledge distillation and the co-training strategy","To analyze the impact of DisCo, we present a modified version that only has the model view shown in Figure 1(b). In this changed form, called DisCo-variant, the labeled and unlabeled data is copied and given straight to the student models. Both DisCo and DisCo-variant enable cooperative learning between the student models and can improve their generalizability through consistent constraints. We now explain DisCo's design regarding knowledge distillation and co-training.","For benchmarking DisCo, we describe an altered form with just the model view in Figure 1(b), termed DisCo-alt. Here, the labeled and unlabeled samples are duplicated and directly input to the students. DisCo and DisCo-alt facilitate collaborative learning between the student models and enhance their adaptability through uniformity constraints. We now elucidate two key aspects of DisCo: distillation and co-training.  ","To evaluate DisCo, we present a modified version having only the model view shown in Figure 1(b), called DisCo-mod. In DisCo-mod, the labeled and unlabeled data is copied and fed straight into the student models. Both DisCo and DisCo-mod enable cooperative distillation among the students and can boost their flexibility through consistency constraints. We now explain DisCo regarding knowledge transfer and co-learning.",A,0
662,Standardizing Distress Analysis,"Reconstruction Loss:  An auto-encoder reconstructs adjective-noun pair (ANP) features6 and produces latent features while maintaining emotion information in the learned latent space to match label and ANP feature structures.  By optimizing the following loss function, the auto-encoder input (A) and output (Aˆ) must be sufficiently close to identify its parameters.   Also, optimizing this loss results in lowerdimensional input features and high-accuracy feature reconstruction.  Adversarial loss:  Our objective is to maintain the discriminative capacity of the combined feature of the text and visual i.e.A(IMF(a, t)), and combine it with the rich emotional structural data contained in feature ϕ(lemo).  This is accomplished by using an adversarial restriction that seeks to trick a discriminator network D such that the output A(IMF(a, t)) features are as comparable as the ANP features: ","The autoencoder rebuilds descriptive word pairs and generates hidden representations while keeping emotion data in the learned hidden space to match label and descriptive word pair structures. By enhancing the following loss function, the autoencoder input and output must be adequately close to determine its variables. Also, optimizing this loss results in lower-dimension input features and high-precision feature rebuilding. Adversarial loss: Our goal is to maintain the discriminative ability of the combined feature of the text and visual i.e. A(IMF(a, t)), and combine it with the rich emotional structural information contained in feature φ(lemo). This is accomplished by using an adversarial constraint that attempts to deceive a discriminator network D such that the output A(IMF(a, t)) features are as similar as the descriptive word pair features.","The autoencoder reconstructs attribute-noun pairs and generates latent representations while retaining emotion data in the acquired latent space to align label and attribute-noun pair structures. Through enhancing the following loss function, the autoencoder input and output must be sufficiently close to pinpoint its parameters. Also, optimizing this loss yields lower-dimensional input features and high-fidelity feature reconstruction. Adversarial loss: Our aim is to maintain the discriminative power of the combined feature of the text and visual i.e. A(IMF(a, t)), and unite it with the rich emotional structural knowledge contained in feature φ(lemo). This is realized by employing an adversarial constraint that attempts to mislead a discriminator network D such that the output A(IMF(a, t)) features are as comparable as the attribute-noun pair features.  ","The autoencoder rebuilds adjective-noun pairs and forms latent features while keeping emotion information in the learned latent space to align label and adjective-noun pair structures. By enhancing the following loss function, the autoencoder input and output must be adequately similar to identify its variables. Also, optimizing this loss yields lower-dimension input features and high-accuracy feature reconstruction. Adversarial loss: Our objective is to maintain the discriminative ability of the combined feature of the text and visual i.e. A(IMF(a, t)), and fuse it with the rich emotional structural data contained in feature φ(lemo). This is realized by utilizing an adversarial restriction that tries to mislead a discriminator network D such that the output A(IMF(a, t)) features are as comparable as the adjective-noun pair features.",A,0
436,Eliminating Lipschitz Singularities in Diffusion Models,"In this paper, we elaborate on the infinite Lipschitz of the diffusion model near the zero point from both theoretical and empirical perspectives, which hurts the stability and accuracy of the diffusion process. A novel E-TSDM is further proposed to mitigate the corresponding singularities in a timestep-sharing manner. Experimental results demonstrate the superiority of our method in both performance and adaptability to the baselines, including unconditional generation, conditional generation, and fast sampling. This paper may not only improve the performance of diffusion models, but also help to make up the critical research gap in the understanding of the rationality underlying the diffusion process.","This paper expands on the infinite Lipschitz constant of the diffusion model around the origin, looking at it from theoretical and experimental viewpoints. This harms the stability and precision of diffusion. We also introduce a new E-TSDM method to reduce the singularities in a way that shares timesteps. Tests show our method is better than existing ones at unconditional generation, conditional generation, and fast sampling, both in performance and adaptability. This research could enhance diffusion models and help fill an important gap in comprehending the logic of diffusion.","In this paper, we provide more details on the infinite Lipschitz constant of the diffusion model at the zero point, analyzing it theoretically and empirically. This damages the stability and accuracy of diffusion. We also present a new E-TSDM technique to mitigate the singularities by sharing timesteps. Experiments prove our method surpasses baselines like unconditional generation, conditional generation, and fast sampling in both capabilities and adaptability. This paper has the potential to not only improve diffusion models, but also address a critical research gap in understanding the reasoning behind diffusion.","This paper expands on the infinite Lipschitz constant of the diffusion model at the origin using theoretical and experimental perspectives, which harms diffusion stability and accuracy. We introduce a novel E-TSDM approach to reduce singularities in a timestep-sharing manner. Tests show our method outperforms baselines including unconditional generation, conditional generation, and fast sampling in performance and adaptability. This research could enhance diffusion models and fill a key research gap in comprehending the logic behind diffusion.",A,0
525,Neural Fine-Tuning Search for Few-Shot Learning,"In FSL, Efficient Transformer Tuning (ETT) (Xu et al., 2022) apply a similar strategy to few-shot ViT adaptation using a DINO (Caron et al., 2021) pre-trained backbone. PMF (Hu et al., 2022), FLUTE (Triantafillou et al., 2021) and FT (Dhillon et al., 2020) focus on adapting existing parameters without inserting new ones. To manage the adaptation/overfitting tradeoff in the few-shot regime, PMF fine-tunes the whole ResNet or ViT backbone, but with carefully managed learning rates. Meanwhile, FLUTE hand-picks a set of FILM parameters with a modified ResNet backbone for few-shot fine-tuning, while keeping the majority of the feature extractor frozen.","In FSL, Efficient Transformer Tuning (ETT) (Xu et al., 2022) utilize a comparable approach to adapting ViT models pre-trained with DINO (Caron et al., 2021) using only a few examples. PMF (Hu et al., 2022), FLUTE (Triantafillou et al., 2021) and FT (Dhillon et al., 2020) concentrate on tuning existing parameters without adding new ones. To balance adaptation and overfitting when only a few examples are available, PMF fine-tunes the whole ResNet or ViT backbone, but with carefully controlled learning rates. In contrast, FLUTE selects a specific set of FILM parameters along with a modified ResNet backbone for tuning with few examples, while keeping most of the feature extractor frozen.","In few-shot learning, Efficient Transformer Tuning (ETT) (Xu et al., 2022) employs a similar tactic to adapting vision transformer models pre-trained with DINO (Caron et al., 2021) using minimal data. PMF (Hu et al., 2022), FLUTE (Triantafillou et al., 2021) and FT (Dhillon et al., 2020) aim to adjust existing parameters without introducing new ones. To balance adaptation and overfitting when data is scarce, PMF fine-tunes the entire ResNet or vision transformer backbone, but with carefully regulated learning rates. In contrast, FLUTE chooses specific FILM parameters along with an altered ResNet backbone to tune with minimal data, while keeping most of the feature extractor fixed.","In few-shot learning settings, Efficient Transformer Tuning (ETT) (Xu et al., 2022) takes a similar approach to adapting vision transformer models pre-trained with DINO (Caron et al., 2021) using very limited data. PMF (Hu et al., 2022), FLUTE (Triantafillou et al., 2021) and FT (Dhillon et al., 2020) focus on tuning existing parameters without introducing new ones. To balance adaptation and overfitting with scarce data, PMF fine-tunes the whole ResNet or vision transformer backbone, but with carefully controlled learning rates. Meanwhile, FLUTE selects specific FILM parameters along with a modified ResNet backbone to tune with minimal data, while keeping most of the feature extractor frozen.",A,0
30,A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We also observe the behavior of the Flan-T5 and Flan-T5Few-Shot models across stereotypical and anti-stereotypical examples. Using the few-shot debiasing in Spanish, the model demonstrates a higher success rate in correcting the profession translations associated with anti-stereotypical examples (235) compared to stereotypical examples (80) out of a total of 348 identified examples.","We also notice the actions of the Flan-T5 and Flan-T5Few-Shot models for stereotypical and opposite-of-stereotypical instances. Utilizing the small-data debiasing in Spanish, the model shows a higher achievement percentage in fixing the profession translations linked to opposite-of-stereotypical examples (235) versus stereotypical examples (80) out of a sum of 348 recognized instances.","We furthermore examine the conduct of the Flan-T5 and Flan-T5Few-Shot models over stereotypical and non-stereotypical cases. Leveraging the limited-data mitigating bias in Spanish, the model displays a superior success frequency in amending the job translations connected with non-stereotypical examples (235) compared with stereotypical examples (80) from a quantity of 348 spotted examples. ","We additionally inspect the actions of the Flan-T5 and Flan-T5Few-Shot models across stereotypical and non-stereotype examples. Utilizing the small-sample debiasing in Spanish, the model shows a higher success percentage in changing the profession translations linked to non-stereotype examples (235) versus stereotypical examples (80) out of a totality of 348 pinpointed examples.",A,0
682,Standardizing Distress Analysis,"All experiments are carried out on an NVIDIA GeForce RTX 2080 Ti GPU.  We conducted a grid search across 200 epochs.  We find empirically that our Embedding size is 812 bytes.  We use Adam (Kingma and Ba, 2015) for optimization.  The learning rate is 0.05, and the dropout is 0.5.  The auto-latent encoder’s dimension is fixed at 812.  The discriminator D consists of two completely linked layers and a ReLU layer and accepts 812-D input features.  Stochastic gradient descent has a learning rate of 1e-4 and a weight decay of 1e-3.  with a momentum of 0.5.  We perform 5 cross-validations of the DCaM dataset for training and testing purposes. ","All trials are done on an NVIDIA GeForce RTX 2080 Ti graphics card. We carried out a grid search over 200 epochs. We find through experience that our Embedding size is 812 bytes. We utilize Adam (Kingma and Ba, 2015) to optimize. The learning rate is 0.05, and the dropout is 0.5. The auto-latent encoder's size is fixed at 812. The discriminator D has two fully connected layers and a ReLU layer and takes 812-D input features. Stochastic gradient descent has a learning rate of 1e-4 and a weight decay of 1e-3 with a momentum of 0.5. We do 5 cross-validations of the DCaM dataset for training and testing.","All experiments are performed on an NVIDIA GeForce RTX 2080 Ti GPU. We conducted a parameter search across 200 epochs. We empirically find that our Embedding dimension is 812 bytes. We use the Adam optimizer (Kingma and Ba, 2015). The learning rate is 0.05, and the dropout rate is 0.5. The auto-latent encoder has a fixed size of 812. The discriminator D has two dense layers and a ReLU activation, taking 812-D inputs. Stochastic gradient descent uses a 1e-4 learning rate and 1e-3 weight decay with 0.5 momentum. We do 5-fold cross-validation on the DCaM dataset. ","All trials run on an NVIDIA GeForce RTX 2080 Ti graphics processing unit. We did a grid search over 200 epochs. We determine through testing that our Embedding length is 812 bytes. We apply Adam optimization (Kingma and Ba, 2015). The learning rate is 0.05, and the dropout probability is 0.5. The auto-latent encoder's dimensions are fixed at 812. The discriminator D has two fully-connected layers and a ReLU activation, accepting 812-D inputs. Stochastic gradient descent uses a 1e-4 learning rate, 1e-3 weight decay, and 0.5 momentum. We perform 5-fold cross-validation of the DCaM dataset.",A,0
33,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"To this end, we re-examine the typical metric-based methods, and propose a new adaptive end-to-end metric learning scheme for the challenging zero-shot slot filling. Considering simplicity, efficiency and generalizability, we present a cascade-style joint learning framework coupled with context aware soft label representations and slot-level contrastive representation learning to mitigate the data and label shift problems effectively. Extensive experiments on public benchmarks demonstrate the superiority of the proposed approach over a series of competitive baselines.","For this purpose, we re-evaluate the standard methods based on metrics, and suggest a new flexible end-to-end metric learning plan for the difficult zero-shot slot filling task. With simplicity, efficiency and generalizability in mind, we introduce a cascading joint learning structure paired with context aware soft label representations and slot-level contrastive representation learning to successfully mitigate the data and label shift problems. Extensive experiments on public benchmarks show the superiority of the proposed approach over a series of competitive baseline methods.","To accomplish this goal, we re-examine the typical methods that use metrics, and put forth a new adaptive end-to-end metric learning scheme for the challenging zero-shot slot filling task. Considering ease of use, efficiency and adaptability, we present a waterfall style joint learning framework together with context aware pliable label representations and slot-level contrastive representation learning to effectively address the data and label shift problems. Comprehensive experiments on public benchmarks demonstrate the advantage of the proposed approach over a series of competitive alternative methods.  ","For this purpose, we re-assess the standard metric-driven techniques, and suggest a new flexible end-to-end metric learning plan for the difficult zero-shot slot filling challenge. With simplicity, efficiency and versatility in mind, we introduce a tiered joint learning architecture paired with context aware malleable label representations and slot-level contrastive representation learning to successfully tackle the data and label shift problems. Extensive experiments on public benchmarks exhibit the superiority of the proposed approach over a series of competitive alternative methods.",A,0
31,A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"To broaden our study and provide groundwork on interpretability for gender-neutral MT, we conducted a preliminary analysis of the 240 WinoMT samples requiring gender-neutral translation. These instances result from compiling templates with a gender-neutral pronoun (e.g., “The technician told the customer that they could pay with cash.”). Table 10 provides a detailed overview of the results for Flan-T5 and mT0. Considering Flan-T5 and En-Es, we did not find either inflected form in 67 (28%) cases (i.e., they did not match with any entry in our dictionary).","We expanded our research and laid the foundation for understandability in gender-neutral machine translation by initially analyzing the 240 WinoMT examples that need gender-neutral translation. These examples come from templates with gender-neutral pronouns (for instance, ""The technician told the customer that they could pay with cash.""). Table 10 thoroughly summarizes the outcomes for Flan-T5 and mT0. For Flan-T5 and En-Es, we did not find either inflected form in 67 (28%) cases (meaning they did not correspond to any entry in our dictionary).","To widen our investigation and establish a basis for lucidity in genderless machine translation, we performed a preliminary review of the 240 WinoMT specimens necessitating genderless translation. These specimens originate from formats containing genderless pronouns (like ""The technician told the customer that they could pay with cash.""). Table 10 gives a meticulous overview of the results for Flan-T5 and mT0. Regarding Flan-T5 and En-Es, we did not encounter either inflected variant in 67 (28%) instances (that is, they did not match any entry in our lexicon).  ","In order to expand our analysis and lay the groundwork for intelligibility in gender-neutral machine translation, we conducted an initial examination of the 240 WinoMT examples requiring gender-neutral translation. These examples stem from templates containing gender-neutral pronouns (for example, ""The technician told the customer that they could pay with cash.""). Table 10 provides a thorough summary of the outcomes for Flan-T5 and mT0. With respect to Flan-T5 and En-Es, we did not find either inflected form in 67 (28%) cases (meaning they did not correspond with any entry in our dictionary).",A,0
294,"Explain, Edit, Generate","The same entity token is processed consistently throughout all pieces of evidence, to preserve the multi-hop correlation within the evidence. For example, if an entity is identified in one piece of evidence, it will be consistently replaced or swapped across all pieces of evidence within the instance. We use the editing rules to produce one edited evidence for each instance based on a random seed. Notably, the PERSON and ORG entities are unique to each instance, rather than across the entire dataset. Thus, we prefer random in-instance swapping over in-dataset replacing to avoid introducing irrelevant information from the dataset into the edited evidence. See examples in Appendix A.","The identical entity marker is handled uniformly across all fragments of proof, to maintain the multi-step link within the proof. For instance, if an entity is singled out in one fragment, it will be reliably substituted or traded across all fragments inside the case. We utilize the editing guidelines to generate one altered proof for each case based on an arbitrary seed. Importantly, the PERSON and ORG entities are exclusive to each case, rather than across the whole dataset. Therefore, we favor random in-case swapping over in-dataset replacing to avoid introducing irrelevant material from the dataset into the edited proof. Refer to examples in Appendix A.","The same entity label is managed consistently throughout all pieces of confirmation, to keep the multi-hop connection within the confirmation. As an example, if an entity is pinpointed in one piece, it will be consistently replaced or exchanged across all pieces within the example. We employ the editing protocols to construct one modified confirmation for each example based on a random seed. Notably, the PERSON and ORG entities are unique to each example, rather than across the whole dataset. Hence, we prefer random in-example swapping over in-dataset substituting to avoid introducing irrelevant content from the dataset into the edited confirmation. See instances in Appendix A.  ","The identical entity tag is handled uniformly across all segments of corroboration, to maintain the multi-step linkage within the corroboration. For instance, if an entity is identified in one segment, it will be reliably substituted or swapped across all segments within the instance. We utilize the editing guidelines to generate one altered corroboration for each instance based on an arbitrary seed. Importantly, the PERSON and ORG entities are exclusive to each instance, rather than across the entire dataset. Therefore, we favor random in-instance swapping over in-dataset replacing to avoid introducing irrelevant information from the dataset into the edited corroboration. Refer to examples in Appendix A.",A,0
290,"Explain, Edit, Generate","There is a growing academic interest in CDA to improve model robustness. Initial studies focus on human crafted counterfactuals (Kaushik et al., 2020; Gardner et al., 2020). Recently, numerous automatic CDA methods have been proposed for sentiment analysis (Wang and Culotta, 2021; Yang et al., 2021; Howard et al., 2022), question answering (Paranjape et al., 2022; Dixit et al., 2022), and natural language inference (Glockner et al., 2018). However, these methods are primarily targeted to NLP tasks without requiring complex reasoning about the input. Thus, their direct application to the multi-hop fact verification task presents considerable challenges.","An increasing number of academics are becoming interested in using CDA to make models more robust. Early research concentrated on counterfactuals devised by people (Kaushik et al., 2020; Gardner et al., 2020). More recently, many automatic CDA techniques have been suggested for sentiment analysis (Wang and Culotta, 2021; Yang et al., 2021; Howard et al., 2022), question answering (Paranjape et al., 2022; Dixit et al., 2022), and natural language inference (Glockner et al., 2018). However, these techniques are mostly aimed at NLP tasks that don't require complex reasoning about the input. So applying them directly to the multi-hop fact verification task poses significant difficulties.","There is growing scholarly fascination with leveraging CDA to enhance model resilience. Initial inquiries revolve around human-authored counterfactuals (Kaushik et al., 2020; Gardner et al., 2020). Numerous automated CDA approaches have been put forward lately for sentiment analysis (Wang and Culotta, 2021; Yang et al., 2021; Howard et al., 2022), question answering (Paranjape et al., 2022; Dixit et al., 2022), and natural language inference (Glockner et al., 2018). However, these tactics chiefly target NLP chores sans needing intricate deduction about the input. Hence, their straightforward application to the multi-hop fact verification task presents major obstacles.  ","Expanding academic attentiveness exists regarding harnessing CDA for fortifying model robustness. Early explorations centered on human-formulated counterfactuals (Kaushik et al., 2020; Gardner et al., 2020). Recently, copious automated CDA ploys materialized for sentiment analysis (Wang and Culotta, 2021; Yang et al., 2021; Howard et al., 2022), question answering (Paranjape et al., 2022; Dixit et al., 2022), and natural language inference (Glockner et al., 2018). Though, these maneuvers principally aim at NLP errands minus necessitating knotty inference regarding the input. Thereby, their outright employment for the multi-hop fact verification task poses sizable tribulations.",A,0
432,Eliminating Lipschitz Singularities in Diffusion Models,"As shown in Table 2, our E-TSDM achieves excellent performance across different noise schedules. Besides, the comparison of Lipschitz constants between E-TSDM and baseline on different noise schedules, as illustrated in Appendix D.1, show that E-TSDM can mitigate the Lipschitz singularities issue besides the scenario of the linear schedule, highlighting that its effects are independent of the specific noise schedule. Additionally, the continuous-time version of E-TSDM outperforms the corresponding baseline, indicating that E-TSDM is effective for both continuous-time and discrete time diffusion models.","The results presented in Table 2 demonstrate that our E-TSDM method produces outstanding results across various noise protocols. Furthermore, the comparison of Lipschitz constants for E-TSDM and the baseline under different noise protocols, as shown in Appendix D.1, indicates that E-TSDM can alleviate the Lipschitz singularities problem in scenarios other than the linear schedule. This highlights that its effects do not rely on the particular noise protocol used. Moreover, the continuous-time variant of E-TSDM surpasses the related baseline, signifying that E-TSDM is successful for both continuous-time and discrete time diffusion models.","As exhibited in Table 2, our E-TSDM approach yields superb performance with diverse noise agendas. Additionally, the juxtaposition of Lipschitz values for E-TSDM and the standard method under assorted noise agendas, as presented in Appendix D.1, proves that E-TSDM can mitigate the Lipschitz singularities issue outside of the linear schedule situation, underlining that its impacts are not contingent on the exact noise agenda employed. Furthermore, the continuous-time form of E-TSDM outdoes the associated standard method, denoting that E-TSDM is fruitful for both continuous-time and discrete time diffusion models.  ","The data shown in Table 2 makes evident that our E-TSDM methodology produces first-rate results across a variety of noise regimens. Also, the comparison of Lipschitz numbers for E-TSDM and the baseline with different noise regimens, as depicted in Appendix D.1, establishes that E-TSDM can relieve the Lipschitz singularities problem apart from the linear schedule case, highlighting that its effects do not hinge on the particular noise regimen used. Additionally, the continuous-time variant of E-TSDM exceeds the related baseline, signifying that E-TSDM is effectual for both continuous-time and discrete time diffusion models.",A,0
749,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"In this paper, we present innovative unsupervised techniques to produce synthetic parallel data and train a critic to evaluate the grammaticality of sentences. By combining our methods with BIFI, we develop an unsupervised GEC system that achieves results comparable to models utilizing substantial labeled data. The core idea is to employ languageindependent erroneous models to construct realistic synthetic data, and then create an unsupervised critic utilizing high-confidence predictions from the fixer model. Our system does not require any manually defined or extracted confusion sets, making it an ideal solution for developing GEC models for low-resource languages. ","This paper introduces new unsupervised methods to generate synthetic parallel data and train an evaluator to assess the grammatical correctness of sentences. By integrating our techniques with BIFI, we build an unsupervised GEC system that attains performance similar to models leveraging considerable labeled data. The key concept is to use language-agnostic faulty models to construct realistic synthetic data, and then develop an unsupervised evaluator using high-confidence predictions from the corrector model. Our system does not need any manually defined or extracted confusion sets, making it ideal for building GEC models for low-resource languages.","In this work, we present novel unsupervised approaches to synthesize parallel corpora and educate a reviewer to judge the grammaticality of sentences. Through combining our procedures with BIFI, we construct an unsupervised GEC framework that reaches results on par with models harnessing substantial annotated data. The fundamental notion is to harness language-independent inaccurate models to produce realistic synthetic data, followed by creating an unsupervised assessor capitalizing on high-confidence inferences from the rectifier model. Our system does not necessitate any hand-engineered or extracted confusion sets, rendering it optimal for developing GEC models for low-resource tongues.  ","Here, we introduce cutting-edge unsupervised methodologies to generate synthetic parallel content and drill an appraiser to evaluate the grammaticality of sentences. By fusing our techniques with BIFI, we architect an unsupervised GEC framework that attains performance analogous to models leveraging considerable labeled content. The cardinal concept is to wield language-agnostic defective models to fabricate realistic synthetic content, succeeded by constituting an unsupervised appraiser capitalizing on high-confidence deductions from the amender model. Our system does not mandate any manually defined or gleaned confusion sets, constituting it ideal for constructing GEC models for low-resource lexes.",A,0
745,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"Additionally, the fixer’s improvement is considerably smaller when data augmentation (DA) or self-knowledge distillation (SKD) is excluded. Moreover, similar to LM-critic, the fixer’s improvement comes to a halt after the first iteration without updating the critic. This demonstrates the significance of updating both the critic and the fixer throughout the process. Critic’s performance through iterations. In Figure 6, we observe a consistent improvement in the performance of the critic throughout the iterations. This indicates a mutually beneficial learning process between the critic and the fixer: the critic improves the fixer, which in turn refines the critic even further. The plot on the right shows a correlation between pseudo-label precision and fixer iteration. ","Furthermore, the enhancement of the corrector is much slighter when augmenting data (DA) or self-teaching through distillation (SKD) is omitted. Also, akin to LM-critic, the corrector's enhancement plateaus after the first cycle without refreshing the critic. This proves the importance of revamping both the critic and corrector over the course of the procedure. Critic's capabilities across cycles. In Figure 6, we see a steady boost in the critic's performance across the cycles. This signifies a mutually advantageous learning experience between the critic and corrector: the critic betters the corrector, which in turn hones the critic even more. The graph on the right displays a link between pseudo-label precision and corrector iteration.","In addition, the improver's progress is far smaller without data inflation (DA) or self-guided learning through knowledge distillation (SKD). Furthermore, the improver's gains stop after the first round without updating the reviewer, similar to LM-critic. This shows the value of enhancing both the reviewer and improver throughout the workflow. Reviewer's scores by round. In Figure 6, we notice the reviewer's consistent gains by round. This indicates cooperative learning between the reviewer and improver: the reviewer assists the improver, which then refines the reviewer more. The right chart displays a correlation between pseudo-label accuracy and improver round.  ","Moreover, the enhancer's advance is much smaller excluding data expansion (DA) or self-tutoring through know-how distillation (SKD). Also, the enhancer's headway halts after the first step without revising the assessor, like LM-critic. This proves the importance of improving both the assessor and enhancer during the process. Assessor's ratings by step. In Figure 6, we see the assessor's steady gains by step. This shows cooperative education between the assessor and enhancer: the assessor helps the enhancer, which then hones the assessor further. The right graph displays a link between pseudo-label precision and enhancer step.",A,0
486,Model Tells you what to Discard - Adaptive KV Cache Compression for LLMs,"Inspired by the success of Transformer, extensive studies have been conducted to explore the underlying mechanism of different self-attention heads. Voita et al. (2019) analyzed the self-attention heads in BERT (Devlin et al., 2019) using LRF (Bach et al., 2015) and characterized them into interepretable roles, one of which is attending adjacent tokens all the time. Michel et al. (2019) demonstrated that even heads in the same layer could have different impact on the performance while the importance of each head change across tasks. Clark et al. (2019) and Kovaleva et al. (2019) identified the patterns that some heads primarily attend to separator tokens, adjacent tokens and a combination of these. We observe consistent patterns in decoder-only models, despite previous studies are mainly done on encoder models. Our work shares similar spirits with these studies but focus on characterizing the KV cache of different attention heads.","Motivated by the triumph of Transformer, numerous investigations have been carried out to understand the fundamental working of diverse self-attention heads. Voita et al. (2019) probed the self-attention heads in BERT (Devlin et al., 2019) utilizing LRF (Bach et al., 2015) and categorized them into interpretable roles, one of which constantly takes note of adjacent tokens. Michel et al. (2019) showed that even heads in a similar layer could have varying impact on performance while the significance of each head changes across tasks. Clark et al. (2019) and Kovaleva et al. (2019) recognized the patterns that some heads primarily focus on separator tokens, neighboring tokens and a mix of these. We notice consistent patterns in decoder-only models, despite past studies mostly done on encoder models. Our work shares comparable motivations with these investigations yet centers around describing the KV cache of various attention heads.","Propelled by the victory of Transformer, broad examinations have been attempted to comprehend the basic instrument of various self-attention heads. Voita et al. (2019) dissected the self-attention heads in BERT (Devlin et al., 2019) utilizing LRF (Bach et al., 2015) and described them into interpretable jobs, one of which is going to adjacent tokens constantly. Michel et al. (2019) showed that even heads in a similar layer could have various effect on execution while the significance of each head changes across assignments. Clark et al. (2019) and Kovaleva et al. (2019) recognized the examples that some heads essentially go to separator tokens, contiguous tokens and a blend of these. We watch steady examples in decoder-just models, in spite of past examinations are dominantly done on encoder models. Our work shares comparative spirits with these investigations yet center around portraying the KV reserve of various attention heads.","Enlivened by the achievement of Transformer, broad examinations have been attempted to investigate the basic component of various self-attention heads. Voita et al. (2019) broke down the self-attention heads in BERT (Devlin et al., 2019) utilizing LRF (Bach et al., 2015) and described them into reasonable jobs, one of which is taking note of adjacent tokens constantly. Michel et al. (2019) showed that even heads in a similar layer could have various effect on execution while the significance of each head changes across assignments. Clark et al. (2019) and Kovaleva et al. (2019) recognized the examples that some heads basically go to separator tokens, contiguous tokens and a blend of these. We watch steady examples in decoder-just models, notwithstanding past examinations are dominantly done on encoder models. Our work shares comparable spirits with these investigations yet spotlight on portraying the KV store of various attention heads.",A,0
594,RESEE,"In visual-enhanced conversational recommendation, MMD (Saha et al., 2018) was a multimodal dataset under a shopping situation and aimed at providing applicable recommendations based on textual conversations as well as images of potential shopping items.  MMConv (Liao et al., 2021) was applied in tourism scenarios across 5 real situations, it also provided a knowledge base and a photo gallery about recommended items.  Recently, MMDialog (Feng et al., 2022) was proposed with massive multimodal open-domain conversations and associated images derived from social media.  IMAD (Viktor and Denis, 2023) was constructed using massive amount of dialogues, with the last utterance to be replaced with collected images. ","In visually boosted chat-based recommendation, MMD (Saha et al., 2018) was a multimedia dataset in a shopping context, seeking to give useful suggestions based on text chats and photos of possible products. MMConv (Liao et al., 2021) was used in travel settings across 5 real cases, also offering a knowledge repository and an image gallery regarding suggested items. Recently, MMDialog (Feng et al., 2022) emerged with huge multimedia open-domain chats and related images from social platforms. IMAD (Viktor and Denis, 2023) was built using massive dialogues, with the last remark substituted with gathered images.","For conversational recommendation enhanced by visuals, MMD (Saha et al., 2018) was a multimodal data source for shopping where recommendations were given from textual dialog and product photos. MMConv (Liao et al., 2021) applied conversational recommendation to tourism over 5 real situations, providing knowledge and images about recommendations. MMDialog (Feng et al., 2022) recently introduced large scale multimodal open domain conversations from social media with associated images. IMAD (Viktor and Denis, 2023) constructed dialog data by replacing final utterances with collected images.  ","In visually augmented conversational recommender systems, MMD (Saha et al., 2018) collected multimodal shopping dialogs and product images for recommendations. MMConv (Liao et al., 2021) did tourism recommendation over real cases, giving knowledge and images. MMDialog (Feng et al., 2022) has large open domain social media conversations and images. IMAD (Viktor and Denis, 2023) built dialogs, replacing final utterances with images.",A,0
381,INSTRUCTSCORE,"We apply the respective prompts defined in Appendix Tables 29, 30, 31, and 32 to generate synthetic data. We define four evaluation scenarios: 1) evaluation with reference only; 2) evaluation with reference and additional data; 3) evaluation with reference where the source has different modalities; 4) evaluation with reference and world knowledge. For each scenario, we obtain 10k candidate reference pairs as input and structured diagnostic reports as output. We train a separate checkpoint for each evaluation scenario, resulting in four checkpoints in total. All models are fine-tuned with language modeling loss with 10k synthetic data. Each model is trained for three epochs, with a learning rate, batch size, and weight decay of 2e-5, 128, and 0, respectively.","We use the prompts listed in the tables in the appendix to create synthetic data. We have four ways to evaluate the model: 1) just use the reference; 2) use the reference plus extra data; 3) use a reference with different types of data; 4) use the reference and general knowledge. For each evaluation method, we take 10,000 reference pairs as input and structured reports as output. We train one model per evaluation method, so there are four models total. All the models are fine-tuned by predicting the next word, using 10,000 synthetic examples. We train each model for 3 epochs, with a learning rate of 0.00002, a batch size of 128, and no weight decay.","We generate synthetic data by applying the prompts specified in the appendix tables. There are four evaluation scenarios: 1) reference only; 2) reference plus additional data; 3) multimodal reference; 4) reference plus world knowledge. For each scenario, we use 10,000 reference pairs as input and structured diagnostic reports as output. We train a separate model for each scenario, for a total of four models. All models are fine-tuned using language modeling on 10,000 synthetic examples. We train for 3 epochs, with a learning rate of 2e-5, batch size of 128, and no weight decay.","The prompts from the appendix tables are used to create synthetic data. There are four evaluation setups: 1) just the reference; 2) reference and extra data; 3) reference with different modalities; 4) reference and general knowledge. In each setup, 10,000 reference pairs go in and structured reports come out. One model per setup is trained, so four total. All models are fine-tuned by predicting the next word, using 10,000 synthetic samples. 3 epochs of training are done, with learning rate at 0.00002, batch size of 128, and no weight decay.",A,0
523,Neural Fine-Tuning Search for Few-Shot Learning,"Overall, our deferred NAS approach where a large-scale search is conducted up-front during meta-train and a small candidate set search is conducted per meta-test episode, provides a reasonable trade-off between per-episode cost and efficacy. While our cost at recommended N = 3 is slightly higher than competitors with a single fine-tuning, it is similar or less than competitors who repeat adaptation with different learning rates during testing (Hu et al., 2022) (4× cost), or exploit a backbone ensemble (8× cost) (Dvornik et al., 2020b; Liu et al., 2021a). Where this cost is not acceptable, our single architecture NFTS-1 already provides state-of-the-art results.","In summary, our postponed NAS method where a large-scale search happens ahead of time during meta-train and a small candidate set search happens per meta-test episode, gives a decent compromise between per-episode expense and effectiveness. Although our cost at suggested N = 3 is somewhat higher than competitors with a single fine-tuning, it is comparable or less than competitors who repeat adaptation with various learning rates during testing (Hu et al., 2022) (4× cost), or use a backbone ensemble (8× cost) (Dvornik et al., 2020b; Liu et al., 2021a). Where this cost is not acceptable, our single architecture NFTS-1 already provides state-of-the-art results.","To summarize, our delayed NAS approach conducting a large-scale search during meta-train and a small candidate set search per meta-test episode provides a reasonable balance between per-episode expenditure and performance. While our cost at N = 3 is slightly higher than competitors with one fine-tuning, it is similar to or less than competitors repeating adaptation with different learning rates during testing (Hu et al., 2022) (4× cost) or using a backbone ensemble (8× cost) (Dvornik et al., 2020b; Liu et al., 2021a). If this cost is unacceptable, our single NFTS-1 architecture already delivers state-of-the-art performance.  ","In conclusion, our postponed NAS methodology performing extensive search during meta-train and limited search per meta-test episode offers a decent trade-off between per-episode price and efficacy. Although our price at N = 3 exceeds competitors with single fine-tuning, it approximates or undercuts competitors reiterating adaptation with varied learning rates during testing (Hu et al., 2022) (4× price) or leveraging backbone ensemble (8× price) (Dvornik et al., 2020b; Liu et al., 2021a). Where this price is prohibitive, our sole NFTS-1 architecture already furnishes state-of-the-art outcomes.",A,0
327,Fifty Shades of Bias,"BWS or Maximum Difference Scaling (MaxDiff) proposed by Louviere (1991) has been long used in many psychological studies. BWS is an efficient comparative annotation framework. Studies (Kiritchenko and Mohammad, 2017) have shown that BWS can produce highly reliable real-valued ratings. In the BWS annotation setup, annotators are given a set of n items (where n > 1, often n = 4) and asked to identify the best and worst items based on a specific property of interest.","The Maximum Difference Scaling technique created by Louviere in 1991 has frequently been utilized in psychological research. This comparative annotation system is productive. Analyses (Kiritchenko & Mohammad, 2017) have demonstrated that it can generate very consistent quantitative evaluations. With BWS annotation, reviewers are provided with multiple items (usually more than 1, typically 4) and instructed to choose the most and least ideal ones based on a particular attribute.","The BWS or MaxDiff approach designed by Louviere in '91 has been commonly applied in many studies of psychology. This comparative tagging framework is efficient. Investigations (Kiritchenko and Mohammad, 2017) have proven it can produce very reliable numerical scores. In BWS tagging, evaluators get a set of things (n > 1, often n = 4) and are asked to pinpoint the best and worst items according to a specific characteristic of interest.","The Maximum Difference Scaling method conceived by Louviere in 1991 has frequently been used in psychological experiments. This comparative annotation model is productive. Examinations (Kiritchenko and Mohammad, 2017) have shown it can generate very consistent quantitative ratings. In BWS annotation, appraisers are given multiple items (n > 1, typically n = 4) and tasked to identify the most and least optimal ones based on a particular property of interest.",A,0
234,Cultural Concept Adaptation on Multimodal Reasoning,"Strategies like restricting the use of higher-order hypernyms and implementing an exponential decay of sampling probability for concept inclusion were employed to enhance accuracy and authenticity, ensuring the graphs’ overall quality and reliability. See the appendix B for details. We use the Detic model (Zhou et al., 2022) available at Facebook Research’s GitHub repository for object detection. It employs the CLIP (Radford et al., 2021) classifier and offers open-vocabulary capabilities. With this model, we can freely configure the vocabulary, allowing us to detect specific cultural concepts within images. If you want a more precise result, it is recommended to use the segment anything model (Kirillov et al., 2023), but it may require some manual clicking.","Tactics such as limiting the usage of high-level hypernyms and implementing an exponential decay of sampling likelihood for concept addition were utilized to improve precision and genuineness, guaranteeing the graphs’ total caliber and dependability. Refer to appendix B for particulars. We employ the Detic framework (Zhou et al., 2022) accessible on Facebook Research's GitHub repository for object identification. It uses the CLIP (Radford et al., 2021) classifier and provides open-vocabulary abilities. With this model, we can freely configure the vocabulary, permitting us to identify specific cultural ideas within images. If you desire a more accurate outcome, it is suggested to utilize the segment anything model (Kirillov et al., 2023), but it may necessitate some manual clicking.","Procedures like constraining the application of higher-order hypernyms and executing an exponential decay of sampling probability for concept inclusion were used to enhance accuracy and authenticity, ensuring the graphs' overall excellence and reliability. See appendix B for information. We utilize the Detic system (Zhou et al., 2022) available on Facebook Research's GitHub repository for object detection. It uses the CLIP (Radford et al., 2021) classifier and provides open-vocabulary capabilities. With this model, we can freely configure the vocabulary, allowing us to detect specific cultural concepts within images. If you want a more precise result, it is recommended to use the segment anything model (Kirillov et al., 2023), but it may need some manual clicking.","Plans like limiting the use of higher-level hypernyms and implementing an exponential decay of sampling chance for concept addition were employed to improve precision and genuineness, guaranteeing the graphs' total quality and dependability. Refer to appendix B for specifics. We use the Detic framework (Zhou et al., 2022) accessible on Facebook Research's GitHub repository for object recognition. It utilizes the CLIP (Radford et al., 2021) classifier and provides open-vocabulary abilities. With this model, we can freely configure the vocabulary, permitting us to identify specific cultural concepts within images. If you want a more accurate outcome, it is suggested to use the segment anything model (Kirillov et al., 2023), but it may need some manual clicking.",A,0
442,Eliminating Lipschitz Singularities in Diffusion Models,"In other words, on the one hand, Modified-NS seriously reduces the amount of noise added near zero point, which can be detrimental to the accurate prediction. On the other hand, Modified-NS alleviates the Lipschitz singularities, which is beneficial to the synthesis performance. As a result, for linear and cosine schedules, Modified-NS performs better than baseline but worse than E-TSDM. However, for the quadratic schedule, although we force the SNR of Modified-NS at t = T similar to the SNR of the original schedule, the SNR at other timesteps is significantly increased, leading to a worse performance of Modified-NS compared to that of baseline.","To put it another way, Modified-NS greatly decreases the quantity of noise applied near the origin, which can negatively impact precise forecasting. However, Modified-NS also minimizes Lipschitz irregularities, improving synthesis capabilities. Therefore, Modified-NS is superior to baseline yet inferior to E-TSDM for linear and cosine plans. But for the quadratic schedule, despite equalizing the SNR of Modified-NS at t = T to the original schedule's SNR, the SNR at other times is considerably raised, resulting in worse performance of Modified-NS versus baseline.","In other terms, Modified-NS substantially lowers the noise near zero, potentially harming accurate predictions. Though, Modified-NS lessens Lipschitz discontinuities, benefiting synthesis. So, Modified-NS surpasses baseline but not E-TSDM for linear and cosine agendas. However, even aligning Modified-NS's SNR at t = T to the original schedule, the SNR at other points is increased significantly, causing worse Modified-NS performance compared to baseline for the quadratic schedule.  ","To put it differently, Modified-NS greatly reduces noise around zero, which can impede precise forecasts. However, Modified-NS decreases Lipschitz irregularities, improving synthesis. Thus, Modified-NS is better than baseline but worse than E-TSDM for linear and cosine plans. But despite matching Modified-NS's SNR at t = T to the original schedule, the SNR at other times rises substantially for the quadratic schedule, leading to worse Modified-NS performance versus baseline.",A,0
363,INSTRUCTSCORE,"We evaluate INSTRUCTSCORE on a variety of generation tasks, including translation, captioning, data-to-text, and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our INSTRUCTSCORE, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.","We assess INSTRUCTSCORE on several generation activities, including translation, captioning, data-to-text, and commonsense generation. Tests indicate that our 7B model exceeds all other unsupervised metrics, including those founded on 175B GPT-3 and GPT-4. Remarkably, our INSTRUCTSCORE, even without direct guidance from human-evaluated data, attains performance heights on par with cutting-edge metrics like COMET22, which were fine-tuned on human ratings.","We evaluate INSTRUCTSCORE across a variety of generative tasks, such as translation, captioning, data-to-text, and commonsense generation. Experiments demonstrate that our 7B model surpasses all other unsupervised metrics, including those derived from 175B GPT-3 and GPT-4. Incredibly, our INSTRUCTSCORE, even without direct supervision from human-judged data, achieves performance levels comparable to state-of-the-art metrics like COMET22, which were fine-tuned on human evaluations.  ","We test INSTRUCTSCORE on multiple generative assignments, including translation, captioning, data-to-text, and commonsense generation. Tests show that our 7B model outperforms all other unsupervised metrics, including those originating from 175B GPT-3 and GPT-4. Amazingly, our INSTRUCTSCORE, even without direct oversight from human-rated data, reaches performance heights on par with cutting-edge metrics like COMET22, which were fine-tuned on human assessments.",A,0
405,INTERFAIR Debiasing with Natural Language Feedback for Fair Interpretable Predictions,"We break our experiments into two parts: 1) developing the NL parser and 2) interactive debiasing with INTERFAIR. We use BiosBias (De-Arteaga et al., 2019), a dataset made from a large-scale user study of gender in various occupations. It contains short biographies labeled with gender and profession information, and a possible confluence exists between gender and annotated profession labels. Using INTERFAIR, we would like to predict the profession from biographies without the influence of gender.","Our experiments are divided into two sections: 1) creating the natural language parser and 2) interacting with INTERFAIR to remove bias. We utilize BiosBias (De-Arteaga et al., 2019), a dataset produced from a large user study about gender across various jobs. It has short biographies labeled with gender and occupation details, and there may be a connection between gender and the tagged occupation labels. With INTERFAIR, we want to forecast the job from biographies without being impacted by gender.","We separate our experiments into two components: 1) building the natural language processing module and 2) interacting with INTERFAIR for debiasing. Our data comes from BiosBias (De-Arteaga et al., 2019), which contains short biographical information labeled with gender and profession, created through a large study of gender bias across occupations. There could be an association between the annotated gender and profession labels. Our goal is to predict profession from the biographies without being influenced by gender, using INTERFAIR.","Our experiments have two parts: 1) developing the natural language understanding system, and 2) using INTERFAIR to remove bias interactively. Our data is from BiosBias (De-Arteaga et al., 2019), which has short biographies labeled with gender and job information, collected from a large study of gender bias in jobs. There may be a link between the labeled gender and job labels. We want to predict job from the biographies without influence from gender, by using INTERFAIR.",A,0
109,Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering,"In addition, the gradient only back propagates through the efficient side block, reducing the cost of back propagation. As shown in Figure 3, the efficient side block consists of a series of adapter modules (Houlsby et al., 2019). We plug an adapter between every two Transformer layers (Vaswani et al., 2017).","Moreover, the slope only spreads backward through the effective side component, decreasing the expense of backward spread. As depicted in Figure 3, the effective side component contains a sequence of adapter modules (Houlsby et al., 2019). We connect an adapter between each pair of Transformer layers (Vaswani et al., 2017).","Furthermore, the incline exclusively retrogrades through the productive side square, diminishing the expense of in reverse proliferation. As exhibited in Figure 3, the productive side square comprises of an arrangement of adapter modules (Houlsby et al., 2019). We plug an adapter between each two Transformer layers (Vaswani et al., 2017). ","Additionally, the grade only regresses through the efficient side chunk, reducing the toll of regression. As revealed in Figure 3, the efficient side chunk holds a succession of adapter modules (Houlsby et al., 2019). We insert an adapter between every two Transformer beds (Vaswani et al., 2017).",A,0
54,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Since label shift is a critical challenge in zero-shot learning, to verify the generalization capacity, we specifically test our method on the unseen target data. Following Liu et al. (2022), we split the dataset into the seen and unseen group, where we only evaluate on unseen slot entities during training in the unseen slot group, while evaluate on the whole utterance in the unseen uttr group. From Table 6, our method performs better than other metric-based baselines, showing the superiority of our method for unseen domain generalization.","Because label shift poses a major problem in zero-shot learning, we specifically evaluated our method's ability to generalize by testing it on previously unseen target data. As in Liu et al. (2022), we divided the dataset into seen and unseen groups, where we only assessed performance on unseen slot entities in the unseen slot group during training, while evaluating on the full utterance in the unseen uttr group. As shown in Table 6, our method outperformed other metric-based baselines, demonstrating its superiority for generalizing to unseen domains.","Since label shift presents a critical challenge for zero-shot learning, we tested our method's generalization capacity by intentionally evaluating it on target data that was not seen during training. We split the data into seen and unseen groups, following Liu et al. (2022)'s approach of only measuring performance on unseen slot entities in the unseen slot group during training, while evaluating on complete utterances in the unseen uttr group. Our method surpassed other metric-based baselines, as evidenced in Table 6, proving its advantage for generalizing to unseen domains.  ","Label shift being a major obstacle in zero-shot learning, we specifically assessed our method's ability to generalize by evaluating it on previously unseen target data. We divided the data into seen and unseen groups, evaluating only on unseen slot entities in the unseen slot group during training per Liu et al. (2022), while evaluating on full utterances in the unseen uttr group. As shown in Table 6, our method outperformed other metric-based baselines, demonstrating its superior generalization to unseen domains.",A,0
738,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"We generate 145 million synthetic sentence pairs with the method described in §3.2.2. These synthetic pairs are used to fine-tune the Flan-T5-xxl model (Chung et al., 2022) to create the initial fixer f0. Following Yasunaga et al. (2021), our monolingual dataset Dm contains both grammatical and ungrammatical sentences. Concretely, we randomly select 10 million unlabeled sentences from various sources: Yahoo!Answer corpus (Zhang et al., 2015), Wikipedia history (Grundkiewicz and Junczys-Dowmunt, 2014), Lang8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), and FCE (Yannakoudakis et al., 2011) datasets. Notably, as Wikipedia history, Lang8, NUCLE, and FCE are labeled datasets, we only take sentences from the source side of these datasets5. ","We generate 145 million artificial sentence pairs using the process outlined in section 3.2.2. These fabricated pairs are utilized to fine-tune the Flan-T5-xxl model (Chung et al., 2022) to construct the initial fixer f0. As in Yasunaga et al. (2021), our monolingual data set Dm contains both grammatically correct and incorrect sentences. Specifically, we randomly select 10 million unlabeled sentences from various sources: Yahoo!Answer corpus (Zhang et al., 2015), Wikipedia edit history (Grundkiewicz and Junczys-Dowmunt, 2014), Lang8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), and FCE (Yannakoudakis et al., 2011) datasets. Importantly, as Wikipedia edit history, Lang8, NUCLE, and FCE are labeled datasets, we only take sentences from the source side of these datasets.","We produce 145 million synthetic sentence pairs utilizing the technique outlined in section 3.2.2. These fabricated pairs are leveraged to fine-tune the Flan-T5-xxl model (Chung et al., 2022) to build the initial fixer f0. As in Yasunaga et al. (2021), our monolingual corpus Dm contains both grammatically accurate and inaccurate sentences. Specifically, we arbitrarily choose 10 million unlabeled sentences from various sources: Yahoo!Answer corpus (Zhang et al., 2015), Wikipedia revision history (Grundkiewicz and Junczys-Dowmunt, 2014), Lang8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), and FCE (Yannakoudakis et al., 2011) datasets. Significantly, as Wikipedia revision history, Lang8, NUCLE, and FCE are labeled datasets, we only take sentences from the source side of these datasets.","We generate 145 million synthetic sentence pairs by the method outlined in section 3.2.2. These fabricated pairs are used to fine-tune the Flan-T5-xxl model (Chung et al., 2022) to build the initial fixer f0. As in Yasunaga et al. (2021), our monolingual data set Dm has both grammatically correct and incorrect sentences. Specifically, we randomly select 10 million unlabeled sentences from various sources: Yahoo!Answer corpus (Zhang et al., 2015), Wikipedia edit history (Grundkiewicz and Junczys-Dowmunt, 2014), Lang8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), and FCE (Yannakoudakis et al., 2011) datasets. Importantly, as Wikipedia edit history, Lang8, NUCLE, and FCE are labeled datasets, we only take sentences from the source side of these datasets.",A,0
192,Counting the Bugs in ChatGPTs Wugs A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,"Other typical tasks in computational research on inflection are morphological segmentation (Cotterell et al., 2015, 2016b,c; Kann et al., 2016), unsupervised morphology induction (Hammarström and Borin, 2011; Soricut and Och, 2015; Xu et al., 2018; Weissweiler et al., 2022), and morphological paradigm completion (Erdmann et al., 2020a,b; Jin et al., 2020). There has also been some interest in the modeling of derivation (Cotterell et al., 2017b; Vylomova et al., 2017; Deutsch et al., 2018; Hofmann et al., 2020b,c).","Additional common jobs in computer-based research on inflection are splitting words into morphemes (Cotterell et al., 2015, 2016b,c; Kann et al., 2016), unsupervised learning of morphological structure (Hammarström and Borin, 2011; Soricut and Och, 2015; Xu et al., 2018; Weissweiler et al., 2022), and filling in missing forms in morphological paradigms (Erdmann et al., 2020a,b; Jin et al., 2020). There has also been some attention paid to modeling word formation through affixes (Cotterell et al., 2017b; Vylomova et al., 2017; Deutsch et al., 2018; Hofmann et al., 2020b,c).","Other typical undertakings in computational work on inflection include morphological segmentation (Cotterell et al., 2015, 2016b,c; Kann et al., 2016), unsupervised induction of morphology (Hammarström and Borin, 2011; Soricut and Och, 2015; Xu et al., 2018; Weissweiler et al., 2022), and completion of morphological paradigms (Erdmann et al., 2020a,b; Jin et al., 2020). Additionally, some interest has been shown in modeling derivational morphology (Cotterell et al., 2017b; Vylomova et al., 2017; Deutsch et al., 2018; Hofmann et al., 2020b,c).","Further common activities in computer-assisted research on inflection are splitting words into constituent morphemes (Cotterell et al., 2015, 2016b,c; Kann et al., 2016), learning morphological patterns without supervision (Hammarström and Borin, 2011; Soricut and Och, 2015; Xu et al., 2018; Weissweiler et al., 2022), and filling in missing forms in morphological paradigms (Erdmann et al., 2020a,b; Jin et al., 2020). There has been some focus as well on modeling word formation processes like derivation and compounding (Cotterell et al., 2017b; Vylomova et al., 2017; Deutsch et al., 2018; Hofmann et al., 2020b,c).",A,0
644,Standardizing Distress Analysis," 1.  We propose the novel task of Unified Distress Identification and Cause Extraction (DICE) from multimodal online posts.  2.  We develop a multi-task deep framework for the simultaneous detection of distress content and identify connected causal phrases from the text using emotional information.  3.  We devise a zero-shot strategy to dynamically incorporate emotional information into training and propose a novel fusion mechanism to infuse the features of multimodal inputs.  4.  The first Distress and Cause annotated Multimodal (DCaM) corpus is created consisting over 20,764 social media posts.  5.  Resources are open-sourced to aid research.  The rest of the paper is organized as follows. ","1. We present the new challenge of Integrated Distress Recognition and Cause Extraction (DICE) from online posts with multiple modes. 2. We build a multi-task deep system to concurrently detect distress content and identify related causal phrases from the text using emotional cues. 3. We design a zero-shot approach to dynamically integrate emotional knowledge into training and suggest a new fusion system to combine the characteristics of multimodal inputs. 4. The first Distress and Cause annotated Multimodal (DCaM) dataset is assembled containing over 20,764 social media posts. 5. Resources are openly shared to assist research. The remainder of the paper is structured as follows.","1. We put forth the original task of Unified Distress Identification and Cause Detection (DICE) from online posts with text, images, etc. 2. We construct a multi-task deep framework to simultaneously recognize distress content and pinpoint associated causal phrases from the text leveraging affective clues. 3. We invent a zero-shot strategy to dynamically incorporate emotional insights into training and propose an innovative fusion mechanism to blend the attributes of multimedia inputs. 4. The inaugural Distress and Cause annotated Multimedia (DCaM) collection is compiled comprising over 20,764 social media posts. 5. Resources are publicly released to further research. The rest of the paper is outlined as follows.  ","1. We introduce the novel challenge of Combined Distress Recognition and Cause Extraction (DICE) from online posts containing multiple modes like text, images, etc. 2. We develop a multi-task deep system to concurrently identify distress content and isolate related causal phrases from the text capitalizing on affective information. 3. We conceive a zero-shot approach to dynamically integrate emotional knowledge into training and design an original fusion mechanism to consolidate the features of multimedia inputs. 4. The first-ever Distress and Cause annotated Multimedia (DCaM) corpus is created containing over 20,764 social media posts. 5. Resources are openly shared to promote research. The remainder of the paper is organized as follows.",A,0
118,Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering,"The result is shown in Figure 4. We can observe that Tent, EATA and SAR are very sensitive to the learning rate. With the increase in the learning rate, the performance of them drops rapidly after reaching the maximum, which indicates that they are prone to model collapse under a large learning rate. OIL performs better than Tent, EATA and SAR, but it still rapidly deteriorates after maintaining performance for a while. In contrast, Anti-CF is less sensitive to the learning rate. As the learning rate increases, the performance of Anti-CF will slowly decline until it approaches the performance of the source model.","The outcome is depicted in Figure 4. We can see that Tent, EATA and SAR are very responsive to the learning rate. As the learning rate rises, their performance sharply decreases after hitting the peak, showing they are susceptible to model collapse with a high learning rate. OIL does better than Tent, EATA and SAR, but its performance still quickly worsens after staying steady for some time. In comparison, Anti-CF is less reactive to the learning rate. When the learning rate goes up, Anti-CF's performance will gradually drop until it nears the source model's performance.","The finding is presented in Figure 4. We notice Tent, EATA and SAR are very sensitive to changes in the learning rate. Their performance falls steeply after reaching maximum with increasing learning rate, indicating they easily experience model collapse at a large learning rate. OIL outperforms Tent, EATA and SAR, however its performance still rapidly deteriorates after maintaining for a period. In contrast, Anti-CF is less affected by the learning rate. As learning rate rises, Anti-CF's performance slowly declines until approaching the source model's performance.  ","The data is shown in Figure 4. We see Tent, EATA and SAR are highly reactive to the learning rate. Their performance plummets rapidly after peaking as the learning rate increases, demonstrating susceptibility to model collapse at a high learning rate. OIL is better than Tent, EATA and SAR but still quickly deteriorates after holding steady temporarily. Comparatively, Anti-CF is less responsive to the learning rate. With increasing learning rate, Anti-CF's performance gradually decreases until nearing the source model's performance.",A,0
723,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"The contributions of our paper are as follows: • We introduce a novel method for unsupervised synthetic data generation, based on MLM and language-independent error patterns. Compared to existing approaches, our method generates more realistic synthetic data, and provides a better unsupervised fixer. • We propose a new method to build an unsupervised critic with high-confidence predictions from the fixer model. This approach enables the critic model to continually enhance its performance over iterations, demonstrating better performance than prior methods. ","The main innovations presented in this paper are: - We put forward a new technique for creating synthetic data without supervision, using MLM and language-agnostic error patterns. Our approach produces more believable synthetic data and a superior unsupervised error correction system compared to current methods. - We suggest a novel way to construct an unsupervised critic using high-confidence predictions from the error correction model. This allows the critic to continuously improve its performance over iterations, outperforming previous approaches.","The key contributions of our research are: - We develop a novel unsupervised approach for generating synthetic data based on MLM and language-independent mistakes. Our method generates more realistic synthetic examples and a better unsupervised error fixer versus existing techniques. - We introduce a new technique to build an unsupervised critic utilizing high-confidence forecasts from the fixer model. This enables the critic to steadily enhance its capabilities over cycles, surpassing prior methods. ","The main novel aspects of our study are: - We present a new unsupervised method to synthesize data using MLM and language-neutral error patterns. Compared to current approaches, our technique produces more believable synthetic data and a superior unsupervised error corrector. - We propose a novel way to construct an unsupervised critic leveraging high-confidence predictions from the corrector model. This allows the critic to iteratively improve performance over iterations, outperforming previous techniques.",A,0
409,INTERFAIR Debiasing with Natural Language Feedback for Fair Interpretable Predictions,"The gender split of the subject pool was 1:1. To understand the change in model performance and bias, we consider two other debiasing models along with the base model (He et al., 2022) used in INTERFAIR: (1) Rerank, an inference-time debiasing variant where the task rationale is considered based on ascending order of bias energy (He et al., 2022); (2) Adv, a model trained with an adversarial objective (Zhang et al., 2018) to debias the model’s latent space, but incapable of producing any rationales.","The proportion of males to females in the group of participants was equal. To analyze how model accuracy and prejudice differ, we examine two other methods of reducing bias along with the original model (He et al., 2022) used in INTERFAIR: (1) Rerank, a version that debias at prediction time by considering the task reasoning in order of increasing bias (He et al., 2022); (2) Adv, a model trained to remove bias from its internal representations using an adversarial goal (Zhang et al., 2018), but unable to generate any explanations.","The number of men and women in the test group was the same. To understand how model performance and unfairness change, we look at two other ways to decrease bias as well as the starting model (He et al., 2022) used in INTERFAIR: (1) Rerank, a variant that reduces bias when making predictions by thinking about the task justification sorted by low to high bias (He et al., 2022); (2) Adv, a model trained to take out bias from its hidden features using an opposing objective (Zhang et al., 2018), but not able to produce any rationales.","The test group had an equal number of males and females. To grasp how model accuracy and discrimination shift, we examine two additional bias mitigation techniques along with the baseline model (He et al., 2022) used in INTERFAIR: (1) Rerank, an alternative that lessens bias during inference by considering the task reasoning ordered by increasing bias (He et al., 2022); (2) Adv, a model trained to eliminate bias from its latent space using an adversarial aim (Zhang et al., 2018), but unable to provide any explanations.",A,0
752,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"We show the insertion and deletion error pattern for English in Figure 7. The insertion and deletion error pattern for Chinese is shown in Figure 8. The replacement error pattern for English is shown in Figure 9. The replacement error pattern for Chinese is shown in Figure 10. Extracting GED Pseudo-Labels from the Fixer: The complete correlation between the probability of producing ˆy(i) and precision of z(i) is shown in Figure 11. Detailed Experimental Settings Implementation details and training configuration: We build our fixer using both the fairseq8 and transformers9 toolkit. Specifically, since the Flan- T5-xxl model has around 11B parameters, we use the transformers toolkit with DeepSpeed10 ZeROOffload to build the fixer for English and use the fairseq toolkit to build the rest of the components. ","The visual representation of addition and removal of characters for English is presented in Figure 7. Figure 8 displays the same for Chinese script. Substitution of characters in English is depicted in Figure 9. Figure 10 exhibits substitution errors in Chinese text. Linking GED Mock-Labels from the Corrector: The full relationship between the chance of generating ˆy(i) and precision of z(i) appears in Figure 11. In-depth Experimental Particulars Realization specifics and preparation arrangement: We construct our corrector utilizing both the fairseq8 and transformers9 toolkit. Explicitly, since the Flan-T5-xxl model has around 11B parameters, we utilize the transformers toolkit with DeepSpeed10 ZeROOffload to assemble the corrector for English and utilize the fairseq toolkit to assemble the remainder of the parts.","The example of inserting and removing characters for English is shown in Figure 7. Figure 8 presents the insertion and removal errors for Chinese. The substitution of characters in English is pictured in Figure 9. Figure 10 displays the substitution errors for Chinese. Acquiring GED Quasi-Labels from the Rectifier: The complete link between the odds of producing ˆy(i) and accuracy of z(i) is exhibited in Figure 11. Comprehensive Experimental Settings Execution subtleties and preparing setup: We construct our rectifier utilizing both the fairseq8 and transformers9 toolkit. Explicitly, since the Flan-T5-xxl model has around 11B parameters, we utilize the transformers toolkit with DeepSpeed10 ZeROOffload to assemble the rectifier for English and utilize the fairseq toolkit to assemble the remainder of the parts.","Figure 7 illustrates the pattern of insertion and deletion errors for English text. The insertion and deletion error pattern for Chinese text is presented in Figure 8. The pattern of replacement errors in English is shown in Figure 9. Figure 10 shows the replacement error pattern for Chinese. Relationship between Probability of ˆy(i) and Accuracy of z(i): The full correlation between the probability of generating ˆy(i) and the accuracy of z(i) is presented in Figure 11. Implementation Details and Training Configuration: We implemented our fixer using both the fairseq8 and transformers9 toolkits. Specifically, since the Flan-T5-xxl model contains around 11B parameters, we used the transformers toolkit with DeepSpeed10 ZeROOffload to build the fixer for English, and used the fairseq toolkit for the remaining components.",A,0
587,RESEE,"(3) When considering embedding-based metrics, our method is better than baselines in Avg.  and Ext., but it is slightly inferior to two GPT models in Gre..  That is to say, though RESEE may not reach the similarity upper bound compared to pre-trained GPTs, it is still advantageous in the averaged sentence similarity comparing strong baselines.  We also observe that finetuned GPT-2 and DIALOGPT perform better than our method in PPL on both datasets.  This is attributed to their pretraining stage which dedicates in directly optimizing model generation ability.  However, our model can achieve better diversity compared with baselines, especially our model variants without textual entity input and/or entity-level visual knowledge.  We also present human evaluation results in Table 5,6 which further justify the outcomes and findings from automatic metrics above. ","When looking at embedding-based metrics, our approach is superior to baseline methods in Avg. and Ext., but it is slightly worse than two GPT models in Gre.. This means that although RESEE may not reach the similarity upper limit compared to pre-trained GPTs, it still has an advantage in average sentence similarity over strong baselines. We also see that fine-tuned GPT-2 and DIALOGPT have better PPL on both datasets than our method. This is because their pretraining phase directly focuses on optimizing model generation ability. However, our model can achieve greater diversity compared to baselines, especially our model versions without textual entity input and/or entity-level visual knowledge. We also provide human evaluation results in Tables 5 and 6 which further validate the findings from the automatic metrics above.","When examining embedding-focused metrics, our technique surpasses baseline approaches in Avg. and Ext., but lags slightly behind two GPT models in Gre. In other words, although RESEE may not attain the similarity ceiling compared to pre-trained GPTs, it still holds an edge in mean sentence similarity over robust baselines. We also notice that fine-tuned GPT-2 and DIALOGPT have superior PPL on both datasets versus our approach. This owes to their pretraining phase concentrating directly on enhancing model generation capability. Nonetheless, our model can realize greater diversity relative to baselines, especially our model variants lacking textual entity input and/or entity-level visual knowledge. We additionally furnish human evaluation outcomes in Tables 5 and 6 which further corroborate the conclusions from the automatic metrics above.  ","Analyzing embedding-centric metrics reveals our method bests baseline techniques in Avg. and Ext. but slightly trails two GPT models in Gre. That is, RESEE may not match similarity upper limits of pre-trained GPTs yet retains averaged sentence similarity advantages over stout baselines. We also find fine-tuned GPT-2 and DIALOGPT have superior PPL on both datasets versus our method, attributable to their pretraining directly honing generative ability. However, our model achieves greater diversity than baselines, especially variants sans textual entity input and/or entity visual knowledge. Human evaluations in Tables 5 and 6 further validate automatic metric findings above.",A,0
542,Non-autoregressive Text Editing with Copy-aware Latent Alignments,"We begin by introducing some notations. The goal for text-editing is to transform the source sentence x “ x0, x1, ...  , xN into the desired target y “ y0, y1, ...  , yM with N and M tokens, respectively. Connectionist Temporal Classification was first introduced in auto speech recognition (ASR) (Graves et al., 2006), aiming to circumvent the problems of no explicit alignments between ASR inputs/outputs. Specifically, CTC introduces a special blank token ∅ on top of the vocabulary V, and defines a latent alignment path a “ a0, a1, ...  , aN between x and y with ai P V Ť t∅u, which is of equal length as x.","Let's start by presenting some symbols. The objective for editing text is to change the original sentence x = x0, x1, ..., xN into the wanted target y = y0, y1, ..., yM with N and M words, respectively. Connectionist Temporal Classification was first presented in automatic speech recognition (ASR) (Graves et al., 2006), with the goal of avoiding the issues of no explicit alignments between ASR inputs/outputs. Specifically, CTC brings in a special blank token ∅ in addition to the vocabulary V, and defines a hidden alignment path a = a0, a1, ..., aN between x and y with ai ∈ V ∪ {∅}, which has the same length as x.","We will begin by introducing some notations. The aim for text editing is to transform the source sentence x = x0, x1, ..., xN into the desired target y = y0, y1, ..., yM with N and M tokens, respectively. Connectionist Temporal Classification was first brought in for automatic speech recognition (ASR) (Graves et al., 2006), with the goal of getting around the problems of no explicit alignments between ASR inputs/outputs. In particular, CTC adds a special blank token ∅ along with the vocabulary V, and defines a latent alignment path a = a0, a1, ..., aN between x and y with ai ∈ V ∪ {∅}, which has the same length as x. ","Let's start by presenting some symbols. The goal for editing text is to change the original sentence x = x0, x1, ..., xN into the wanted target y = y0, y1, ..., yM with N and M words, respectively. Connectionist Temporal Classification was first introduced for automated speech recognition (ASR) (Graves et al., 2006), with the aim of avoiding the issues of no explicit alignments between ASR inputs/outputs. Specifically, CTC brings in a special blank token ∅ in addition to the vocabulary V, and defines a hidden alignment path a = a0, a1, ..., aN between x and y with ai ∈ V ∪ {∅}, which is the same length as x.",A,0
335,Fifty Shades of Bias,"Akin to offensive language and harshness, ‘perceived gender bias’ is an inherently subjective concept based on lived experiences, community, education, etc., (Blodgett et al., 2020; Davani et al., 2022; Biester et al., 2022). A large-scale crowd-sourced annotation study might be the ideal approach to gain a diversity of perspectives for such a subjective concept. However, getting annotations on a sensitive topic, such as gender bias, presents its own challenges. Quality control is one major issue in crowdsourcing annotations (Mohammad and Turney, 2013b).","Similar to offensive speech and callousness, the notion of 'apparent gender prejudice' is an intrinsically subjective idea grounded in life experiences, community, schooling, and so on (Blodgett et al., 2020; Davani et al., 2022; Biester et al., 2022). A large-scale crowd-sourced labeling investigation might be the perfect tactic to obtain a range of outlooks on such a subjective concept. However, acquiring annotations on a sensitive subject, like gender bias, poses its own difficulties. Quality assurance is one major concern in crowdsourcing labels (Mohammad and Turney, 2013b).","In the same vein as inappropriate language and harshness, the concept of 'seeming gender bias' is an inherently relative one based in lived realities, social circles, education, and more (Blodgett et al., 2020; Davani et al., 2022; Biester et al., 2022). A wide-scale crowd-sourced tagging study could be the best approach to capture a diversity of perspectives on such a subjective notion. However, soliciting annotations on a delicate topic, such as gender bias, brings its own challenges. Quality control is one major hurdle in crowdsourcing tags (Mohammad and Turney, 2013b).  ","Similar to offensive rhetoric and callousness, the idea of 'perceived gender prejudice' is an intrinsically subjective construct grounded in life journeys, community, schooling, etc. (Blodgett et al., 2020; Davani et al., 2022; Biester et al., 2022). A large-scale crowd-sourced labeling investigation might be the optimal tactic to obtain a range of views on such a subjective concept. However, procuring annotations on a sensitive subject, like gender bias, poses its own difficulties. Quality assurance is one major issue in crowdsourcing labels (Mohammad and Turney, 2013b).",A,0
588,RESEE,"We conduct extensive ablation experiments over variants of the input information to better understand their respective roles in the dialogue generation task.  (1) The performance improvement on our model benefits from both aspects of visual knowledge in providing external information.  (2) Fine-grained visual information (i.e., entity-level), plays a more important role in improving the generation performance than turn-level visual knowledge, which explains the necessity to find and utilize fine-grained visual clues.  (3) Turn-level images also prompt model performance (i.e., “- E.” v.s.  “- E.  - T.V.”), which is consistent with findings from the traditional visual dialogue. ","We perform comprehensive removal experiments on versions of the input data to more fully grasp their individual functions in the dialogue creation assignment. (1) The enhanced execution on our framework stems from both features of visual understanding when supplying external material. (2) Precise visual information (namely, entity-level) has a greater impact on refining the generation capability versus turn-level visual comprehension, clarifying the need to identify and leverage fine-grained visual hints. (3) Turn-level pictures also prod model capability (see ""- E."" compared to ""- E. - T.V.""), aligning with discoveries from the conventional visual discussion.","We undertake expansive excision trials over forms of the input knowledge to more completely comprehend their discrete roles within the dialogue construction effort. (1) The improved enactment of our system originates from both constituents of visual cognition when furnishing outward intelligence. (2) Exact visual data (specifically, entity-level) exercises a superior sway over augmenting the generation aptitude against turn-level visual discernment, elucidating the necessity to pinpoint and harness fine-grained visual clues. (3) Turn-level imagery likewise goads model aptitude (observe ""- E."" juxtaposed with ""- E. - T.V.""), congruent with conclusions from the archetypal visual colloquy.  ","We embark on comprehensive removal experiments on variants of the input understanding to more thoroughly grasp their individual functions within the dialogue fabrication endeavor. (1) The enhanced performance of our framework stems from both elements of visual comprehension upon furnishing external material. (2) Precise visual knowledge (namely, entity-level) wields a greater impact on enhancing the generation capability compared to turn-level visual acumen, elucidating the need to identify and leverage fine-grained visual hints. (3) Turn-level graphics likewise prod model capability (see ""- E."" contrasted with ""- E. - T.V.""), congruous with deductions from the prototypical visual exchange.",A,0
84,ALDi Quantifying the Arabic Level of Dialectness of Text,"The main model we use to predict ALDi is a BERTbased regression model. Using the training split of AOC-ALDi, we fine-tune a regression head on top of MarBERT, an Arabic BERT model (AbdulMageed et al., 2021a), and clip the output to the range [0, 1]. To measure the consistency of the model’s performance, we repeat the fine-tuning process three times using 30, 42, and 50 as the random seeds, and report averaged evaluation scores for the model (similarly for Baseline #3). We compare this model to three baselines, which use existing Arabic resources and are not trained on AOC-ALDi.","Our primary approach for forecasting ALDi utilizes a BERT-founded regression framework. Leveraging the training portion of AOC-ALDi, we optimize a regression module on top of MarBERT, an Arabic BERT architecture (AbdulMageed et al., 2021a), and bound the output to the interval [0, 1]. To quantify the consistency of the model, we reiterate the tuning process three times employing 30, 42, and 50 as the random seeds, and document averaged assessment results for the model (likewise for Baseline #3). We contrast this approach with three benchmarks, exploiting existing Arabic resources without training on AOC-ALDi.","The central technique we harness to predict ALDi is a BERT-rooted regression system. Capitalizing on the training split of AOC-ALDi, we calibrate a regression component atop MarBERT, an Arabic BERT design (AbdulMageed et al., 2021a), and clamp the output to the range [0, 1]. To gauge the stability of the model, we replicate the tuning workflow thrice exercising 30, 42, and 50 as the stochastic seeds, and chronicle averaged evaluation metrics for the model (analogously for Baseline #3). We juxtapose this tactic with three standards, leveraging extant Arabic assets sans tuning on AOC-ALDi.  ","Our cardinal solution for prognosticating ALDi is a BERT-anchored regression architecture. Exploiting the training partition of AOC-ALDi, we tune a regression module on MarBERT, an Arabic BERT blueprint (AbdulMageed et al., 2021a), and confine the output to the bracket [0, 1]. To calibrate the robustness of the model, we triplicate the tuning course thrice applying 30, 42, and 50 as the aleatory seeds, and tabulate averaged performance numerics for the model (likewise for Baseline #3). We counterpose this strategy with three benchmarks, harnessing available Arabic resources sans adapting on AOC-ALDi.",A,0
10,A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We expect LMs to inflect gender in occupation words according to overt contextual and lexical clues. Instead, a biased model is one, which relies on stereotypical gender-role associations. Both open source and commercial MT systems have been shown to rely on these associations, with a marked tendency to associate women with less prestigious roles (e.g., Stanovsky et al., 2019; Saunders and Byrne, 2020; Chung et al., 2022, inter alia). Echoing Blodgett et al. (2020), such systems risk representational harms, as they portray women in a less favorable light than men.","We anticipate language models will modify the gender in job titles based on clear contextual and word clues. However, a prejudiced model is one that depends on stereotypical gender-role connections. Both public and private MT platforms have demonstrated dependence on these connections, with a distinct tendency to relate women with less prestigious jobs (see Stanovsky et al., 2019; Saunders and Byrne, 2020; Chung et al., 2022, among others). Mirroring Blodgett et al. (2020), such systems risk harmful misrepresentations, as they depict women less positively than men.","We expect language models to change the gender in occupation terms according to unambiguous contextual and lexical hints. But a biased model relies on stereotypical gender-role links. Open source and commercial machine translation services have shown reliance on these links, frequently associating women with less respected roles (for example, Stanovsky et al., 2019; Saunders and Byrne, 2020; Chung et al., 2022, and more). Echoing Blodgett et al. (2020), such systems endanger harmful mischaracterizations, since they portray women less favorably than men.  ","We want language models to vary the gender in job words based on clear contextual and word signals. However, a prejudiced model depends on stereotypical gender-role connections. Both community-developed and private machine translation platforms have exhibited dependence on these connections, frequently relating women with less prestigious jobs (see Stanovsky et al., 2019; Saunders and Byrne, 2020; Chung et al., 2022, and others). Mirroring Blodgett et al. (2020), such systems risk harmful misrepresentations, since they depict women less positively than men.",A,0
620,SOUL,"This flexibility breaks the restriction of SC purely focusing on sentiment polarity and allows for the introduction of more complex sentiment problems. In Figure 1, the reviewer’s sentiment towards the raptor graphics lacks specific reasons, making it difficult for a simple pattern matching model to accurately predict the first statement as not-given without contextual understanding. The second statement in Figure 1 also presents a challenge for models in detecting sarcasm. The JG task, on the other hand, seeks to provide an explanation for the rationale behind the model’s interpretation of sentiment, answering the question of why the sentiment is as predicted. ","This adaptability removes the limitation that SC only concentrates on sentiment polarity and enables more intricate sentiment issues to be introduced. As shown in Figure 1, the reviewer's sentiment towards the raptor graphics does not have precise justifications, making it problematic for a basic pattern matching model to accurately anticipate the first statement as not-given without contextual comprehension. The second statement in Figure 1 also poses a test for models in identifying sarcasm. In contrast, the JG task aims to give an explanation for the reasoning behind the model's understanding of sentiment, responding to the inquiry of why the sentiment is as predicted.","This flexibility eliminates the constraint that SC focuses solely on sentiment direction and allows for bringing in more complicated sentiment challenges. As illustrated in Figure 1, the reviewer's attitude toward the raptor graphics lacks specific reasons, causing trouble for a simple pattern recognition model to correctly predict the first statement as not-given without contextual understanding. The second statement in Figure 1 also presents an obstacle for models in detecting sarcasm. On the other hand, the JG task seeks to provide a rationale for the model's interpretation of sentiment, addressing the question of why the sentiment is as forecasted. ","This adaptability removes the limitation that SC concentrates exclusively on sentiment orientation and enables the introduction of more complex sentiment problems. As shown in Figure 1, the reviewer's attitude toward the raptor graphics does not have precise justifications, making it difficult for a straightforward pattern identification model to accurately anticipate the first statement as not-given without contextual comprehension. The second statement in Figure 1 also poses a hurdle for models in spotting sarcasm. In contrast, the JG task aims to furnish an explanation for the model's understanding of sentiment, responding to the query of why the sentiment is as predicted.",A,0
736,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"To mitigate this issue, we utilize a self-knowledge distillation (SKD) technique to gather additional training data and enhance the model’s generalizability. Specifically, for each x(i) ∈ D′m , we follow the method used by (Xie et al., 2016; Meng et al., 2020) to construct soft pseudo-labels ˜z(i) c 4: Iteratively Refining the Fixer and Critic Algorithm 3 provides a high-level overview of our unsupervised grammatical error correction (GEC) system. We start by applying the unsupervised technique outlined in §3.2.2 to corrupt Dseed m and yield synthetic data. This synthetic data is then employed to train an initial fixer, denoted by f0. ","To address this problem, we use a self-knowledge teaching (SKT) method to obtain extra training information and improve the model's adaptability. In particular, for each x(i) ∈ D′m, we follow the approach utilized by (Xie et al., 2016; Meng et al., 2020) to generate soft pseudo-labels  ̃z(i)c","To tackle this issue, we employ a self-knowledge tutoring (SKT) procedure to collect supplementary training material and enhance the model's transferability. Specifically, for every x(i) ∈ D′m, we adopt the technique used by (Xie et al., 2016; Meng et al., 2020) to produce soft pseudo-labels  ̃z(i)c  ","To resolve this problem, we make use of a self-knowledge instruction (SKI) process to amass extra training data and boost the model's generalizability. In particular, for each x(i) ∈ D′m, we follow the approach leveraged by (Xie et al., 2016; Meng et al., 2020) to construct soft pseudo-labels  ̃z(i)c",A,0
777,VECHR,"Fig 2b illustrates the detailed architecture of the concept-aware model. For more details, see App K. The concept-aware model exhibits increased robustness to distributional shift and shows an improvement on the challenge set, owed to the incorporation of the vulnerability type descriptions. Overall, our results show promise for the feasibility of the task yet indicate room for improvement. We present VECHR, an ECtHR dataset consisting of 1,070 cases for vulnerability type classification and 40 cases for token-level explanation. We also release a set of baseline results, revealing the challenges of achieving accuracy, explainability, and robustness in vulnerability classification. ","The specific design of the concept-aware model is shown in Figure 2b. More information can be found in Appendix K. The concept-aware model is more robust to distributional changes and performs better on the challenge set because it uses the vulnerability type descriptions. Our results indicate that this task is feasible but can still be improved. We introduce VECHR, a dataset of ECtHR cases with 1,070 for vulnerability type labeling and 40 for token-level justification. We also provide some baseline results, which show the difficulties of getting good accuracy, explainability, and robustness for vulnerability classification.","Figure 2b provides the detailed layout of the concept-aware model. Additional details are in Appendix K. Incorporating the vulnerability type explanations makes the concept-aware model more resistant to distributional shifts and improves performance on the challenge set. In general, our results demonstrate this task is possible but needs enhancement. We present VECHR, a collection of ECtHR cases comprising 1,070 for vulnerability type categorization and 40 for token-level clarification. We also make available some baseline outcomes, which highlight the challenges of achieving precision, interpretability, and robustness for vulnerability classification.  ","The exact blueprint of the concept-aware model is depicted in Figure 2b. Further information is available in Appendix K. The descriptions of vulnerability types make the concept-aware model more impervious to distributional changes and enhances its performance on the challenge set. Overall, our findings indicate the feasibility of this task but room for upgrades. We introduce VECHR, a set of ECtHR cases with 1,070 for vulnerability type tagging and 40 for token-level elucidation. We also provide some initial results, revealing the difficulties of obtaining accuracy, lucidity, and resilience in vulnerability categorization.",A,0
641,Standardizing Distress Analysis,"The exponential expansion of microblogging sites and social media not only empowers free expression and individual voices, but also allows individuals to exhibit anti-social conduct (ElSherief et al., 2018), such as cyberbullying, online rumours, and [*] These authors contributed equally to this work and are the joint first authors.  spreading hate remarks (Ribeiro et al., 2018).  Abusive speech based on race, religion, and sexual orientation is becoming more common (Karim et al., 2020).  Automatic identification of hate speech and raising public awareness are critical tasks (Karim et al., 2020).  Manually evaluating and validating a large volume of web information, on the other hand, is time-consuming and labor-intensive.","The rapid growth of microblogging platforms and social networks not only enables free speech and individual voices, but also enables people to engage in antisocial behaviors (ElSherief et al., 2018), like cyberbullying, spreading rumors online, and making hateful remarks (Ribeiro et al., 2018). Abusive language targeting race, religion, and sexual orientation is becoming more widespread (Karim et al., 2020). Automatically detecting hate speech and increasing public awareness are vital jobs (Karim et al., 2020). However, manually assessing and verifying a large amount of web content takes extensive time and effort.","The explosive expansion of microblogging websites and social media gives individuals the power of free expression and individual voices, but it also provides opportunities for individuals to exhibit anti-social behaviors (ElSherief et al., 2018), such as cyberbullying, circulating online rumors, and making hateful comments (Ribeiro et al., 2018). Abusive speech based on race, religion, and sexual orientation is becoming more prevalent (Karim et al., 2020). Automatically identifying hate speech and raising public awareness are crucial tasks (Karim et al., 2020). However, manually reviewing and validating the huge volume of web information is time-consuming and labor-intensive.","The exponential growth of microblogging platforms and social networks not only enables free speech and amplifies individual voices, but also allows people to engage in anti-social conduct (ElSherief et al., 2018), such as cyberbullying, spreading rumors online, and making hateful remarks targeting protected characteristics (Ribeiro et al., 2018). Abusive language based on race, religion, and sexual orientation is becoming more pervasive (Karim et al., 2020). Automated detection of hate speech and increasing public awareness are important jobs (Karim et al., 2020). However, manually evaluating and verifying the massive amount of web content is time-consuming and labor-intensive.",A,0
519,Neural Fine-Tuning Search for Few-Shot Learning,"To analyse the role that our architecture search plays in few-shot performance more precisely, we also conduct an ablation study of our final model against four corners of our search space: (i) Initial model only, using a pre-trained feature extractor and simple NCC classifier, which loosely corresponds to SimpleShot (Wang et al., 2019), (ii) Full adaptation only, using a fixed feature extractor, which loosely corresponds to TSA (Li et al., 2022), ETT (Xu et al., 2022), FLUTE (Triantafillou et al., 2021), and others – depending on base architecture and choice of adapter, (iii) Fully fine-tuned model, which loosely corresponds to PMF (Hu et al., 2022), and (iv) Combination of full fine-tuning and adaptation.","To analyze the role our architecture search has in few-shot performance more accurately, we also do an ablation study of our final model compared to four corners of our search space: (i) Just the initial model, using a pre-trained feature extractor and simple NCC classifier, which is similar to SimpleShot (Wang et al., 2019), (ii) Only full adaptation, using a fixed feature extractor, which is similar to TSA (Li et al., 2022), ETT (Xu et al., 2022), FLUTE (Triantafillou et al., 2021), and others - depending on base architecture and choice of adapter, (iii) Fully fine-tuned model, which is similar to PMF (Hu et al., 2022), and (iv) Combination of full fine-tuning and adaptation.","To examine the role our architecture search plays in few-shot performance more thoroughly, we also conduct an ablation analysis of our final model versus four extremes of our search space: (i) Starting model only, utilizing a pre-trained feature extractor and simple NCC classifier, which loosely resembles SimpleShot (Wang et al., 2019), (ii) Just full adaptation, employing a fixed feature extractor, which loosely resembles TSA (Li et al., 2022), ETT (Xu et al., 2022), FLUTE (Triantafillou et al., 2021), and others - based on base architecture and adapter choice, (iii) Completely fine-tuned model, which loosely resembles PMF (Hu et al., 2022), and (iv) Mix of full fine-tuning and adaptation.","To analyze the contribution our architecture search makes to few-shot performance more precisely, we also do an ablation evaluation of our final model compared to four extremes of our search space: (i) Initial model only, leveraging a pre-trained feature extractor and simple NCC classifier, which is similar to SimpleShot (Wang et al., 2019), (ii) Just full adaptation, utilizing a fixed feature extractor, which is comparable to TSA (Li et al., 2022), ETT (Xu et al., 2022), FLUTE (Triantafillou et al., 2021), and others - contingent on base architecture and adapter selection, (iii) Fully fine-tuned model, which is analogous to PMF (Hu et al., 2022), and (iv) Blend of full fine-tuning and adaptation.",A,0
708,"Unnatural Error Correction, GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text","Results 3:  Finally, we test the generality of the finding across datasets by two additional datasets for ScrQA.  For scrambled DREAM dataset, we evaluate performance not only overall but also on different categories of questions, using the annotations.  The performance disparities between GPT-4 and other models are more pronounced than those observed on RealtimeQA, possibly since DREAM requires higher-level comprehension of longer texts.  Performance on arithmetic questions tends to be more susceptible to scrambled text compared to other categories, even for GPT-4.  Table 1 demonstrates experimental results with a 4-shot CoT setting on scrambled AQuA-RAT dataset (we only test the performance of three closed-source models here because even the original questions in AQuA-RAT are too challenging for most open-source models). ","Finally, we assess how widely applicable the finding is by testing two more datasets for ScrQA. For the jumbled DREAM dataset, we not only look at overall performance but also at performance on different question types, leveraging the annotations. The gaps between GPT-4 and the other models are more pronounced than on RealtimeQA, possibly since DREAM needs higher comprehension of longer texts. Performance on math questions is more impacted by scrambled text relative to other categories, even for GPT-4. Table 1 shows experimental outcomes with a 4-shot CoT setting on the scrambled AQuA-RAT dataset (we only evaluate three closed-source models here because even the original AQuA-RAT questions are too difficult for most open-source models).","In conclusion, we evaluate the generality of the result using two more datasets for ScrQA. With the garbled DREAM dataset, we measure performance overall and by question category, utilizing the annotations. GPT-4's advantages over other models are greater than on RealtimeQA, likely because DREAM requires deeper understanding of longer texts. Even for GPT-4, performance on arithmetic questions is more sensitive to scrambled text compared to other categories. Table 1 displays experimental findings with a 4-shot CoT configuration on the jumbled AQuA-RAT dataset (we only test three proprietary models here since even the original AQuA-RAT questions are too challenging for most public models).  ","To finish, we test how widely the finding holds using two extra datasets for ScrQA. For the shuffled DREAM dataset, we assess performance in total and by question type, leveraging the labels. GPT-4's edges over other models are more pronounced than on RealtimeQA, possibly since DREAM needs higher comprehension of longer texts. Even for GPT-4, performance on math questions suffers more from scrambled text versus other types. Table 1 shows experimental results with 4-shot CoT on the mixed-up AQuA-RAT dataset (we only evaluate three private models since even the original AQuA-RAT is too hard for most public models).",A,0
29,A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"In both stereotypical and nonstereotypical examples, we observe a correct shift in articles (“El” for male, “La” for female) and gender inflection corresponding to the profession (e.g., “the librarian” - “el bibliotecario” (male), “la bibliotecaria” (female)). Interestingly, while Flan-T5 translates poorly the profession “clerk” with “el secretario” (second row), Flan-T5Few-Shot chooses the right word and gender inflection (“la empleada”). We attribute this improvement in translation to the presence of the profession “clerk” in the few-shot examples, which likely allows the model to learn the correct profession translation.","Both in stereotypical and non-stereotypical instances, we notice an accurate change in articles (""El"" for male, ""La"" for female) and gender endings that match the job (for example, ""the librarian"" - ""el bibliotecario"" (male), ""la bibliotecaria"" (female)). Interestingly, while Flan-T5 poorly translates the profession ""clerk"" as ""el secretario"" (second row), Flan-T5Few-Shot chooses the right word and gender ending (""la empleada""). We credit this enhancement in translation to the presence of the profession ""clerk"" in the few-shot examples, which likely enables the model to learn the correct job translation.","In prototypical and atypical cases, we discern proper adjustments in definite articles (""El"" for masculine, ""La"" for feminine) and grammatical gender concordant with the occupation (like ""the librarian"" becoming ""el bibliotecario"" (masculine) or ""la bibliotecaria"" (feminine)). Remarkably, whereas Flan-T5 improperly renders the job ""clerk"" as ""el secretario"" (second line), Flan-T5Few-Shot selects the accurate term and grammatical gender (""la empleada""). We attribute this refinement in translation to the inclusion of the occupation ""clerk"" in the few-shot samples, which probably empowers the model to acquire the right occupational translation.  ","Across both stereotypical and non-stereotypical instances, we notice suitable shifts in definite articles (""El"" for men, ""La"" for women) and grammatical gender endings fitting the profession (for example, ""the librarian"" becoming ""el bibliotecario"" (male) or ""la bibliotecaria"" (female)). Intriguingly, while Flan-T5 poorly translates the job ""clerk"" as ""el secretario"" (second line), Flan-T5Few-Shot selects the correct word and gender inflection (""la empleada""). We ascribe this improvement in translation to the presence of the occupation ""clerk"" in the few-shot examples, which likely enables the model to learn the accurate professional translation.",A,0
220,Cultural Concept Adaptation on Multimodal Reasoning,"This is an example of cultural adaptation. Leveraging the relationships of hypernyms, hyponyms, and synonyms from publicly accessible semantic dictionaries, our method maps source cultural concepts to their corresponding target concepts, thereby eliminating the need for costly manual annotation. To support the model’s understanding of cultural concept mappings, we subsequently introduce a novel cultural concept-based multimodal data augmentation technique.","This demonstrates cultural adjustment. By utilizing the connections between superordinate terms, subordinate terms, and synonymous words from publicly available semantic lexicons, our technique matches source cultural ideas to their related target concepts. This removes the requirement for expensive manual labeling. To bolster the model's comprehension of cultural concept mappings, we present a new multimodal data expansion method based on cultural concepts.","This illustrates cultural adaptation. Through exploiting the links between broader terms, narrower terms, and words with similar meanings from public semantic dictionaries, our approach aligns source cultural notions with their corresponding target notions. This eliminates the need for costly human annotation. To support the model's grasp of cultural concept alignments, we put forward an original multimodal data augmentation technique founded on cultural concepts. ","This shows cultural accommodation. By harnessing the associations between superterms, subterms, and equivalent words from publicly accessible semantic wordbooks, our process correlates source cultural concepts with their linked target concepts. This abolishes the necessity for expensive manual tagging. To reinforce the model's understanding of cultural concept correlations, we bring in a novel multimodal data expansion technique based on cultural concepts.",A,0
711,"Unnatural Error Correction, GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text","However, it is difficult to conclude the reason why (some) LLMs are capable to these tasks.  Especially, the reason why GPT-4 can perform almost perfectly would be an interesting topic worth further investigation.  We can not access the closedsource models directly and are aware of little information about them (even the exact model size of GPT-4).  These situation make investigating the reason difficult.  An hypothesis is that this capability might be related to training methods, such as incorporating tasks similar to denoising in the training objectives, or using a vast amount of text data containing various errors in the training process.  Another hypothesis is that this capability emerges as LLMs scale. ","Nevertheless, it is not easy to determine the cause of (some) LLMs having the ability to perform these tasks. Particularly, why GPT-4 can carry out almost flawlessly is an intriguing subject deserving more examination. We can't directly access the proprietary models and know little about them (even the exact dimensions of GPT-4). This circumstance makes investigating the rationale tough. One theory is that this skill might connect to training techniques, like integrating objectives akin to denoising in the training goals, or utilizing a huge volume of text data with various errors in the training workflow. Another hypothesis is that this capability materializes as LLMs expand in scale.","However, it is challenging to conclude why (certain) LLMs possess the capacity for these tasks. Especially, the reasons behind GPT-4's near perfect performance would make an exciting research topic. We have limited access to closed-source models and scarce knowledge about them (not even GPT-4's precise model size). This situation hinders investigating the reasons. One conjecture is that this ability could relate to training approaches, like incorporating denoising-like objectives in the training aims, or leveraging massive text data with diverse errors in training. Another speculation is that this capability emerges as LLMs grow in scale.  ","Nonetheless, determining the basis for (some) LLMs having the aptitude for these tasks is difficult. Specifically, why GPT-4 can execute almost flawlessly would make an intriguing research subject. We can't directly access proprietary models and have little insight into them (not even GPT-4's exact model dimensions). This impedes investigating the reasons. One hypothesis is that this skill might connect to training techniques, like integrating noise removal-esque goals in training objectives, or employing huge text data with varied errors during training. Another guess is that this ability materializes as LLMs expand in size.",A,0
65,ALDi Quantifying the Arabic Level of Dialectness of Text,"Transcribed speech and user-generated text in Arabic typically contain a mixture of Modern Standard Arabic (MSA), the standardized language taught in schools, and Dialectal Arabic (DA), used in daily communications. To handle this variation, previous work in Arabic NLP has focused on Dialect Identification (DI) on the sentence or the token level. However, DI treats the task as binary, whereas we argue that Arabic speakers perceive a spectrum of dialectness, which we operationalize at the sentence level as the Arabic Level of Dialectness (ALDi), a continuous linguistic variable.","Recorded and user-created Arabic text often includes both Modern Formal Arabic (MFA), the standardized language used in education, and Colloquial Arabic (CA), used in everyday talk. To manage this difference, past Arabic NLP research has concentrated on Dialect Classification (DC) at the sentence or word level. However, DC views the task as having two options, while we believe Arabic speakers see a range of dialectness, which we define at the sentence level as the Arabic Extent of Colloquialism (AEC), a steady linguistic factor.","Arabic speech transcripts and user writings frequently have a combination of Contemporary Standard Arabic (CSA), the formal language taught in schools, and Local Arabic (LA), used in daily communication. To account for this variation, earlier Arabic NLP work focused on Dialect Identification (DI) on the sentence or word level. However, DI considers the task as binary, while we argue that Arabic speakers perceive a spectrum of dialectness, which we quantify at the sentence level as the Arabic Degree of Colloquialness (ADC), a continuous linguistic variable.","Recorded and informal Arabic texts often contain both Contemporary Formal Arabic (CFA), the standardized language learned in education, and Local Dialect Arabic (LDA), used in everyday interactions. To address this variation, previous Arabic NLP research concentrated on Dialect Categorization (DC) at the sentence or token level. However, DC approaches the task as having two categories, whereas we believe Arabic speakers see a range of dialectness, which we measure at the sentence level as the Arabic Extent of Informality (AEI), a steady linguistic factor.",A,0
239,Cultural Concept Adaptation on Multimodal Reasoning,"As indicated in Table 2, the performance of the model improves with an increase in the volume of augmented data. Taking into account the results from these two sets of experiments, we decide to amplify the dataset to twenty times the original size and choose a ratio of x : y = 15 : 5 for our subsequent zero-shot and few-shot experiments. Although we do not contend that the ratio of x : y = 15 : 5 is optimal, we assert that this choice is adequate to demonstrate the effectiveness of our approach.","The results shown in Table 2 demonstrate that the model's performance gets better as more augmented data is used for training. Considering the outcomes from these two groups of tests, we opt to expand the dataset to 20 times its original size and pick a ratio of x : y = 15 : 5 for our next zero-shot and few-shot trials. While we don't claim that the x : y ratio of 15 : 5 is ideal, we argue that this selection is sufficient to exhibit the usefulness of our method.","As exhibited in Table 2, the model's capabilities improve when the amount of enhanced information is increased. Taking into account the conclusions from these two sets of analyses, we decide to multiply the data collection to twentyfold its former magnitude and select a proportion of x : y = 15 : 5 for our forthcoming zero-shot and few-shot assessments. Despite not contending the x : y ratio of 15 : 5 is flawless, we state this preference adequately proves our approach's effectiveness.  ","The data in Table 2 shows that the model gets better when more augmented data is used for training. Based on the results of these two groups of tests, we choose to increase the dataset to 20 times its original size and use a ratio of x : y = 15 : 5 for our next zero-shot and few-shot experiments. While we don't say the 15 : 5 ratio for x : y is the best, we say it is good enough to show that our approach works well.",A,0
450,LLM-enhanced Self-training for Cross-domain Constituency Parsing,"In this section, we introduce a vanilla self-training method for cross-domain constituency parsing, which has been investigated in other tasks. Please note that we refer to the standard self-training method as vanilla self-training. The primary goal of self-training is to generate high-quality training instances for the target domain, subsequently using these instances to train the target domain model. The vanilla self-training-based cross-domain constituency parsing is an iterative process aimed at training a target parser. Specifically, in each iteration of the vanilla approach, three main steps are conducted: 1) Training the parser: We train the Berkeley Neural Parser using the source domain constituency treebank. ","In this part, we present a basic self-training technique for cross-domain constituency parsing, which has been studied in other jobs. Note that we call the standard self-training method vanilla self-training. The main objective of self-training is to generate high-quality training examples for the target domain, then using these examples to train the model for the target domain. The vanilla self-training-based cross-domain constituency parsing is a repetitive process with the goal of training a parser for the target domain. Specifically, in each repetition of the vanilla approach, three main steps are taken: 1) Educating the parser: We teach the Berkeley Neural Parser using the source domain constituency treebank.","In this portion, we introduce an unmodified self-training procedure for cross-domain constituency parsing, which has been explored in other tasks. Be aware that we refer to the standard self-training method as vanilla self-training. The primary aim of self-training is to produce high-quality training data for the target domain, subsequently utilizing this data to educate the model for the target domain. The vanilla self-training-based cross-domain constituency parsing is an iterative progression with the objective of developing a parser for the target domain. Precisely, in each cycle of the vanilla approach, three main actions are performed: 1) Instructing the parser: We educate the Berkeley Neural Parser utilizing the source domain constituency treebank.  ","In this segment, we present a basic self-training technique for cross-domain constituency parsing, which has been analyzed in other jobs. Note that we call the standard self-training approach vanilla self-training. The principal purpose of self-training is to create high-quality training samples for the target domain, then employing these samples to develop the model for the target domain. The vanilla self-training-based cross-domain constituency parsing is a repetitive procedure with the goal of constructing a parser for the target domain. Specifically, in each repetition of the vanilla method, three main steps are executed: 1) Teaching the parser: We instruct the Berkeley Neural Parser employing the source domain constituency treebank.",A,0
639,Standardizing Distress Analysis,"Due to its growing impact on public opinion, hate speech on social media has garnered increased attention.  While automated methods for identifying hate speech have been presented in the past, they have mostly been limited to analyzing textual content.  The interpretability of such models has received very little attention, despite the social and legal consequences of erroneous predictions.  In this work, we present a novel problem of Distress Identification and Cause Extraction (DICE) from multimodal online posts.  We develop a multi-task deep framework for the simultaneous detection of distress content and identify connected causal phrases from the text using emotional information. ","Hate speech on social media has become more influential in shaping public views, so it is receiving more attention. Past automatic methods for detecting hate speech have focused on analyzing the text, with little consideration of how understandable the models are. This is concerning given the social and legal results of incorrect predictions. Here we introduce a new challenge of finding distressing content in multimedia online posts and extracting related causes from the text using emotional cues. We build a multi-objective deep system to concurrently recognize distress and pull out related phrases in the text.","Due to its growing sway over public opinion, hateful content on social media platforms has attracted increased focus. While automated techniques for identifying such speech have been presented previously, they have largely concentrated on inspecting the textual information. The interpretability of these models has gotten very little focus, despite the societal and legal consequences of inaccurate predictions. In this work, we put forth a new problem of Identifying and Extracting Distressed Content and Causes (IEDCC) from multimedia online posts. We construct a multi-task deep framework for simultaneously detecting distressing material and pinpointing connected causal text chunks using affective clues.","Hate speech on social media has an expanding influence on public views, so it's receiving more attention. Past automated ways to identify hate speech mostly looked at the text, with little focus on how understandable the models are. This is problematic given the social and legal impacts of wrong predictions. Here we present a new challenge of finding distressing content in multimedia online posts and pulling out related causes from the text using emotional signals. We develop a multi-goal deep system to concurrently detect distress and extract related phrases in the text.",A,0
81,ALDi Quantifying the Arabic Level of Dialectness of Text,"The rest of the sentence has MSA terms that will not sound natural if pronounced according to the phonetic rules of a variant of DA. Unsurprisingly, two annotators considered the sentence to be in MSA, while the third might have perceived the presence of the loanword as a sign of dialectness, thus marking the sentence as little dialectal.","The remaining words in the sentence contain Modern Standard Arabic language terms that would not sound normal if vocalized based on the sound system of a form of Dialectal Arabic. As expected, two people labeling sentences thought this sentence was in Modern Standard Arabic, while the third may have seen the presence of the borrowed word as an indicator of dialect, so they labeled the sentence as a little dialectal.","The rest of the sentence has words from Modern Standard Arabic that would not fit the pronunciation patterns of a type of colloquial Arabic dialect. Not shockingly, two people tagging sentences considered this sentence to be Modern Standard Arabic, while the third perhaps saw the existence of the adopted word as a sign of informality, so they marked the sentence as somewhat informal. ","The other words in the sentence are from Modern Standard Arabic and would sound unnatural if spoken following the phonology of a variety of spoken Arabic dialect. Unsurprisingly, two annotators judging the sentences categorized this sentence as Modern Standard Arabic, while the third may have taken the occurrence of the loanword as a clue that it was a dialect, thus labeling the sentence as somewhat dialectal.",A,0
567,RESEE,"(2) Potentially inaccurate visual knowledge:  though recent explorations come up with using fine-grained images, they are limited in searching from small-scale image caption datasets (e.g., Shen et al.  (2021) employs Flickr30k (Young et al., 2014) for this process).  These defects will introduce knowledge bias into the system (e.g., entity images retrieved from Flickr30k may be wrong or monotonous w.r.t.  given entities in Figure 2) and impair the conversational skills of a dialogue agent.  To overcome the above two shortcomings, we believe:  (1) Compared with session-level visual knowledge, fine-grained visual knowledge such as entity-level image is more competent to help models build a comprehensive understanding of ongoing conversations. ","Though recent explorations utilize detailed images, they are constrained to small image caption datasets (e.g. Shen et al. (2021) uses Flickr30k (Young et al., 2014)). These flaws bring biased knowledge into the system (e.g. entity images from Flickr30k may be inaccurate or monotonous compared to Figure 2 entities) and weaken conversational abilities. To address these issues, we believe: (1) Unlike session visuals, fine details like entity images build more comprehensive understanding of conversations.","While new research leverages granular images, it is limited to small image description sets (Shen et al. (2021) uses Flickr30k (Young et al., 2014)). These problems introduce prejudice into the system (e.g. entity photos from Flickr30k could be wrong or repetitive relative to Figure 2) and impair dialogue skills. To fix this: (1) Unlike session visuals, precise entity images better aid comprehensive conversation understanding.","Although recent work utilizes specific images, it relies on tiny image annotation datasets (e.g. Shen et al. (2021) utilizes Flickr30k (Young et al., 2014)). These flaws introduce bias (e.g. entity images from Flickr30k may be inaccurate or monotonous compared to Figure 2) and weaken conversational ability. To address this: (1) Unlike session images, precise entity images build more holistic conversation understanding.",A,0
583,RESEE,"We employ automatic metrics to assess the model performance:4 (1) Fluency:  perplexity (PPL) measures the confidence of the generated responses; (2) Token-based Relevance:  BLEU (Papineni et al., 2002) and Rouge- L (Lin, 2004); Embedding-based Relevance:  (Serban et al., 2017):  Embedding Average cosine similarity (Avg.), Vector Extrema cosine similarity (Ext.), and Embedding Greedy Matching score (Gre.).  (3) Diversity:  Distinct-1 (Dist-1) and Distinct-2 (Dist-2) (Li et al., 2016) measure the number of distinct unigrams and bi-grams divided by the total grams.  Human Evaluation.  We perform human evaluation over the generated responses.  We consider three conventional criteria:  fluency (Flue.), informativeness (Info.), and relevance (Relv.) following Song et al.  (2021). ","We make use of automated metrics to evaluate the performance of the model: (1) Fluency: perplexity (PPL) calculates the certainty of the produced responses; (2) Token-based Relevance: BLEU (Papineni et al., 2002) and Rouge-L (Lin, 2004); Embedding-based Relevance: (Serban et al., 2017): Average cosine similarity of embeddings (Avg.), Vector Extrema cosine similarity (Ext.), and Greedy Matching score of embeddings (Gre.). (3) Diversity: Distinct-1 (Dist-1) and Distinct-2 (Dist-2) (Li et al., 2016) calculate the number of unique unigrams and bigrams divided by the total n-grams. Human Evaluation. We conduct human evaluation of the generated responses. We consider three standard criteria: fluency (Flue.), informativeness (Info.), and relevance (Relv.) following Song et al. (2021).","We make use of computerized metrics to measure the performance of the model: (1) Fluency: perplexity (PPL) computes the sureness of the created responses; (2) Token-based Relevance: BLEU (Papineni et al., 2002) and Rouge-L (Lin, 2004); Embedding-based Relevance: (Serban et al., 2017): Mean cosine similarity of embeddings (Avg.), Vector Extrema cosine similarity (Ext.), and Greedy Matching score of embeddings (Gre.). (3) Diversity: Distinct-1 (Dist-1) and Distinct-2 (Dist-2) (Li et al., 2016) compute the number of unique unigrams and bigrams divided by the total n-grams. Human Evaluation. We do human evaluation of the generated responses. We consider three common criteria: fluency (Flue.), informativeness (Info.), and relevance (Relv.) following Song et al. (2021). ","We utilize automated metrics to evaluate the performance of the model: (1) Fluency: perplexity (PPL) calculates the confidence of the produced responses; (2) Token-based Relevance: BLEU (Papineni et al., 2002) and Rouge-L (Lin, 2004); Embedding-based Relevance: (Serban et al., 2017): Average cosine similarity of embeddings (Avg.), Vector Extrema cosine similarity (Ext.), and Greedy Matching score of embeddings (Gre.). (3) Diversity: Distinct-1 (Dist-1) and Distinct-2 (Dist-2) (Li et al., 2016) calculate the number of unique unigrams and bigrams divided by the total n-grams. Human Evaluation. We conduct human evaluation of the generated responses. We consider three standard criteria: fluency (Flue.), informativeness (Info.), and relevance (Relv.) following Song et al. (2021).",A,0
490,Model Tells you what to Discard - Adaptive KV Cache Compression for LLMs,"During the prompt encoding phase, model profiling is conducted to discern the behavior of various attention heads, thereby choosing the most appropriate compression strategies for each head. Then, in the token generation phase, rather than indiscriminately appending new key/value vectors, we strategically manage the KV cache in alignment with the selected compression strategies. We will first introduce our profiling method, and then proceed to discuss compression strategies.","In the initial step of encoding the prompt, we analyze the different attention heads to understand how they behave. This allows us to choose the best compression methods for each head. Then, when generating the tokens, instead of blindly adding new key/value vectors, we carefully control the KV cache based on the compression methods we picked. We'll first explain how we profile the heads, then talk about the compression techniques.","During prompt encoding, we study the various attention heads to see how they act. This lets us select the right compression approaches for each head. When generating tokens after that, rather than haphazardly tacking on new key/value pairs, we strategically organize the KV store following the compression plans we chose. We'll start by describing our head profiling process, then explain the compression strategies. ","In encoding the prompt, we inspect the different attention heads to grasp their patterns. This enables us to pick the optimal compression tactics for each head. Subsequently, in token generation, instead of blindly appending new key/value elements, we deliberately manage the KV collection per the selected compression tactics. We will first elucidate our profiling methodology, followed by discussion of the compression strategies.",A,0
265,DisCo Distilled Student Models Co-training for Semi-supervised Text Mining,"For text classification tasks, we compare DisCo with: (i) supervised baselines, BERTBASE and default TinyBERT (Jiao et al., 2020), (ii) semi-supervised UDA (Xie et al., 2020) and FLiText (Liu et al., 2021). We also compare with other prominent SSL text classification methods and report their results on the Unified SSL Benchmark (USB) (Wang et al., 2022a). Most of these SSL methods work well on computer vision (CV) tasks, and Wang et al. (2022a) generalize them to NLP tasks by integrating a 12-layer BERT. More detailed introductions are given in Appendix A.4. For extractive summarization tasks, we compare: (i) supervised basline, BERTSUM (Liu and Lapata, 2019), (ii) two SOTA semi-supervised extractive summarization methods, UDASUM and CPSUM (Wang et al., 2022b), (iii) three unsupervised techniques, LEAD-3, TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004). We use the open-source releases of the competing baselines.","For evaluating performance on text classification tasks, we make comparisons between DisCo and: (i) supervised models, BERTBASE and the default TinyBERT (Jiao et al., 2020), (ii) semi-supervised UDA (Xie et al., 2020) and FLiText (Liu et al., 2021). We also benchmark against other prominent self-supervised learning text classification methods by looking at their results on the Unified SSL Benchmark (USB) (Wang et al., 2022a). Many of these SSL methods are effective on computer vision (CV) tasks, and Wang et al. (2022a) adapt them to NLP by integrating a 12-layer BERT. More details are provided in Appendix A.4. For extractive summarization tasks, we compare against: (i) supervised baseline, BERTSUM (Liu and Lapata, 2019), (ii) two state-of-the-art semi-supervised extractive summarization methods, UDASUM and CPSUM (Wang et al., 2022b), (iii) three unsupervised techniques, LEAD-3, TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004). We utilize the open-source code of the competing baselines.","To evaluate text classification, we pit DisCo against: (i) supervised models BERTBASE and standard TinyBERT (Jiao et al., 2020), (ii) semi-supervised UDA (Xie et al., 2020) and FLiText (Liu et al., 2021). We also hold it up against prominent self-supervised text classification methods by examining their scores on the Unified SSL Benchmark (USB) (Wang et al., 2022a). Many SSL methods thrive on computer vision but Wang et al. (2022a) tailor them to NLP by integrating 12-layer BERT. See Appendix A.4 for more. For summarization, we compare DisCo to: (i) supervised BERTSUM (Liu and Lapata, 2019), (ii) semi-supervised UDASUM and CPSUM (Wang et al., 2022b), (iii) unsupervised LEAD-3, TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004). We use available baseline implementations.","We assess DisCo on text classification against: (i) supervised BERTBASE and default TinyBERT (Jiao et al., 2020), (ii) semi-supervised UDA (Xie et al., 2020) and FLiText (Liu et al., 2021), (iii) prominent self-supervised methods via the Unified SSL Benchmark (USB) (Wang et al., 2022a), adapted from computer vision to NLP by Wang et al. (2022a) using 12-layer BERT. See Appendix A.4. For summarization, we compare DisCo to: (i) supervised BERTSUM (Liu and Lapata, 2019), (ii) semi-supervised UDASUM and CPSUM (Wang et al., 2022b), (iii) unsupervised LEAD-3, TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004). We leverage available baselines.",A,0
578,RESEE,"We employ different encoders for different modality encoding.  In concrete, we utilize transformer blocks (Vaswani et al., 2017) for word encoding, which projects word tokens to a continuous word embedding space.  For image encoding, we utilize CLIP encoder (Radford et al., 2021) to capture the global information of a picture and then use MLP functions to transform it into the same embedding space as the word.  To distinguish different modality information and to identify dialogue contexts from responses, we employ three kinds of token-wise embeddings and sum them up as the input to our transformer-based dialogue systems, namely token embedding, position embedding, and segment embedding. ","We make use of various encoders for encoding different types of data. Specifically, we use transformer blocks (Vaswani et al., 2017) to encode word tokens into a continuous word embedding space. For encoding images, we use the CLIP encoder (Radford et al., 2021) to capture the overall information of an image and then utilize MLP functions to transform it into the same embedding space as words. To differentiate between various data types and to separate dialogue contexts from responses, we utilize 3 kinds of token-level embeddings - token, position and segment - which are summed up as the input to our dialogue systems based on transformers.","We utilize different encoding mechanisms for different data modalities. In particular, we employ transformer blocks (Vaswani et al., 2017) for encoding word tokens into a continuous vector representation. For images, we leverage the CLIP encoder (Radford et al., 2021) to extract global image features and then convert them to the same space as words using MLPs. To distinguish between modalities and dialogue context versus response, we use 3 token-wise embeddings - token, position and segment - which are summed as the input to our transformer dialogue models. ","We make use of various encoders for different modalities. Specifically, we employ transformer blocks (Vaswani et al., 2017) to encode words into continuous embeddings. For images, we use the CLIP encoder (Radford et al., 2021) to obtain global features and transform them into the word space via MLPs. To differentiate modalities and context from response, we utilize 3 token-level embeddings - token, position, segment - that are summed as input to our transformer dialogue systems.",A,0
537,Non-autoregressive Text Editing with Copy-aware Latent Alignments,"GECToR demonstrates remarkable results on many tasks, meanwhile being orders of magnitude faster than its autoregressive counterparts (Rothe et al., 2020). However, several challenges arise as we try to have the cake and eat it. We argue that Seq2Edit works represented by GECToR still suffer from two main issues: i Flexibility: Seq2Edit learns to edit text by predefining a fixed and relatively small (e.g., 5,000) edit vocabulary collected from the training data, which is at the sacrifice of generation flexibility. ii Language generalization: Seq2Edit needs to delve into linguistic features to customize the edit actions, e.g., VB-VBZ for subject-agreement edits and PLURAL for singular-plural form conversions, thus diminishing its ability to generalize to other languages. ","GECToR shows impressive performance on many tasks while being much faster than models that generate text from scratch (Rothe et al., 2020). However, trying to get the best of both worlds comes with challenges. We argue seq2edit models like GECToR still have two key problems: i. Constrained flexibility: Seq2edit learns text edits from a fixed, small set of possible edits seen during training. This limits how flexible it can be. ii. Limited language generalization: Seq2edit relies on linguistic knowledge to customize edits, like subject-verb agreement. This makes adapting to new languages difficult.","GECToR achieves remarkable results on various tasks and is substantially faster than autoregressive models (Rothe et al., 2020). However, aiming to obtain the advantages of multiple approaches introduces issues. We posit seq2edit approaches typified by GECToR still suffer from two primary problems: i. Inflexibility: Seq2edit learns text editing from a predefined, restricted edit vocabulary extracted from the training data, sacrificing generative flexibility. ii. Poor language generalization: Seq2edit depends on linguistic features to tailor edits, like subject-verb agreement, hampering generalization to other languages.  ","GECToR shows impressive performance across tasks while being much faster than autoregressive models (Rothe et al., 2020). However, trying to get the best of both worlds has downsides. We contend seq2edit approaches like GECToR still have two main flaws: i. Limited flexibility: Seq2edit learns from a fixed, small edit vocabulary from training data, restricting flexibility. ii. Weak language generalization: Seq2edit uses linguistic features for edits, like subject-verb agreement, hindering adapting to new languages.",A,0
328,Fifty Shades of Bias," Using 4-tuples is particularly efficient in best-worst annotations because each annotation generates inequalities for 5 out of the 6 possible item pairs. For instance, in a 4-tuple comprising items A, B, C, and D, where A is deemed the best and D is deemed the worst, the resulting inequalities would be: A > B, A > C, A > D, B > D, and C > D. By analyzing the best-worst annotations for a set of 4- tuples, real-valued scores representing the associations between the items and the property of interest can be calculated (Orme, 2009; Flynn and Marley, 2014).","Employing groups of four is especially productive in best-worst annotations because each annotation forms inequalities for 5 of the 6 potential item pairs. For example, in a group of four containing items A, B, C, and D, where A is considered the best and D is considered the worst, the resulting inequalities would be: A is preferred over B, A is preferred over C, A is preferred over D, B is preferred over D, and C is preferred over D. By examining the best-worst annotations for a set of groups of four, real-valued scores representing the connections between the items and the attribute of interest can be determined (Orme, 2009; Flynn and Marley, 2014).","Using sets of four elements is very effective in best-worst tagging because each tag forms preferential relationships for 5 of the 6 feasible pairs of elements. As an illustration, in a set of four comprising elements A, B, C, and D, where A is tagged the best and D is tagged the worst, the resulting preferential relationships would be: A is favored over B, A is favored over C, A is favored over D, B is favored over D, and C is favored over D. By analyzing the best-worst tags for a collection of sets of four elements, numeric scores representing the links between the elements and the characteristic of interest can be calculated (Orme, 2009; Flynn and Marley, 2014).  ","Utilizing groups of four items is especially productive in best-worst marking because each mark generates preferential rankings for 5 of the 6 potential pairs of items. For example, in a group of four with items A, B, C, and D, where A is selected as the best and D is selected as the worst, the resulting preferential rankings would be: A ranks above B, A ranks above C, A ranks above D, B ranks above D, and C ranks above D. By examining the best-worst marks for a collection of groups of four items, numeric values representing the associations between the items and the trait of interest can be determined (Orme, 2009; Flynn and Marley, 2014).",A,0
76,ALDi Quantifying the Arabic Level of Dialectness of Text,"Arabic speakers tend to use MSA in formal situations, and their regional dialects in informal ones. However, an Arabic speaker can still use MSA and speak informally, or use their dialect and speak formally. The case studies described in §5 show how Arab presidents use sentences of different levels of dialectness in their political speeches. While these speeches would all be considered to be formal, using different levels of dialectness might be to sound authoritative (using MSA) or seek sympathy (using a regional dialect). Therefore, we believe the level of dialectness and formality are related yet not interchangeable.","Arabic speakers often use Modern Standard Arabic in official settings and their local dialects in casual ones. However, an Arabic speaker may still utilize Modern Standard Arabic in an informal way, or use their regional dialect in a formal manner. The examples in section 5 demonstrate how Arab presidents include sentences with varying degrees of dialect in their political speeches. Although these addresses would all be seen as formal, using different amounts of regional dialect could be to appear authoritative (with Modern Standard Arabic) or gain empathy (with a local dialect). As a result, we think that the level of dialect and formality are connected but not the same.","Arabic speakers tend to speak Modern Standard Arabic in professional contexts and their own regional dialects in informal ones. But an Arabic speaker can still speak Modern Standard Arabic informally, or use their local dialect formally. The case studies in section 5 exhibit how Arab presidents use sentences with differing quantities of dialect in their political speeches. While these speeches would all be formal, utilizing different extents of dialect could be to sound powerful (with Modern Standard Arabic) or relatable (with a regional dialect). Therefore, we believe the amount of dialect and formality are related but not identical.  ","Arabic speakers often speak Modern Standard Arabic in official settings and their own local dialects in casual ones. However, an Arabic speaker may still speak Modern Standard Arabic informally, or use their regional dialect formally. The examples in section 5 show how Arab presidents use sentences with various degrees of dialect in their political speeches. Although these speeches would all be seen as formal, using different levels of dialect could be to sound authoritative (with Modern Standard Arabic) or approachable (with a regional dialect). As a result, we believe the degree of dialect and formality are connected but not interchangeable.",A,0
754,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"When building the critic, we follow the approach used by Yasunaga et al. (2021) to randomly select 600 grammatical sentences and 600 ungrammatical sentences from the BEA-2019 dev set and Chinese Lang8 dataset as the validation set for English and Chinese, respectively. Hyper-parameter settings. We tune two hyperparameters in our system, the edit distance threshold, as mentioned in §3.2.2, and the masking percentage, denoted as p%, which is outlined in §3.3. We select the edit distance threshold from {1, 2, 3, 4, 5} for English GEC and select the the edit distance threshold from {0, 1, 2} for Chinese. For both English and Chinese p is selected from {5, 10, 15}. ","While constructing the critic, we adopt the technique utilized by Yasunaga et al. (2021) to arbitrarily choose 600 well-formed sentences and 600 ill-formed sentences from the BEA-2019 dev set and Chinese Lang8 dataset as the validation set for English and Chinese, respectively. Parameter configurations. We adjust two hyperparameters in our framework, the edit distance threshold, as stated in §3.2.2, and the masking percentage, denoted as p%, which is outlined in §3.3. We choose the edit distance threshold from {1, 2, 3, 4, 5} for English GEC and select the edit distance threshold from {0, 1, 2} for Chinese. For both English and Chinese p is chosen from {5, 10, 15}.","When putting together the critic, we use the approach employed by Yasunaga et al. (2021) to randomly pick 600 grammatically correct sentences and 600 grammatically incorrect sentences from the BEA-2019 dev set and Chinese Lang8 dataset as the validation set for English and Chinese, correspondingly. Settings for hyperparameters. We fine-tune two hyperparameters in our system, the edit distance limit, as stated in §3.2.2, and the masking percentage, denoted as p%, which is described in §3.3. We choose the edit distance limit from {1, 2, 3, 4, 5} for English GEC and select the edit distance limit from {0, 1, 2} for Chinese. For both English and Chinese p is selected from {5, 10, 15}.  ","In developing the critic, we use the technique used by Yasunaga et al. (2021) to arbitrarily choose 600 well-formed sentences and 600 poorly-formed sentences from the BEA-2019 dev set and Chinese Lang8 dataset as the validation set for English and Chinese, respectively. Adjustments of hyperparameters. We calibrate two hyperparameters in our framework, the edit distance cap, as noted in §3.2.2, and the masking rate, denoted as p%, which is outlined in §3.3. We pick the edit distance cap from {1, 2, 3, 4, 5} for English GEC and choose the edit distance cap from {0, 1, 2} for Chinese. For both English and Chinese p is chosen from {5, 10, 15}.",A,0
196,Counting the Bugs in ChatGPTs Wugs A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,"In this paper, we examine ChatGPT’s morphological behavior on a typologically diverse set of languages: English, German, Tamil, and Turkish. While English and German belong to the same language family, German has a more fusional morphological system than English. Turkish is chosen since it is a non-Indo-European language with a fully agglutinative morphology. Tamil is chosen since it is a Dravidian language exhibiting an agglutinative morphology with fusional elements.","This study analyzes ChatGPT's ability to handle word formation across languages with different morphological patterns: English, German, Tamil, and Turkish. Although English and German are related languages, German words tend to fuse multiple morphemes together more than English does. Turkish was selected because it is from a different language family than English/German and its words are formed by stringing morphemes together. Tamil was included as an example of a Dravidian language that combines agglutinative and fusional morphological processes.","In this work, we inspect ChatGPT's morphological capabilities in four typologically varied languages: English, German, Tamil, and Turkish. English and German are in the same family but German morphology is more fusional. Turkish, being non-Indo-European, has a fully agglutinative morphology. Tamil is Dravidian with an agglutinative morphology and some fusional elements. ","This article investigates ChatGPT's ability to handle word formation in four languages with diverse morphological patterns: English, German, Tamil, and Turkish. Although English and German are related Indo-European languages, German morphology tends to fuse morphemes together more than English. Turkish was selected as a non-Indo-European language with strictly agglutinative morphology. Tamil represents the Dravidian family, exhibiting agglutination along with some fusional morphology.",A,0
306,"Explain, Edit, Generate","We present a novel rationale-sensitive pipeline counterfactual data augmentation method (RACE) to generate logical, diverse, and label-flipping counterfactuals for multi-hop fact verification task. An Explain-Edit-Generate architecture is constructed to generate diverse and logical counterfactual claims based on the rationales. Then, a filter process with two modules is employed to further regularize semantic and topic consistency. Experimental results reveal the improvement in OOD generalization and robustness of the proposed method. Intrinsic evaluation and qualitative evaluation of counterfactual claims show that RACE can generate linguistically diverse and label-flipping counterfactual data while preserving logical relationships.","We introduce a new pipeline approach for creating counterfactual augmented data (RACE) that is sensitive to the reasoning behind claims. It generates logical, varied, and label-changing counterfactuals for multi-step fact checking. An Explain-Edit-Generate design is used to make diverse and logical counterfactual claims based on the reasoning. Then, a filtering process with two components further enforces semantic and topic consistency. Tests showed improvements in out-of-distribution generalization and robustness with this approach. Intrinsic and qualitative evaluation of the counterfactual claims indicate that RACE can produce linguistically diverse and label-flipping counterfactual data while keeping logical relationships intact.","We put forward a fresh rationale-aware process for counterfactually augmenting data (RACE) to produce sensible, wide-ranging, and label-inverting counterfactuals for multi-hop factual verification. An Explain-Edit-Generate structure is built to create diverse and rational counterfactual claims using the rationales. Subsequently, a filtration procedure having two modules is utilized to further regularize semantic and topic congruity. Experimental outcomes exhibit the enhancement in OOD generalization and sturdiness of the proposed method. Inherent and qualitative assessment of counterfactual claims evince that RACE can engender linguistically diverse and label-flipping counterfactual data while conserving logical relationships.","We introduce a new pipeline method for counterfactually augmenting data in a rationale-sensitive way (RACE) to generate logical, varied, and label-changing counterfactuals for multi-step fact checking. An Explain-Edit-Generate architecture is used to create diverse and rational counterfactual claims based on the reasoning. Afterward, a filtering process with two components is employed to further enforce semantic and topic consistency. Experimental results show improvements in out-of-distribution generalization and robustness with this method. Inherent and qualitative evaluation of the counterfactual claims demonstrate that RACE can produce linguistically diverse and label-flipping counterfactual data while retaining logical relationships.",A,0
72,ALDi Quantifying the Arabic Level of Dialectness of Text,"Much of this work has been done at the sentence or document level, but there has also been work on token-level DI for code-switching, for example on Egyptian Arabic-MSA tweets (Solorio et al., 2014; Molina et al., 2016) and on Algerian Arabic (Adouane and Dobnik, 2017). Both sentence-level and token-level DI methods fail to distinguish between sentences having the same number of dialectal cues, yet different levels of dialectness.","A significant portion of this research has focused on the sentence or document level, however there have also been efforts on token-level DI for code-switching, for instance on tweets mixing Egyptian Arabic and MSA (Solorio et al., 2014; Molina et al., 2016) and on Algerian Arabic (Adouane and Dobnik, 2017). Both sentence-level and token-level DI approaches are unable to differentiate between sentences containing the same quantity of dialectal hints, despite having varying degrees of dialectness.","Much of these studies have examined the sentence or document scale, but some have also worked on token-level DI for code-switching, like on Egyptian Arabic-MSA tweets (Solorio et al., 2014; Molina et al., 2016) and Algerian Arabic (Adouane and Dobnik, 2017). Both sentence-level and token-level DI techniques fail to distinguish sentences with the same number of dialectal markers, though with differing extents of dialectness. ","A large portion of this work has focused on the sentence or document level, however there have also been efforts on token-level DI for code-switching, for instance on tweets mixing Egyptian Arabic and MSA (Solorio et al., 2014; Molina et al., 2016) and Algerian Arabic (Adouane and Dobnik, 2017). Both sentence-level and token-level DI methods are unable to differentiate between sentences containing the same number of dialectal indicators, despite possessing varying degrees of dialectness.",A,0
495,Model Tells you what to Discard - Adaptive KV Cache Compression for LLMs,"As in Figure 3, attention heads in different layers have vastly different structures. Specifically, for the initial and final layers, they have more attention heads assigned to the full KV cache, indicating more attention heads of these layers broadly attend to all tokens. Meanwhile, for middle layers, the attention map focuses more on special tokens, indicating most attention heads of these layers primarily attend to special tokens (i.e., the accumulated attention score on special tokens is higher than 0.95 for these attention heads).","Similar to Figure 3, the attention heads across the various layers have very different patterns. In particular, the first and last layers have more attention heads that are looking at the entire KV cache, meaning those heads are focused on all of the tokens. In contrast, the middle layers have attention maps that are concentrated on the special tokens, so most of the heads in those layers are primarily focused on just the special tokens (where over 0.95 of the attention score for those heads is on the special tokens).","As shown in Figure 3, the attention heads in the different layers have very distinct structures. Specifically, the initial and final layers have more attention heads that cover the full KV cache, signifying those heads broadly focus on all tokens. However, the middle layers have attention maps that are more concentrated on the special tokens, meaning the majority of attention heads in those layers chiefly focus on the special tokens (where the accumulated attention score on the special tokens is higher than 0.95 for those heads).","Similar to Figure 3, the attention heads in each layer have very varied patterns. In particular, the first and last layers have more attention heads looking at the whole KV cache, denoting those heads pay attention to all tokens. In contrast, the middle layers have attention maps more focused on special tokens, indicating most heads in those layers primarily concentrate on just the special tokens (where over 0.95 of the attention score for those heads is on special tokens).",A,0
411,INTERFAIR Debiasing with Natural Language Feedback for Fair Interpretable Predictions,"In the unconstrained setup, users are able to modify bias rationales in such a way that improves task performance while decreasing bias. Most importantly, even though 81% (Full Text performance) is the upper bound of accuracy for purely training-based frameworks, users achieve a better task performance (4-5%) while keeping the bias in rationales minimal. In both setups, gradient-based changes in model states are superior to the heuristic strategy to modify the final task rationales. Since unconstrained setup can also confuse users and may lead to failure modes, we see the lowest bias F1 is achieved in the unconstrained setup; however, users were able to keep the bias as low as the INTERFAIR-base model in all interactive settings.","In the unrestricted configuration, users can alter bias explanations in a way that improves job effectiveness while reducing prejudice. Most notably, even though 81% (Full Text performance) is the maximum accuracy for purely training-centered frameworks, users attain superior task results (4-5%) while keeping the bias in explanations minimal. In both arrangements, gradient-founded shifts in model conditions are better than the heuristic plan to change the final task explanations. Since the unrestricted setup can also perplex users and may lead to failure methods, we see the lowest bias F1 is reached in the unrestricted setup; however, users were able to keep the bias as low as the INTERFAIR-base model in all interactive settings.","In the unhindered system, users can modify biased clarifications in a manner that enhances work productivity while decreasing unfairness. Critically, despite 81% (Full Text execution) being the ceiling of precision for purely training-focused structures, users produce superior task outputs (4-5%) while maintaining minimal bias in clarifications. In both systems, gradient-established alterations in model circumstances exceed the heuristic tactic to modify the final task clarifications. Because the unhindered system can also confuse users and may result in failure tactics, we observe the lowest bias F1 is attained in the unhindered system; however, users could keep the bias as low as the INTERFAIR-base model in all interactive configurations.  ","In the unimpeded arrangement, users can change biased elucidations in a way that boosts work efficiency while lessening inequity. Importantly, even though 81% (Full Text functioning) is the maximum accuracy for purely training-centered frameworks, users generate superior task results (4-5%) while preserving minimal bias in elucidations. In both arrangements, gradient-founded shifts in model conditions surpass the heuristic plan to alter the final task elucidations. Since the unimpeded arrangement can also bewilder users and may cause failure plans, we discern the lowest bias F1 is reached in the unimpeded arrangement; however, users could maintain the bias as low as the INTERFAIR-base model in all interactive configurations.",A,0
595,RESEE,"Open-domain dialogue models aim at responding to general human-like conversations in various circumstances.  While dialogue generation has a rich history, the area has made significant progress with the rising of pretrained models in varied linguistic domains (Zhang et al., 2020; Mi et al., 2022; Zhu et al., 2023b; Touvron et al., 2023b).  The introduction of external knowledge in traditional models plays a vital role in leading them to intellectual dialogue agents.  For example, Wu et al.  (2021) leveraged three domains of knowledge to enhance the model performance in Chinese contexts.  Wang et al.  (2022) employed an extra retrieval process to find knowledgeable evidence as input to enlarge dialogue model capacities. ","Open-ended chatbot models try to have general human-like chats in many situations. While making chatbots has a long history, the field has advanced a lot with the development of pre-trained models in many language areas (Zhang et al., 2020; Mi et al., 2022; Zhu et al., 2023b; Touvron et al., 2023b). Adding external knowledge to traditional models is key to making them into intelligent chat agents. For instance, Wu et al. (2021) used 3 knowledge domains to improve models for Chinese. Wang et al. (2022) added a retrieval step to find knowledgeable evidence as input to expand chatbot abilities.","Chatbots that can discuss any topic aim to have human-like conversations in diverse circumstances. Although chatbot creation has a rich past, progress accelerated with pre-trained models in various language domains (Zhang et al., 2020; Mi et al., 2022; Zhu et al., 2023b; Touvron et al., 2023b). Incorporating outside knowledge into traditional models is crucial for making intellectual chatbots. Wu et al. (2021) leveraged 3 knowledge areas to enhance Chinese chatbot performance. Wang et al. (2022) used an extra retrieval process to find informative evidence as input to expand chatbot skills.","Chatbots designed for open-ended discussion try to mimic human conversations in many situations. Despite chatbot development having a long history, breakthroughs occurred with pre-trained models covering diverse language domains (Zhang et al., 2020; Mi et al., 2022; Zhu et al., 2023b; Touvron et al., 2023b). Integrating external knowledge into traditional models is key for intellectual chatbots. Wu et al. (2021) employed 3 knowledge domains to improve Chinese chatbot abilities. Wang et al. (2022) utilized an extra retrieval step to identify knowledgeable evidence as input to expand chatbot capacities.",A,0
204,Counting the Bugs in ChatGPTs Wugs A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,"This is not the case, however, for tense markers. Among linguists working on Tamil, it is not completely agreed upon how many verb classes there are in the language, with some proposing up to 13 and others as few as three (Lisker, 1951; Agesthialingom, 1971). In the spoken form of Tamil, there are points where verbs are part of completely different classes than their literary counterpart, so in this study we focus exclusively on the written form (Schiffman and Renganathan, 2009). To simplify the analysis, we utilize a modification of Graul’s classification seen in The English Dictionary of the Tamil Verb, where there are seven primary classes (Schiffman and Renganathan, 2009).","However, this is not true for markers of tense. There is disagreement among experts studying Tamil regarding the number of verb classes in the language, with proposals ranging from as many as 13 down to just 3 classes (Lisker, 1951; Agesthialingom, 1971). Between spoken and written Tamil, verbs can fall into completely different classes, so this study concentrates only on the written form (Schiffman and Renganathan, 2009). To simplify the analysis, we use a modified version of Graul's classification from The English Dictionary of the Tamil Verb, which has seven main classes (Schiffman and Renganathan, 2009).","Nevertheless, tense markers are an exception. Linguists who study Tamil do not completely agree on the number of verb classes in the language, with suggestions going from 13 to as few as 3 (Lisker, 1951; Agesthialingom, 1971). Verbs can belong to totally different classes when comparing spoken versus literary Tamil, so this study focuses exclusively on the written form (Schiffman and Renganathan, 2009). To make the analysis easier, we use a tweaked version of Graul's classification from The English Dictionary of the Tamil Verb, which has seven primary classes (Schiffman and Renganathan, 2009).  ","However, this is not the situation for markers of tense. There is no complete consensus among Tamil linguists regarding the quantity of verb classes in the language, with proposals ranging from 13 to just 3 classes (Lisker, 1951; Agesthialingom, 1971). Verbs can fall into completely separate classes when contrasting spoken and written Tamil, so this study concentrates solely on the written form (Schiffman and Renganathan, 2009). To simplify the analysis, we employ a modified version of Graul's classification in The English Dictionary of the Tamil Verb, which contains seven main classes (Schiffman and Renganathan, 2009).",A,0
63,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Although our work makes a further progress in the challenging zero-shot slot filling, it is subject to several potential limitations. Firstly, since slot label sequence is used as the prefix of the utterance, this directly results in a long input sequence. Secondly, our method may be negatively affected by severe label ambiguity. There are some slot entities with rather similar semantics, leading to wrong slot type predictions. For example, “book a manadonese restaurant”, the slot entity type of “manadonese” is actually cuisine, but is easily identified as country.","While our research represents an advance in the difficult task of zero-shot slot filling, it has some possible shortcomings. First, using the slot label sequence as a prefix for the utterance leads to a very long input sequence. Second, our approach can struggle with highly ambiguous labels. Some slot entities have quite similar meanings, resulting in incorrect slot type predictions. For instance, ""book a manadonese restaurant"", the slot type for ""manadonese"" should be cuisine but it's easily mistaken as country.","Although our work makes progress on zero-shot slot filling, a challenging problem, it has a few potential weaknesses. To start, prefixing the utterance with the slot label sequence produces a very long input. Also, severe label ambiguity can negatively impact our method. Some slots have quite similar semantics, so the wrong slot type may be predicted. For example, ""book a manadonese restaurant"" - ""manadonese"" is actually cuisine but could easily be identified as country. ","While our research advances zero-shot slot filling, a difficult task, some possible limitations exist. First, prepending the slot label sequence creates lengthy inputs. Second, high label ambiguity can hinder our approach. Some slots have very similar meanings, leading to incorrect slot type predictions. For instance, ""book a manadonese restaurant"" - ""manadonese"" should be cuisine but may be wrongly labeled as country.",A,0
311,"Explain, Edit, Generate","To avoid imbalance classes, we randomly select half of the SUP instances and half of the REF instances for perturbation and each perturbation strategy is employed with equal probability. Finally, the fine-tuned RoBERTa-base classifier has 81.23% on label accuracy of claim verification on NEI augmented HOVER development set. The statistics of NEI augmented HOVER are shown in Table 6.","In order to prevent skewed class distributions, we arbitrarily chose half of the SUP samples and half of the REF samples for modification, and each modification approach had an equal chance of being utilized. Ultimately, the fine-tuned RoBERTa-base categorizer achieved 81.23% label accuracy on claim confirmation on the NEI enhanced HOVER development set. The statistics for the NEI boosted HOVER are displayed in Table 6.","To avoid having some classes be much larger than others, we randomly selected 50% of the SUP cases and 50% of the REF cases to be altered, and applied each alteration method with equal likelihood. In the end, the fine-tuned RoBERTa-base classifier attained 81.23% accuracy on labeling claims as true or false on the NEI expanded HOVER dev set. The details of the NEI expanded HOVER are presented in Table 6.  ","In order to prevent having classes of very different sizes, we arbitrarily picked half of the SUP examples and half of the REF examples to modify, and used each modification technique with equal probability. Ultimately, the fine-tuned RoBERTa-base classifier reached 81.23% accuracy at labeling claims as verified or not on the NEI enhanced HOVER development set. The data for the NEI enhanced HOVER is shown in Table 6.",A,0
680,Standardizing Distress Analysis,"A.2 Annotation Guidelines: Our annotation guidelines are rooted in the works of (Poria et al., 2021; Ghosh et al., 2022c).  The annotators were instructed to identify the set of causal spans that accurately depict the reasons for a post being tagged as distressed given an input post with that label.  The annotators annotated a post with the No_cause tag if the cause of the post was latent, that is, if there was no stated causal span.  Two human experts—graduate students with adequate task knowledge—annotated every post.  We used the union of candidate spans from distinct annotators as the final causal span only when the size of their intersection was at least 50% of the size of the smallest candidate span. ","Our annotation instructions are based on the work of (Poria et al., 2021; Ghosh et al., 2022c). The people labeling the data were told to find the parts of the text that accurately show the reasons why a post was marked as distressed. The labelers gave a post the No_cause tag if the reason for distress was hidden, meaning there was no stated cause. Two expert humans—graduate students who understood the task—labeled each post. We only used the union of spans from the different labelers as the final cause if the overlap between them was at least 50% of the size of the smaller span.","Our guidelines for annotating come from the research of (Poria et al., 2021; Ghosh et al., 2022c). The people doing the labeling were instructed to identify the spans of text that accurately capture why a post was classified as distressed. If the cause of distress was implicit, the post was given a No_cause tag by the labelers. Two human specialists—graduate students with sufficient knowledge of the task—annotated each post. We only used the combined spans from the separate labelers as the final cause when the intersection between them was at least 50% of the length of the smaller span.","Our principles for labeling are based on the studies of (Poria et al., 2021; Ghosh et al., 2022c). The annotators were told to pinpoint the parts of the text that accurately explain why a post got a distressed tag. If the reason for distress was unstated, the post received a No_cause tag from the annotators. Two expert people—graduate students who comprehended the task well—labeled every post. We only utilized the union of spans from the different annotators as the final cause if the overlap was at least 50% of the size of the smaller span.",A,0
404,INTERFAIR Debiasing with Natural Language Feedback for Fair Interpretable Predictions,"Specifically, we build a parser that encodes the NL feedback, the bias variable (e.g., gender), and the original task input and produces a sequence of High / Low / NA labels for the complete input token sequence. An example feedback and its parse are shown in Table 1. Such an approach allows us to encode complex feedback on multiple input tokens (see Figure 1). Since we do not have large annotated data for the parsing task, we instead adopt a few-shot framework, following (Slack et al., 2022). We use a large language model (e.g. GPT-3; text-davinci-003) as they have strong priors for language understanding (here, parsing) tasks from their pre-training phase. We use a few demonstrative parsing examples for in-context learning of the parser. See the parsing task example in Table 1.","Specifically, we construct a parser that encodes the natural language feedback, the predisposition variable (e.g., sex), and the original task input and generates a sequence of High / Low / NA labels for the complete input token sequence. An illustration of feedback and its analysis is shown in Table 1. This approach lets us encode intricate feedback on multiple input tokens (see Figure 1). Since we do not have ample annotated data for the parsing task, we instead use a few-shot framework, following (Slack et al., 2022). We utilize a large language model (e.g. GPT-3; text-davinci-003) as they have robust priors for language understanding (here, parsing) tasks from their pre-training phase. We use a few demonstrative parsing examples for in-context learning of the parser. See the parsing task example in Table 1.","In particular, we develop a parser that encodes the natural language feedback, the inclination variable (e.g., gender), and the original task input and produces a sequence of High / Low / NA labels for the complete input token sequence. An instance of feedback and its parsing is shown in Table 1. This approach enables us to encode intricate feedback on multiple input tokens (see Figure 1). Since we do not have substantial annotated data for the parsing task, we instead use a few-shot framework, following (Slack et al., 2022). We leverage a large language model (e.g. GPT-3; text-davinci-003) as they have strong priors for language understanding (here, parsing) tasks from their pre-training phase. We use a few example parsings for in-context learning of the parser. See the parsing task example in Table 1.  ","Specifically, we construct a parser that encodes the natural language feedback, the predisposition variable (e.g., gender), and the original task input and generates a sequence of High / Low / NA labels for the complete input token sequence. An example of feedback and its analysis is shown in Table 1. This approach permits us to encode complex feedback on multiple input tokens (see Figure 1). Since we do not have substantial annotated data for the parsing task, we instead adopt a few-shot framework, following (Slack et al., 2022). We use a large language model (e.g. GPT-3; text-davinci-003) as they have strong priors for language understanding (here, parsing) tasks from their pre-training phase. We use a few demonstrative parsing examples for in-context learning of the parser. See the parsing task example in Table 1.",A,0
703,"Unnatural Error Correction, GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text","Besides, we also introduce two additional datasets:   DREAM (Sun et al., 2019) and AQuARAT (Ling et al., 2017).  DREAM is a dialoguebased multiple-choice reading comprehension dataset.  AQuA-RAT is a dataset of math word problems necessitating multi-step reasoning for their resolution.  For DREAM dataset, we constructed the dataset by selecting 1025 samples with annotated categories from the development and test sets and then scrambling the dialogue part of each question.  For AQuA-RAT dataset, we adopt the few-shot Chain of Thought (CoT) setting as in Wei et al. 2022b and evaluate LLMs with scrambled questions in samples and demonstrations.  For each dataset, we generate scrambled text with various scramble types and rates. ","Moreover, we present two more datasets: DREAM (Sun et al., 2019) and AQuARAT (Ling et al., 2017). DREAM contains dialogue-based multiple choice reading comprehension questions. AQuARAT has math word problems that need multi-step reasoning to solve. For DREAM, we made a dataset by taking 1025 samples with labeled categories from the development and test sets, then jumbling the dialogue part of each question. For AQuARAT, we use the few-shot Chain of Thought (CoT) setting like in Wei et al. 2022b and test LLMs with scrambled questions in examples and demos. For both datasets, we make scrambled text with different scramble types and amounts.","In addition, we use two other datasets: DREAM (Sun et al., 2019) and AQuARAT (Ling et al., 2017). DREAM has dialogue-style multiple choice reading comprehension questions. AQuARAT contains math word problems needing multiple steps of reasoning to answer. For DREAM, we picked 1025 labeled samples from the dev and test sets, scrambling the dialogue in each question to make a dataset. For AQuARAT, we utilize the few-shot Chain of Thought (CoT) method from Wei et al. 2022b, evaluating LLMs on scrambled questions in examples and walkthroughs. For both datasets, we scramble the text in various ways and percentages.  ","Furthermore, we utilize two more datasets: DREAM (Sun et al., 2019) and AQuARAT (Ling et al., 2017). DREAM features dialogue-form multiple choice reading comprehension challenges. AQuARAT provides math word problems necessitating multiple reasoning steps to solve. For DREAM, we constructed a dataset by taking 1025 annotated samples from the development and test sets and randomizing the dialogue component of each query. For AQuARAT, we employ the few-shot Chain of Thought (CoT) framework as in Wei et al. 2022b, assessing LLMs using scrambled questions in instances and tutorials. For both datasets, we generate scrambled text with different scramble types and frequencies.",A,0
395,INTERFAIR Debiasing with Natural Language Feedback for Fair Interpretable Predictions,"However, in an ideal situation, a model should use only the necessary amount of information, irrespective of bias, to achieve an acceptable task performance. This trade-off between task performance and bias mitigation is subjective or varies between users (Yaghini et al., 2021) and is often hard to achieve via learning from data (Zhang et al., 2018; He et al., 2022). Figure 1 shows the limit of an algorithmic approach where ignoring all gendered information can lead to a wrong result.","Nevertheless, in a perfect scenario, a system should utilize only the required quantity of data, regardless of prejudice, to attain sufficient task effectiveness. This compromise between task success and bias reduction is subjective or differs between individuals (Yaghini et al., 2021) and is frequently challenging to accomplish via learning from information (Zhang et al., 2018; He et al., 2022). Figure 1 displays the restriction of a computational technique where overlooking all gendered details can result in an incorrect outcome.","However, in an best case, a program ought to leverage only the essential volume of insights, independent of biases, to reach adequate task performance. This balance between task accomplishment and unfairness mitigation is relative or varies across users (Yaghini et al., 2021) and is regularly difficult to achieve through deriving from data (Zhang et al., 2018; He et al., 2022). Figure 1 exhibits the limit of an algorithmic methodology where disregarding all gendered data can prompt an inaccurate result. ","Though, in a perfect case, a system should employ only the required amount of information, no matter biases, to gain sufficient task effectiveness. This equilibrium between task success and unfairness reduction is subjective or differs among individuals (Yaghini et al., 2021) and is often challenging to realize via deriving from data (Zhang et al., 2018; He et al., 2022). Figure 1 displays the restriction of a computational method where overlooking all gendered details can lead to a wrong outcome.",A,0
368,INSTRUCTSCORE,"Our INSTRUCTSCORE enjoys the following advantages: (i) Compact yet competitive: INSTRUCTSCORE’s 7B version displays strong performance compared to metrics based on closed-source 175B LLMs. (ii) Explainable: INSTRUCTSCORE provides natural language explanations to justify numerical scores. (iii) Generalizable: The unsupervised training pipeline does not require human annotations, making it easily adaptable to different domains and tasks.","Our INSTRUCTSCORE has these benefits: (i) Small but powerful: INSTRUCTSCORE's 7B model has good results compared to metrics using proprietary 175B large models. (ii) Understandable: INSTRUCTSCORE gives explanations in natural language to support numerical scores. (iii) Adaptable: The unsupervised learning process doesn't need human-labeled data, so it can be easily tailored to new areas and jobs.","Our INSTRUCTSCORE has these advantages: (i) Efficient yet strong: INSTRUCTSCORE's 7B edition has impressive performance compared to metrics relying on closed-source 175B huge models. (ii) Interpretable: INSTRUCTSCORE provides explanations in plain language to validate numerical marks. (iii) Flexible: The unsupervised preparing workflow doesn't require human-annotated data, making it simply adjustable to various domains and tasks.  ","Our INSTRUCTSCORE has these positive aspects: (i) Streamlined but mighty: INSTRUCTSCORE's 7B variant displays robust capabilities compared to metrics utilizing proprietary 175B enormous models. (ii) Elucidating: INSTRUCTSCORE furnishes clarifications in natural speech to substantiate quantitative appraisals. (iii) Adaptable: The unsupervised honing procedure doesn't necessitate human-marked information, rendering it handily pliable to discrete spheres and jobs.",A,0
712,"Unnatural Error Correction, GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text","Validating these hypotheses could potentially enhance our understanding of the inner workings of LLMs, thereby enabling us to reverseengineer and recreate more sophisticated models like GPT-4. Input:   The following sentence contains words with scrambled letters.  Please recover the original sentence from it.  Scrambled sentence:   A reGebr byba ulfaorm wsa titbudiserd ot soetsr epdstie a lclera eovr bslpioes ionmanantitco, grnoadicc ot eth ADF.  heT pyomacn si noniacrggue rptsean ot ckhec yna poducsrt yhte evah ta mhoe nda cdisadr sehot taht aym eb ecaeftdf.  Output:   A Gerber baby formula was distributed to stores despite a recall over possible contamination, according to the FDA.  The company is encouraging parents to check any products they have at home and discard those that may be affected.","Validating these theories could enhance our comprehension of the internal operations of large language models, thereby allowing us to deconstruct and recreate more advanced systems like GPT-4.","Confirming these hypotheses might improve our understanding of how LLMs work inside, thus enabling us to reverse engineer and build more sophisticated models such as GPT-4.  ","Proving these assumptions could boost our insight into the inner workings of large language models, thus permitting us to backtrack and develop more complex systems such as GPT-4.",A,0
334,Fifty Shades of Bias,"To further promote syntactic diversity for the generated samples, we prompted the model to do generations across three formats: (a) Conversation, (b) Conversion, and (c) Completion. In the first, we prompted the model to generate a biased conversation against the provided seed, whereas, in (b) and (c), we prompted the model to convert and complete the provided seed, respectively (prompts shown in Appendix A.1). Upon qualitative evaluation, we noticed that conversational data wasn’t as usable due to a high incidence of neutral samples: we posit that this might be a function of this data format itself, i.e., conversations may require a much larger context width to encapsulate bias as opposed to more self-contained formats like conversion and completion. Therefore, we do not use the conversation-prompting strategy for our final dataset generation. Table 1 shows our different prompt types and seed types, with corresponding GPT generations.","To further increase the diversity of syntax for the produced samples, we instructed the model to generate text across three styles: (a) Dialogue, (b) Transformation, and (c) Finalization. In the first, we told the model to create a one-sided dialogue against the given seed, while for (b) and (c), we asked the model to change and finish the provided seed, respectively (prompts are in Appendix A.1). After reviewing them qualitatively, we noticed the dialogue data was not as useful due to many neutral samples: we believe this may be because of the format itself, i.e. conversations likely need a much wider context to capture bias compared to more self-contained styles like transformation and finalization. Therefore, we do not use the dialogue prompting approach for our final dataset creation. Table 1 displays our different prompt and seed types, with matching GPT generations.","To further increase the syntactic diversity of the generated samples, we instructed the model to produce text in three formats: (a) Dialog, (b) Conversion, and (c) Completion. In the first, we told the model to generate a biased dialog against the provided seed, while in (b) and (c), we prompted the model to convert and finish the provided seed, respectively (prompts are in Appendix A.1). After qualitative evaluation, we noticed the dialog data was not as useful due to many neutral samples: we believe this may be because of the format itself, i.e. dialogs likely require a much broader context to capture bias compared to more self-contained formats like conversion and completion. As a result, we do not use the dialog prompting method for our final dataset creation. Table 1 shows our different prompt and seed types, along with corresponding GPT generations.","To further increase the syntactic variety of the generated samples, we instructed the model to produce text across three styles: (a) Conversation, (b) Transformation, and (c) Completion. In the first, we prompted the model to generate a one-sided biased conversation against the provided seed, while in (b) and (c), we prompted the model to transform and complete the provided seed, respectively (prompts are in Appendix A.1). After qualitative assessment, we noticed the conversation data was not as useful due to many neutral samples: we believe this may be because of the format itself, i.e. conversations likely require a much wider context to encapsulate bias compared to more self-contained styles like transformation and completion. Therefore, we do not use the conversation prompting approach for our final dataset creation. Table 1 displays our different prompt and seed types, along with matching GPT generations.",A,0
7,A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Our contributions are three-fold: (1) we provide one the few studies on bias in instruction-tuned models to-date. Focusing on the example of MT and gender bias, we show that despite getting better at zero-shot translation, such models default to male-inflected translations, even in the presence of overt female pronouns and disregarding female occupational stereotypes. (2) To our knowledge, we are among the first to acknowledge the potential of interpretability methods to study IFT language models and why they produce biased predictions.","We have three main contributions: (1) We present one of the first examinations of prejudice in models fine-tuned on training data. Looking at machine translation and gender bias, we demonstrate that although these models improve at translating unseen data, they still lean towards male word choices, even when clear female pronouns are present and female occupational stereotypes are ignored. (2) As far as we know, we are some of the first to recognize the potential of interpretability techniques to analyze IFT language models and why they make biased forecasts.","Our paper has three key additions: (1) We provide a rare investigation into bias in models tuned on instructional data. With machine translation and gender bias as an example, we show that while these models get better at zero-shot translation, they still default to male word forms, disregarding female pronouns and occupational stereotypes. (2) To our knowledge, we are among the first to leverage interpretability methods to inspect IFT language models and understand their biased predictions.","We make three important contributions: (1) We present a scarce examination of prejudice in models trained on instructional information. Looking at machine translation and gender bias, we demonstrate that although these models improve at unsupervised translation, they still favor male terms, ignoring female pronouns and job stereotypes. (2) As far as we know, we are some of the first to employ interpretability techniques to study IFT language models and their biased outputs.",A,0
658,Standardizing Distress Analysis,"This is called the ""shattered gradients problem"".  To address this, residual weights (Balduzzi et al., 2017) provide the gradients with some spatial structure, which aids in training, as shown in Figure 3b.  Inter-modal Fusion (IMF).  The IMF module exchanges information and aligns entities across modalities (text and image) to learn joint intermodality representations.  Figure 4 illustrates the mechanism of inter-modal fusion.  Text infused visual features (and vice-versa).  We use an external word embedding model to build high-level representations (Ti ’) for an image-text 5https: ","This is referred to as the ""fragmented gradients issue"". To tackle this, residual weights (Balduzzi et al., 2017) give the gradients some spatial form, which assists in training, as depicted in Figure 3b. Inter-Modality Blending (IMB). The IMB module swaps information and aligns entities across text and image modalities to acquire collective cross-modality representations. Figure 4 shows the process of inter-modality blending. Text-enriched visual characteristics (and the reverse). We utilize an external word embedding framework to construct high-level representations (Ti') for an image-text pair.","This is known as the ""shattered slopes dilemma"". To address this, residual masses (Balduzzi et al., 2017) impart the slopes with some spatial makeup, which helps with education, as exhibited in Figure 3b. Between-Modality Consolidation (IMC). The IMC element trades data and harmonizes entities across text and visual modalities to learn collaborative between-modality depictions. Figure 4 demonstrates the instrument of between-modality consolidation. Text-infused visual properties (and vice versa). We employ an external word embedding model to assemble high-level portrayals (Ti') for a visual-text pair. ","This is called the ""fragmented gradients issue"". To tackle this, residual loads (Balduzzi et al., 2017) lend the gradients some spatial form, which assists in learning, as pictured in Figure 3b. Inter-Modality Integration (IMI). The IMI unit exchanges information and harmonizes entities across text and visual modalities to acquire collective cross-modality representations. Figure 4 shows the means of inter-modality integration. Text-enriched visual features (and the flipside). We use an external word embedding model to build high-level illustrations (Ti') for a visual-text couple.",A,0
101,Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering,"In contrast to the direct inference, TTA exacerbates this imbalanced distribution, making all outputs of the model to be a specific class. Therefore, we propose Anti-Collapse Fast test-time adaptation (Anti-CF), which utilizes the output of the source model as a soft label to regularize the update of the adapted model during test time to ensure that the adapted model will not deviate too far from the source model, thus avoiding model collapse.","Unlike direct deduction, TTA worsens this uneven allocation, causing all outputs of the model to be a certain type. As a result, we put forward Anti-Collapse Fast test-time adaptation (Anti-CF), which leverages the output of the source model as a soft tag to control the update of the adapted model during test time. This ensures the adapted model does not stray too far from the source model, thereby avoiding model failure.","In opposition to straightforward inference, TTA amplifies this imbalanced distribution, making all outputs of the model belong to a specific category. Therefore, we present Anti-Collapse Fast test-time adaptation (Anti-CF), which harnesses the output of the source model as a flexible label to regulate the adaptation of the model during test time. This guarantees the adapted model does not diverge excessively from the source model, thus circumventing model breakdown.  ","Contrary to direct conclusion, TTA intensifies this uneven dissemination, causing all outputs of the model to be of a certain type. As a result, we bring forward Anti-Collapse Fast test-time adaptation (Anti-CF), which utilizes the output of the source model as a malleable marker to control the modification of the adapted model during test time. This ensures the adapted model does not stray too far from the source model, thereby avoiding model disintegration.",A,0
314,Fifty Shades of Bias,"Harms perpetuated due to human biases are innumerable, and gender bias is one of the most prevalent biases in our society. Past work shows the harm due to gender-based bias ranges from underrepresentation in media stories (Asr et al., 2021) to mis- or no representation in computational models (Bolukbasi et al., 2016; Sun et al., 2019). Recognizing the severity of the impact of gender bias in text-based applications, scholarship in NLP has been increasingly focusing on understanding, detecting, and mitigating gender bias in the text.","The damage caused by prejudices held by people is endless, and sexism is one of the most common biases in our world. Prior research demonstrates the injury resulting from sexist bias spans from lack of representation in media accounts (Asr et al., 2021) to incorrect or absent representation in computer programs (Bolukbasi et al., 2016; Sun et al., 2019). Acknowledging the gravity of the effect of sexism in text-focused applications, academic work in NLP has been progressively concentrating on comprehending, identifying, and reducing sexist bias in writing.","Harm done due to assumptions made by humans is limitless, and gender discrimination is one of the most widespread biases in our civilization. Earlier work shows the detriment due to gender-focused bias includes underrepresentation in news stories (Asr et al., 2021) to inaccurate or missing representation in automated systems (Bolukbasi et al., 2016; Sun et al., 2019). Understanding the seriousness of the impact of gender discrimination in text-oriented applications, studies in NLP have been increasingly focusing on grasping, detecting, and lessening gender discrimination in text.","Injuries caused by the prejudices of people are endless, and sexism is one of the most common biases in our society. Past studies show the damage from sexist bias includes underrepresentation in media accounts (Asr et al., 2021) to false or absent representation in computerized models (Bolukbasi et al., 2016; Sun et al., 2019). Recognizing the severity of the effect of sexism in text-reliant applications, research in NLP has been increasingly concentrating on comprehending, identifying, and reducing sexist bias in writing.",A,0
631,SOUL,"In particular, Chat- GPT appears to have difficulty with the not-given class, due to its overconfidence to misclassify the not-given class as false. This failure shows the challenges posed by SOUL and emphasizes that a large model size alone is not sufficient to ensure comprehensive sentiment capabilities. Justification Generation We exclude Roberta from the JG task as it is a discriminative model and not well-suited for text generation tasks. The results for the JG task are presented in Table 3. We report commonly used text generation metrics as similarity evaluation and overall accuracy for reference. When it comes to accuracy, it appears that SLMs and Flan-T5XXL show either minimal improvement or even a negative impact. ","Specifically, ChatGPT seems to struggle with the not-given category, since it is overconfident and incorrectly classifies the not-given examples as false. This shortcoming highlights the difficulties presented by SOUL and shows that having a large model size alone does not guarantee comprehensive sentiment skills. Justification Writing We leave out Roberta for the justification generation task because it is a discriminative model and not suitable for text generation assignments. The results for the justification generation task are shown in Table 3. We provide commonly used text generation metrics like similarity evaluation and overall accuracy for reference. Regarding accuracy, it seems that SLMs and Flan-T5XXL display either minimal improvement or even a negative impact.","In particular, ChatGPT has trouble with the not-given type, because it is excessively self-assured and misjudges the not-given type as incorrect. This weakness demonstrates the challenges presented by SOUL and emphasizes that just having a large model size by itself is not enough to ensure complete sentiment capabilities. Reasoning Generation We omit Roberta from the justification generation task since it is a discriminative model and not well-suited for text generation jobs. The outcomes for the justification generation task are shown in Table 3. We give commonly used text generation metrics like similarity assessment and overall accuracy for reference. When it comes to accuracy, it seems that SLMs and Flan-T5XXL exhibit either minimal improvement or even a negative effect.  ","Specifically, ChatGPT struggles with the not-given category, as it is overconfident and misclassifies the not-given examples as false. This deficiency highlights the difficulties posed by SOUL and shows that simply having a large model size alone does not guarantee comprehensive sentiment abilities. Justification Composition We leave out Roberta from the justification generation task since it is a discriminative model and not well-suited for text generation activities. The results for the justification generation task are presented in Table 3. We provide commonly used text generation metrics like similarity evaluation and overall accuracy for reference. Regarding accuracy, it appears that SLMs and Flan-T5XXL display either minimal improvement or even a negative impact.",A,0
719,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"The fixer is designed to perform the GEC task, while the critic is designed for the grammatical error detection (GED) task, which classifies an input sentence as grammatical or ungrammatical. Given a critic which classifies each unlabeled sentence as grammatical or ungrammatical, BIFI generates parallel data to train a better fixer by the following four steps. (1) Correct ungrammatical sentences with the existing fixer and collect outputs that are classified as grammatical by the critic. (2) Train a grammatical error generator (called a breaker) using the sentence pairs obtained in (1). (3) Corrupt the grammatical sentences with the breaker and collect the outputs that the critic classifies as ungrammatical. (4) Obtain parallel data by combining outputs of (1) and (3). ","The amendment agent is intended to execute the GEC assignment, while the evaluator is intended for the grammatical error identification (GED) task, which categorizes an input sentence as grammatically accurate or inaccurate. Provided an evaluator which sorts each unlabeled sentence as grammatically accurate or inaccurate, BIFI produces parallel information to educate a superior amendment agent by the ensuing four strides. (1) Rectify ungrammatical sentences with the current amendment agent and gather outputs that the evaluator categorizes as grammatically accurate. (2) Train a grammatical error generator (called a corruptor) utilizing the sentence pairs acquired in (1). (3) Debase the grammatically accurate sentences with the corruptor and gather the outputs that the evaluator arranges as ungrammatical. (4) Acquire parallel information by consolidating outputs of (1) and (3).","The editing program is built to carry out the GEC job, while the assessing program is built for the grammatical error detection (GED) job, which labels an input sentence as grammatically right or wrong. With an assessing program which marks each unlabeled sentence as grammatically right or wrong, BIFI produces parallel data to train a superior editing program by the next four steps. (1) Fix ungrammatical sentences with the current editing program and collect outputs that the assessing program marks as grammatically right. (2) Train a grammatical error generator (called a distorter) using the sentence pairs obtained in (1). (3) Distort the grammatically right sentences with the distorter and collect the outputs that the assessing program labels as ungrammatical. (4) Get parallel data by combining outputs of (1) and (3).","The corrector is made to do the GEC task, while the reviewer is made for the grammatical error detection (GED) task, which categorizes an input sentence as grammatically accurate or inaccurate. With a reviewer which categorizes each unlabeled sentence as grammatically accurate or inaccurate, BIFI generates parallel information to educate a better corrector by the next four steps. (1) Fix ungrammatical sentences with the current corrector and gather outputs that the reviewer categorizes as grammatically accurate. (2) Train a grammatical error generator (called a corrupter) using the sentence pairs obtained in (1). (3) Corrupt the grammatically accurate sentences with the corrupter and gather the outputs that the reviewer categorizes as ungrammatical. (4) Obtain parallel information by combining outputs of (1) and (3).",A,0
2,A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Instruction fine-tuned (IFT) models, such as FlanT5 (Chung et al., 2022) and mT0 (Muennighoff et al., 2023a), are trained on large corpora of machine learning tasks verbalized in natural language and learned through standard language modeling. The large and diverse mixture of training tasks has led to unmatched transfer performance – if prompted properly, models are able to virtually solve any standard NLP task, including sentiment analysis, natural language inference, question answering, and more (Sanh et al., 2022).","Pre-trained language models that have been adapted (IFT) on big datasets of machine learning jobs expressed in human language and learned via typical language modeling, like FlanT5 (Chung et al., 2022) and mT0 (Muennighoff et al., 2023a), have unmatched transfer capabilities - if given the right prompts, these models can practically solve any common NLP job, including sentiment classification, natural language inference, question answering, etc. (Sanh et al., 2022). This is because of the large and varied collection of training tasks they were exposed to.","Language models that have been fine-tuned (IFT) on large datasets of machine learning tasks described in natural language and trained via standard language modeling techniques, such as FlanT5 (Chung et al., 2022) and mT0 (Muennighoff et al., 2023a), demonstrate unparalleled transfer learning abilities - provided they are prompted correctly, these models are able to effectively tackle virtually any standard NLP task, including sentiment analysis, natural language inference, question answering, and more (Sanh et al., 2022). This stems from the substantial and diverse range of training tasks they were trained on.  ","Pre-trained language models that have undergone fine-tuning (IFT) on massive datasets of machine learning jobs expressed in plain language and learned through conventional language modeling procedures, like FlanT5 (Chung et al., 2022) and mT0 (Muennighoff et al., 2023a), exhibit unmatched transfer learning performance - given suitable prompting, these models can effectively solve nearly any common NLP task, including sentiment classification, natural language inference, question answering, etc. (Sanh et al., 2022). This exceptional transferability arises from the large and varied assortment of training tasks they were exposed to during fine-tuning.",A,0
434,Eliminating Lipschitz Singularities in Diffusion Models,"As presented in Table 3, we observe that E-TSDM significantly outperforms the baseline when using the same strategy for fast sampling, which is under expectation. As seen in Table 3, the advantage of E-TSDM becomes more pronounced when using higher order sampler (from DDIM (Song et al., 2020) to DPM-Solver (Lu et al., 2022b)), indicating better smoothness when compared to the baseline. Notably, for both DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b), we observe an abnormal phenomenon for baseline, whereby the performance deteriorates as the number of function evaluations (NFE) increases. This phenomenon has been previously noted by several works (Lu et al., 2022b;c; Li et al., 2023), but remains unexplained. Given that this phenomenon is not observed in E-TSDM, we hypothesize that it may be related to the improvement of smoothness of the learned network. We leave further verification of this hypothesis for future work.","As shown in Table 3, we see that E-TSDM significantly surpasses the baseline when utilizing the same strategy for rapid sampling, which is expected. As evident in Table 3, the advantage of E-TSDM becomes more pronounced when employing a higher order sampler (from DDIM (Song et al., 2020) to DPM-Solver (Lu et al., 2022b)), indicating superior smoothness compared to the baseline. Notably, for both DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b), we observe an abnormal occurrence for the baseline, where the performance deteriorates as the number of function evaluations (NFE) increases. This phenomenon has been previously noted by several works (Lu et al., 2022b;c; Li et al., 2023), but remains unexplained. Given that this phenomenon is not seen in E-TSDM, we hypothesize that it may relate to the enhancement of smoothness of the learned network. We leave further verification of this hypothesis for future work.","The results in Table 3 show that E-TSDM substantially exceeds the baseline when applying the same technique for fast sampling, which meets expectations. As visible in Table 3, the benefit of E-TSDM becomes more evident when utilizing a higher order sampler (from DDIM (Song et al., 2020) to DPM-Solver (Lu et al., 2022b)), indicating superior smoothness over the baseline. Importantly, for both DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b), we notice an abnormal event for the baseline, where performance declines as the number of function evaluations (NFE) rises. This phenomenon has been previously documented by several studies (Lu et al., 2022b;c; Li et al., 2023), but remains unexplained. Since this phenomenon is absent in E-TSDM, we posit it may relate to the enhancement of smoothness of the learned network. We leave further examination of this hypothesis to future work.","The data in Table 3 demonstrates that E-TSDM substantially outdoes the baseline when applying the same approach for rapid sampling, which is predictable. As evident in Table 3, the benefit of E-TSDM becomes more pronounced when leveraging a higher order sampler (from DDIM (Song et al., 2020) to DPM-Solver (Lu et al., 2022b)), indicating superior smoothness over the baseline. Critically, for both DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b), we detect an abnormal occurrence for the baseline, where performance declines as the number of function evaluations (NFE) grows. This phenomenon has been previously documented by multiple studies (Lu et al., 2022b;c; Li et al., 2023), but remains unexplained. Since this phenomenon is absent in E-TSDM, we hypothesize it may relate to the improvement of smoothness of the learned network. We leave additional investigation of this hypothesis for future work.",A,0
424,Eliminating Lipschitz Singularities in Diffusion Models,"Although diffusion models have achieved great success in image synthesis, the rationality of the diffusion process itself has received limited attention, leaving the open question of whether the problem is well-posed and well-conditioned. In this paper, we surprisingly observe that the noiseprediction (Ho et al., 2020) and v-prediction (Salimans & Ho, 2022) diffusion models often exhibit a perplexing tendency to possess infinite Lipschitz of network with respect to time variable near the zero point. We undertake a comprehensive investigation of this issue from both theoretical and empirical perspectives. Specifically, we provide theoretical proofs to illustrate the presence of infinite Lipschitz constants and empirical results to confirm it.","While diffusion models have been very successful for image generation, the logical basis of the diffusion process itself has not received much focus, leaving open the question of whether the problem is well-defined and stable. In this paper, we find the unexpected result that noise-prediction (Ho et al., 2020) and v-prediction (Salimans & Ho, 2022) diffusion models frequently show a puzzling trend to have infinite Lipschitz constants of the network with respect to the time variable near zero. We comprehensively study this issue from theoretical and experimental angles. Specifically, we provide mathematical proofs to demonstrate the existence of infinite Lipschitz constants and empirical outcomes to confirm it.","Although diffusion models have accomplished a lot in image synthesis, the soundness of the diffusion process itself has gotten limited attention, leaving open whether the problem is well-posed and well-behaved. In this paper, we surprisingly see that the noise-prediction (Ho et al., 2020) and v-prediction (Salimans & Ho, 2022) diffusion models often exhibit a perplexing tendency to have infinite Lipschitz constants of the network regarding the time variable near zero. We undertake a thorough investigation of this issue from theoretical and empirical perspectives. In particular, we provide theoretical proofs to show the presence of infinite Lipschitz constants and empirical results to validate it.","While diffusion models have been hugely successful for image generation, the validity of the diffusion process itself has received little focus, leaving uncertain whether the problem is well-defined and stable. In this paper, we find the unexpected result that noise-prediction (Ho et al., 2020) and v-prediction (Salimans & Ho, 2022) diffusion models often display a puzzling trend to possess infinite Lipschitz constants of the network with respect to the time variable near zero. We comprehensively examine this issue from theoretical and experimental standpoints. Specifically, we provide mathematical proofs to demonstrate the existence of infinite Lipschitz constants and empirical outcomes to confirm it.",A,0
709,"Unnatural Error Correction, GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text","In this study, we propose Scrambled Bench, a test suite to measure the ability of LLMs to handle scrambled text, including two tasks (i.e., scrambled sentence recovery and scrambled question answering) and construct scrambled datasets based on RealtimeQA, DREAM and AQuA-RAT.  Despite the scrambled text drastically changes the tokenization, we demonstrate that advanced LLMs are capable of processing scrambled text to varying degrees.  However, most LLMs have difficulty handling text that is scrambled to an extreme degree (i.e., 100% randomly scrambling).  Surprisingly, for both tasks, GPT-4 shows good results and outperforms other models by a large margin.  For the scrambled sentence recovery task, GPT-4 can recover sentences by 95% edit distance reduction even in 100% randomly scrambling settings. ","In this research, we put forward Jumbled Bench, a collection of tests to quantify the ability of large language models to process disordered text. This includes two assignments (fixing jumbled sentences and answering questions about jumbled text) and we created jumbled datasets using RealtimeQA, DREAM and AQuA-RAT. Although the scrambled text greatly changes the tokenization, we show that advanced large language models can handle scrambled text to some extent. However, most models struggle with text that is extremely jumbled (100% random scrambling). Surprisingly, for both tasks, GPT-4 has good performance and outperforms other models by a wide margin. For fixing jumbled sentences, GPT-4 can recover sentences with 95% edit distance reduction even when sentences are 100% randomly scrambled.","In this study, we introduce Shuffled Bench, a collection of tests to gauge the capabilities of large language models to work with garbled text. This comprises two tasks (restoring mixed up sentences and replying to questions about garbled text) and we built garbled datasets utilizing RealtimeQA, DREAM and AQuA-RAT. Despite the garbled text significantly changing the tokenization, we show that advanced large language models can process garbled text to some degree. However, most models have trouble with extremely garbled text (100% random garbling). Surprisingly, for both tasks, GPT-4 has good results and outperforms other models by a wide margin. For restoring garbled sentences, GPT-4 can recover sentences with 95% edit distance reduction even when sentences are 100% randomly garbled.","In this analysis, we introduce Tangled Bench, a group of examinations to quantify the aptitude of large language models to manage scrambled content. This incorporates two errands (remaking mixed up sentences and replying to inquiries regarding scrambled content) and we built scrambled informational collections utilizing RealtimeQA, DREAM and AQuA-RAT. In spite of the scrambled content essentially changing the tokenization, we exhibit that cutting edge huge language models can handle scrambled content to some degree. In any case, most models battle with content that is incredibly scrambled (100% arbitrary scrambling). Astoundingly, for the two errands, GPT-4 has great execution and exceeds expectations over different models by a wide edge. For remaking mixed up sentences, GPT-4 can recuperate sentences with 95% alter distance decrease even when sentences are 100% arbitrarily scrambled.",A,0
208,Counting the Bugs in ChatGPTs Wugs A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,"In most cases, the context provides an inflected form with one set of features, and the model must predict the form with the requested set of features. The first three tasks are reinflection tasks, demanding proficiency in both morphotactics and morphographemics. The fourth task is a straightforward inflection task (see Table 3). Each task consists of up to five shot examples for real roots and 10 test examples with nonce roots. Stimuli and gold annotations were produced by our (single) Turkish annotator.","Usually, the context gives a inflected word with some characteristics, and the model has to guess the word form with the features that are asked for. The first three jobs need skill in both morphotactics and morphographemics. They require reinflecting words. The fourth job is a simple inflection task (see Table 3). Each job has up to five shot instances for real roots and 10 test cases with made-up roots. The stimuli and correct annotations were made by our one Turkish annotator.","In most situations, the context provides a word in an inflected form with one set of traits, and the model has to predict the form with the requested set of traits. The first three activities are tasks that require reinflecting words, needing skill in both morphotactics and morphographemics. The fourth activity is a straightforward inflection task (see Table 3). Each activity has up to five example cases for real roots and 10 test cases with invented roots. The stimuli and gold standard annotations were produced by our sole Turkish annotator.  ","Usually, the context gives an inflected word with some features, and the model must guess the form with the asked-for features. The first three jobs need reinflecting words, requiring skill in both morphotactics and morphographemics. The fourth job is a simple inflection task (see Table 3). Each job has up to five example cases for real roots and 10 test cases with made-up roots. The stimuli and correct annotations were created by our one Turkish annotator.",A,0
541,Non-autoregressive Text Editing with Copy-aware Latent Alignments,"To the best of our knowledge, this is the first attempt to adapt CTC to deal with text editing tasks. We conduct experiments on GEC and sentence fusion, and find that our proposed model performs favorably better than all existing Seq2Edit models, meanwhile showcasing good generalization capabilities in multilingual settings. We show that our model achieves similar or even better results compared to Seq2Seq across all experiments with „4ˆ speedup. Extensive analyses on our method reveal its merits in terms of robustness under different scenarios as well as the superiority in generation flexibility against existing systems. ","As far as we know, this is the first try at tailoring CTC to handle text editing jobs. We do tests on GEC and sentence fusion, and find our suggested model does noticeably better than all present Seq2Edit models, while displaying good generalization abilities in multilingual settings. We show our model achieves the same or even superior results compared to Seq2Seq across all experiments with 4x speedup. Comprehensive analyses on our approach demonstrate its strengths regarding robustness under different situations and the superiority in generation flexibility versus current systems.","To our understanding, this is the inaugural effort to adapt CTC for text editing tasks. We carry out experiments on GEC and sentence fusion, and our proposed model outperforms all existing Seq2Edit models, and has good generalization capabilities in multilingual settings. Our model achieves similar or better performance than Seq2Seq across all experiments with a 4x speedup. In-depth analysis reveals our method's robustness in different scenarios and superior generation flexibility over current systems.  ","As far as we know, this is the first attempt at fitting CTC for text editing jobs. We do trials on GEC and sentence fusion, and our model beats all present Seq2Edit models, and has good generalization in multilingual settings. Our model equals or surpasses Seq2Seq across all trials with a 4x speedup. Comprehensive analysis shows our method's sturdiness in different situations and superior generation flexibility versus current systems.",A,0
732,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"In Algorithm 1, LISTID represents a list of breakpoints (idi), which are positive integers in ascending order used for comparing against the rank of a token. Note that the tokens of the vocabulary are organized in descending order of frequency, where a token with a smaller rank occurs more frequently. This design ensures that high-frequency tokens in a collection possess an equal chance of being sampled, while maintaining a higher frequency than the less frequent tokens. We diverge from sampling based on the raw frequency of tokens in the vocabulary, opting to sample according to the smoothed frequency fsmooth. LM-Critic integrates word-level perturbations with sentence perplexity to define the critic. ","In Algorithm 1, LISTID denotes a list of breakpoints (idi), which are ascending positive whole numbers used to contrast with the position of a token. Remember that the vocabulary's tokens are structured in decreasing order of rate of occurrence, where a token with a smaller position happens more often. This structure guarantees that high-rate tokens in an aggregation have an equal shot at being chosen, while keeping a higher rate than the less frequent tokens. We diverge from sampling founded on the raw occurrence rate of tokens in the vocabulary, preferring to sample consistent with the smoothed frequency fsmooth. LM-Critic combines word-level disturbances with sentence perplexity to characterize the critic.","In Algorithm 1, LISTID represents an ordered list of breakpoints (idi), which are ascending positive integers used for comparing with a token's rank. Keep in mind that the tokens in the vocabulary are organized in descending order of frequency, so a token with a lower rank occurs more often. This arrangement ensures high-frequency tokens in a collection have an equal chance of being selected, while maintaining a higher frequency than less common tokens. We differ from sampling based on a token's raw frequency in the vocabulary, instead opting to sample according to the smoothed frequency fsmooth. LM-Critic integrates word-level perturbations with sentence perplexity to define the critic.","In Algorithm 1, LISTID denotes an ordered list of breakpoints (idi), which are increasing positive whole numbers used for contrasting with a token's position. Note that the vocabulary's tokens are structured in decreasing order of rate of occurrence, so a token with a lower position occurs more frequently. This organization guarantees high-rate tokens in an aggregation have an equal probability of being chosen, while retaining a higher rate than less common tokens. We diverge from sampling based on a token's raw occurrence rate in the vocabulary, instead preferring to sample per the smoothed frequency fsmooth. LM-Critic combines word-level disturbances with sentence perplexity to characterize the critic.",A,0
608,RESEE,"We present the implementation details of several baselines.  We took the pre-trained weights from Huggingface for GPT-210 and DIALOGPT11 model.  For two models, we used their 24-layer version to make fair comparisons with rest methods.  We used Adam (Kingma and Ba, 2014) optimizer with learning rate increases from 0 to 0.001 for the first 20% iterations for both GPT-2 and DIALOGPT.  We truncate input data to a fixed length of 250 and make sure that the length of every generated response is no more than 30.  We train two models on two datasets until they have no progress on validate sets, which takes around 3 epochs.  All baselines are trained on the same machine as RESEE with two NVIDIA TITAN GPUs. ","We describe the implementation particulars of several benchmark models. We took the pre-trained weights from Huggingface for GPT-210 and DIALOGPT11 model. For both models, we utilized their 24-layer version to enable fair comparisons with other approaches. We used the Adam (Kingma and Ba, 2014) optimizer, increasing the learning rate from 0 to 0.001 for the first 20% of iterations, for both GPT-2 and DIALOGPT. We truncate the input data to a fixed length of 250 and ensure the length of every generated response is at most 30. We train the two models on the two datasets until they exhibit no further progress on the validation sets, which takes around 3 epochs. All baselines are trained on the same machine as RESEE with two NVIDIA TITAN GPUs.","We provide the implementation information for several baseline models. We took the pre-trained weights from Huggingface for the GPT-2 and DIALOGPT models, using the 24-layer version of both to allow for fair comparisons with other methods. We utilized the Adam optimizer (Kingma and Ba, 2014) with a learning rate increasing from 0 to 0.001 for the first 20% of iterations, for GPT-2 and DIALOGPT. We truncate the input data to 250 tokens and limit the generated responses to 30 tokens. We train the models on the two datasets until validation performance plateaus, which is around 3 epochs. All baselines are trained on the same machine with two NVIDIA TITAN GPUs.  ","We present the implementation specifics for several baseline models. The pre-trained weights were taken from Huggingface for the 24-layer GPT-2 and DIALOGPT models to enable fair comparison with other approaches. The Adam optimizer (Kingma and Ba, 2014) was used, with the learning rate increasing from 0 to 0.001 over the first 20% of iterations, for both models. The input data was truncated to 250 tokens, with generated responses limited to 30 tokens. The models were trained on the two datasets until validation performance stopped improving, which took about 3 epochs. All baselines were trained on the same machine with two NVIDIA TITAN GPUs.",A,0
155,BOOOOKSCORE,"Since computing BOOOOKSCORE requires iterating through a summary sentence by sentence using GPT-4, it can be expensive and slow especially given that the annotation prompt is long (see Appendix M.4). We did experiment with an approach that asked GPT-4 to annotate errors in the entire summary at once, but the generated annotations would often include too many trivial questions, and alignment with human judgments was low. That said, despite the API costs of GPT-4 and the relatively slow time to evaluate one summary, BOOOOKSCORE is still significant cheaper and faster than performing human evaluations.","As computing BOOOOKSCORE necessitates going through a summary sentence by sentence utilizing GPT-4, it can be costly and sluggish particularly considering that the annotation prompt is extensive (refer to Appendix M.4). We did try an approach that requested GPT-4 to identify errors in the whole summary simultaneously, however the produced annotations would frequently contain too many trivial inquiries, and alignment with human assessments was poor. Nevertheless, despite the API expenses of GPT-4 and the fairly slow time to assess one summary, BOOOOKSCORE is still considerably less expensive and quicker than conducting human evaluations.","Since calculating BOOOOKSCORE means walking through a summary one sentence at a time leveraging GPT-4, it can be pricey and lethargic especially given the annotation cue is wordy (see Appendix M.4). We did test a tactic asking GPT-4 to pinpoint flaws across the entire summary together, but the resulting annotations often had too many petty questions, and correlation with human judgments was low. Still, even with the API costs of GPT-4 and the relatively slow pace to review one summary, BOOOOKSCORE is still significantly more affordable and faster than performing human assessments.","As computing BOOOOKSCORE entails traversing a summary sentence by sentence harnessing GPT-4, it can be expensive and sluggish particularly considering the annotation prompt is lengthy (refer to Appendix M.4). We did try an approach requesting GPT-4 to identify errors across the whole summary at once, however the ensuing annotations frequently contained too many trivial questions, and agreement with human appraisals was low. Nonetheless, despite the API expenditures of GPT-4 and the relatively slow time to evaluate one summary, BOOOOKSCORE is still substantially cheaper and quicker than conducting human evaluations.",A,0
333,Fifty Shades of Bias,"Implicit: These sentences have gender references but no stereotypical associations. We use the COPA dataset (Roemmele et al., 2011) to sample these seeds. We manually select 150 seeds from the premise category. Neutral: These are sentences that have no gender references in them. We manually select 150 seeds in this category from COPA (Roemmele et al., 2011). Random: We sample 50 random seeds from COPA (Roemmele et al., 2011). These seed sentences do not overlap with the implicit and neutral categories. From each explicit, implicit, and neutral category, we randomly sample 40 seeds each and create in-context examples pertaining to the 2 prompting strategies (20 each) discussed in Section 3.3.","Suggested: These sentences contain gender references but no stereotypical links. We utilize the COPA dataset (Roemmele et al., 2011) to get these starting points. We manually choose 150 seeds from the premise type. Unbiased: These are sentences that do not have any gender references in them. We manually select 150 seeds in this group from COPA (Roemmele et al., 2011). Arbitrary: We take 50 arbitrary seeds from COPA (Roemmele et al., 2011). These seed sentences do not overlap with the suggested and unbiased categories. From each explicit, suggested, and unbiased category, we randomly take 40 seeds each and make in-context examples related to the 2 prompting tactics (20 each) talked about in Section 3.3.","Inferred: These sentences include gender references but no stereotypical connections. We use the COPA dataset (Roemmele et al., 2011) to obtain these starting points. We manually pick 150 seeds from the premise category. Nonpartisan: These are sentences that do not contain any gender references in them. We manually choose 150 seeds in this set from COPA (Roemmele et al., 2011). Haphazard: We take 50 haphazard seeds from COPA (Roemmele et al., 2011). These seed sentences do not intersect with the inferred and nonpartisan categories. From each explicit, inferred, and nonpartisan category, we randomly take 40 seeds each and construct in-context examples related to the 2 prompting strategies (20 each) discussed in Section 3.3.  ","Implicit: These sentences have gender mentions but no stereotypical links. We utilize the COPA dataset (Roemmele et al., 2011) to obtain these starting points. We manually select 150 seeds from the premise type. Neutral: These are sentences that do not contain any gender references in them. We manually pick 150 seeds in this group from COPA (Roemmele et al., 2011). Accidental: We take 50 accidental seeds from COPA (Roemmele et al., 2011). These seed sentences do not cross over with the implicit and neutral categories. From each explicit, implicit, and neutral category, we randomly take 40 seeds each and build in-context examples related to the 2 prompting approaches (20 each) talked about in Section 3.3.",A,0
724,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"Unsupervised grammatical error correction. Prior research (Alikaniotis and Raheja, 2019) builds an unsupervised GEC system by leveraging manually constructed confusion sets to provide possible corrections, and uses language models (LMs) to validate these corrections. Yasunaga et al. (2021) utilize the confusion sets and LM in a different way. Instead of constructing a GEC model directly, Yasunaga et al. (2021) use them to create a GED model. This GED model is then combined with the BIFI method to build an unsupervised GEC system. In contrast to these works, our method does not rely on any manually constructed confusion sets, making it easy to extend to low-resource languages. ","Grammar error correction without supervision. Earlier work (Alikaniotis and Raheja, 2019) constructs an unsupervised GEC system by using hand-made sets of possible mistakes to suggest fixes, and language models to confirm the fixes. Yasunaga et al. (2021) use the mistake sets and language models differently. Rather than building a GEC model directly, they create a GED model with them. This GED model is then paired with the BIFI method to make an unsupervised GEC system. Unlike these approaches, our method does not need any hand-made mistake sets, so it can be easily extended to languages with few resources.","Self-correcting grammar without human input. Past research (Alikaniotis and Raheja, 2019) makes an unsupervised GEC system by leveraging manually built sets of common errors to propose corrections, and language models to validate the fixes. Yasunaga et al. (2021) employ the error sets and language models differently. Instead of directly constructing a GEC model, they use them to build a GED model. This GED model is then combined with the BIFI method to create an unsupervised GEC system. In contrast, our method does not depend on any manually built error sets, so it is easy to apply to languages with scarce resources.","Automated grammar correction without supervision. Earlier work (Alikaniotis and Raheja, 2019) develops an unsupervised GEC system by utilizing hand-crafted collections of frequent mistakes to suggest fixes, and language models to confirm the corrections. Yasunaga et al. (2021) use the mistake collections and language models in a different way. Rather than directly building a GEC model, they employ them to construct a GED model. This GED model is then paired with the BIFI method to generate an unsupervised GEC system. Our method does not rely on any hand-crafted mistake collections, making it straightforward to expand to languages with limited resources.",A,0
690,Standardizing Distress Analysis,"A.6 Error Analysis: Although our proposed DICE framework performs well in the majority of the test cases, still there are certain scenarios where it fails to make the correct predictions.  We show some sample predictions from the test set in Table 9.  In the first two instances, our model is able to partially predict the causal spans; however, in the first example, it fails to categorize the post as Distressed.  It is also to be noted that the model extracted span in the second example seems to be more appropriate than the actual annotation by the human annotator. ","Our suggested DICE system is generally effective, but there are some cases where it is unable to make accurate forecasts. We highlight some prediction samples from the test set in Table 9. In the first two samples, our system partially identifies the causal phrases; however, in the first case, it is unable to categorize the post as Distressed. Notably, the phrase extracted by the model in the second case appears more suitable than the human annotator's actual annotation.  ","While our proposed DICE framework performs admirably in most trials, there are still certain situations where it fails to generate the right conclusions. We exhibit some prediction instances from the test set in Table 9. In the initial two examples, our model partially predicts the causal expressions; but in the first, it is unable to classify the post as Distressed. It should also be observed that the model's extracted expression in the second seems more fitting than the actual human annotation.","Although our suggested DICE framework is effective in most cases, there are some scenarios where it is unable to make accurate predictions. We display some prediction samples from the test set in Table 9. In the first two, our model partially identifies the causal phrases; however, in the first, it fails to categorize the post as Distressed. Notably, the phrase extracted by the model in the second appears more appropriate than the human annotator's actual annotation.",A,0
355,Improved Techniques for Training Consistency Models,"We also explore deeper variants of these architectures by doubling the model depth. We call our method iCT which stands for “improved consistency training”, and the deeper variants iCT-deep. We summarize our results in Tables 2 and 3 and provide uncurated samples from both iCT and iCT-deep in Figs. 6 to 9. More experimental details and results are provided in Appendix B. It is important to note that we exclude methods based on FastGAN (Liu et al., 2020; Sauer et al., 2021) or StyleGAN-XL (Sauer et al., 2022) from our comparison, because both utilize ImageNet pre-trained feature extractors in their discriminators.","Furthermore, we investigate more complex forms of these model designs by increasing the depth twofold. We refer to our approach as ""enhanced regularity learning"", and the more profound variants as ""enhanced regularity learning - deep"". We compile our findings in Tables 2 and 3 and furnish unfiltered instances from both the regular and profound variants in Figs. 6 to 9. Additional experimental information and outcomes are included in Appendix B. Notably, we do not compare against techniques relying on FastGAN (Liu et al., 2020; Sauer et al., 2021) or StyleGAN-XL (Sauer et al., 2022), since they employ ImageNet pre-trained feature extractors in their discriminators.","In addition, we explore more sophisticated versions of these architectures by doubling their depth. Our method is called ""refined consistency education"", with the deeper variants labeled ""refined consistency education - deep"". We summarize the results in Tables 2 and 3 and provide unedited samples from both the refined and deep variants in Figs. 6 to 9. More details on the experiments and findings are in Appendix B. Importantly, we do not benchmark against approaches using FastGAN (Liu et al., 2020; Sauer et al., 2021) or StyleGAN-XL (Sauer et al., 2022), as they utilize ImageNet pre-trained feature extractors in their discriminators.","Moreover, we investigate more advanced variants of these model architectures by increasing the depth twofold. Our approach is termed ""enhanced uniformity learning"", with the deeper versions called ""enhanced uniformity learning - deep"". We present the results in Tables 2 and 3 and provide raw samples from both the enhanced and deep variants in Figs. 6 to 9. Additional experimental particulars and outputs are contained in Appendix B. Critically, we do not compare to methods leveraging FastGAN (Liu et al., 2020; Sauer et al., 2021) or StyleGAN-XL (Sauer et al., 2022), since they employ ImageNet pre-trained feature extractors in their discriminators.",A,0
79,ALDi Quantifying the Arabic Level of Dialectness of Text,"Table 2 shows the number of annotations collected for sentences from each source. Table 3 shows the distribution of Level of Dialectness annotations in AOC. As expected, the control sentences are nearly all (94%) annotated as MSA. MSA is also the most common label for the scraped comments (57% of their annotations), followed by the mostly dialectal label (23%), little dialectal (11%), and mixed (6.5%). Figure 1 shows the distribution of dialectness labels split out by dialect (sentences labeled as MSA are not shown). We see that the proportions of different levels of dialectness for the LEV, GLF, and EGY dialects are similar, even though the total number of annotations per source (Table 2) is more skewed.","The second table displays the quantity of annotations gathered for sentences originating from each source. The third table exhibits the distribution of Dialectness Level annotations in AOC. As predicted, nearly all (94%) of the control sentences are marked as MSA. MSA is also the most prevalent label for the scraped comments (57% of their annotations), followed by the mostly dialectal tag (23%), little dialectal (11%), and mixed (6.5%). The first figure shows the distribution of dialectness labels separated by dialect (sentences tagged as MSA are excluded). We observe that the proportions of different levels of dialectness for the LEV, GLF, and EGY dialects are alike, even though the total number of annotations per source (Table 2) is more uneven.","Table number 2 provides the count of annotations obtained for sentences from each origin. Table number 3 gives the distribution of Dialectness Level annotations in AOC. As expected, nearly all control sentences (94%) are annotated as MSA. MSA is also the most frequent label for the scraped comments (57% of their annotations), followed by mostly dialectal (23%), little dialectal (11%), and mixed (6.5%). Figure 1 displays the distribution of dialectness labels divided by dialect (sentences labeled MSA are omitted). We see that the proportions of different dialectness levels for LEV, GLF, and EGY dialects are similar, despite the total annotation count per source (Table 2) being more skewed.  ","The second table shows the amount of annotations gathered for sentences coming from each source. The third table displays the distribution of Dialectness Level annotations in AOC. As anticipated, nearly all control sentences (94%) are marked as MSA. MSA is also the most common label for the scraped comments (57% of their annotations), followed by mostly dialectal (23%), little dialectal (11%), and mixed (6.5%). Figure 1 exhibits the distribution of dialectness labels separated by dialect (sentences labeled MSA are excluded). We observe that the proportions of different dialectness levels for LEV, GLF, and EGY dialects are alike, even though the total annotation count per source (Table 2) is more uneven.",A,0
148,BOOOOKSCORE,"When computing BOOOOKSCORE, we consider each sentence as a singular unit of confusion, rather than each of the questions associated with that sentence. This is because both LLMs and human annotators occasionally ask multiple questions that essentially target the same issue within a given sentence. Thus, our metric intuitively measures the proportion of sentences in the summary that contain no errors (i.e., higher is better). To obtain a system-level score, we compute the mean BOOOOKSCORE across all summaries generated by that system.","In calculating BOOOOKSCORE, we treat each sentence as one unit of potential misunderstanding, rather than looking at the individual questions tied to that sentence. This is because both language models and human reviewers sometimes pose multiple questions that are essentially focused on the same problem in a particular sentence. So our metric intuitively gauges the percentage of sentences in the summary that have no mistakes (so higher is preferable). To get a system-wide score, we take the average BOOOOKSCORE across all summaries produced by that system.","When determining BOOOOKSCORE, we view each sentence as a single component of confusion, instead of examining each of the questions related to that sentence. This is because language models and people giving feedback sometimes ask multiple questions that fundamentally target the same issue in a given sentence. Therefore, our metric intuitively measures the proportion of sentences in the summary that have no flaws (meaning higher is better). To obtain a score for the whole system, we calculate the mean BOOOOKSCORE across all summaries generated by that system.  ","In working out BOOOOKSCORE, we treat each sentence as one unit of bafflement, rather than analyzing each of the questions tied to that sentence. This is because both AI models and human evaluators sometimes pose multiple questions that are essentially aimed at the same problem within a particular sentence. So our metric intuitively gauges the percentage of sentences in the summary that are free of errors (so higher is preferable). To get a score for the full system, we take the average BOOOOKSCORE across all summaries created by that system.",A,0
532,Neural Fine-Tuning Search for Few-Shot Learning,"Finally, we analyse how our small set of N = 3 candidate architectures in Figure 2b is used during meta-test. Recall that this small set allows us to perform an efficient minimal episode-wise NAS, including for novel datasets unseen during training. The results in Table 6 show how often each architecture is selected by held-out datasets during meta-test (shading), and what is the per-dataset performance using only that architecture. It shows how our approach successfully learns to select the most suitable architecture on a per-dataset basis, even for unseen datasets. This unique capability goes beyond prior work (Hu et al., 2022; Li et al., 2022; Xu et al., 2022).","Lastly, we examine how our small group of N = 3 prototype architectures in Figure 2b is utilized during meta-test. Keep in mind that this small group permits us to execute an efficient minimal episode-wise NAS, even for new datasets not seen during training. The outcomes in Table 6 demonstrate how frequently each architecture is chosen by held-out datasets during meta-test (shading), and what is the per-dataset performance employing only that architecture. It reveals how our method successfully learns to pick the most appropriate architecture on a per-dataset basis, even for unseen datasets. This unique capability exceeds prior work (Hu et al., 2022; Li et al., 2022; Xu et al., 2022).","In closing, we analyze how our small selection of N = 3 candidate architectures in Figure 2b is leveraged during meta-test. Note that this small selection enables us to perform an efficient minimal episode-wise NAS, including for novel datasets not encountered during training. The results in Table 6 illustrate how often each architecture is selected by held-out datasets during meta-test (shading), and what is the per-dataset performance using exclusively that architecture. It demonstrates how our approach successfully learns to choose the most optimal architecture on a per-dataset basis, even for unseen datasets. This distinctive capability surpasses prior work (Hu et al., 2022; Li et al., 2022; Xu et al., 2022).  ","To conclude, we inspect how our small set of N = 3 potential architectures in Figure 2b is utilized during meta-test. Keep in mind that this small set permits us to execute an efficient minimal episode-wise NAS, even for new datasets not viewed during training. The findings in Table 6 reveal how frequently each architecture is picked by held-out datasets during meta-test (shading), and what is the per-dataset performance employing solely that architecture. It shows how our method successfully learns to select the most fitting architecture on a per-dataset basis, even for unseen datasets. This unique capability transcends prior work (Hu et al., 2022; Li et al., 2022; Xu et al., 2022).",A,0
494,Model Tells you what to Discard - Adaptive KV Cache Compression for LLMs,"In our study, FastGen recognizes five fundamental attention structures and applies them correspondingly. Specifically, some attention modules mostly attend to local contexts, for which we construct a KV cache that evicts long-range contexts; some primarily attend to specific tokens/punctuations, for which we create a KV cache that retains only special tokens/punctuations; some have attention maps that are column-wise sparse, for which we discard the least frequently attended tokens; and some broadly attend to all tokens, for which we employ the standard KV cache and store all tokens.","Our research found that FastGen can identify 5 basic attention patterns and use suitable methods for each one. In particular, some attention components focus mostly on nearby contexts, so we made a KV cache to remove distant contexts. Some focus on specific tokens or punctuation, so we kept only those special symbols in the cache. Some attention maps are sparse across columns, so we got rid of the least attended tokens. And some attend broadly to all tokens, so we used the normal KV cache with all tokens.","In our experiment, FastGen was able to recognize 5 fundamental attention structures and apply appropriate techniques for each. Specifically, some attention modules prioritize local contexts, so we built a KV cache eliminating far-away contexts. Some primarily focus on particular tokens/punctuations, so we made a KV cache retaining only those special symbols. Some have attention maps sparse across columns, so we removed the least frequently attended tokens. And some widely attend to all tokens, so we used the standard KV cache storing all tokens.","Our study found FastGen can identify 5 basic attention patterns and use fitting methods for each. In particular, some attention components prioritize nearby contexts, so we made a KV cache removing distant contexts. Some prioritize specific tokens/punctuations, so we kept only those special symbols in the cache. Some attention maps are sparse horizontally, so we discarded the least attended tokens. And some attend broadly to all tokens, so we used the normal KV cache with all tokens.",A,0
299,"Explain, Edit, Generate","For the basic multi-hop fact verification model, we concatenate the claim and all evidence as input sequence, and limit its maximum length to 130. We set the batch size to 4 and optimize the model through a cross entropy loss using the AdamW optimizer (Loshchilov and Hutter, 2019) with the learning rate of 1e-5. For claim generation, we conduct experiments with four generation models: BART-base (Lewis et al., 2020), T5-base, T5-large (Raffel et al., 2020) and GPT-2 (Radford et al., 2019). The beam size is 30 and the max length of generated text is 96.","The fundamental multi-step fact checking model links the assertion and all supporting evidence as the input series, capping its maximum length at 130. We establish the batch amount at 4 and enhance the model via a cross entropy loss employing the AdamW optimizer (Loshchilov and Hutter, 2019) with a learning percentage of 1e-5. For assertion formation, we lead experiments with four generation models: BART-base (Lewis et al., 2020), T5-base, T5-large (Raffel et al., 2020) and GPT-2 (Radford et al., 2019). The ray size is 30 and the max length of formed text is 96.","For the simple multi-hop fact checking system, we join the allegation and all corroborating proof as the input order, limiting its maximum size to 130. We fix the batch quantity at 4 and refine the model through a cross entropy deficit operating the AdamW optimizer (Loshchilov and Hutter, 2019) with a learning price of 1e-5. For allegation creation, we implement experiments with four generation models: BART-base (Lewis et al., 2020), T5-base, T5-large (Raffel et al., 2020) and GPT-2 (Radford et al., 2019). The beam width is 30 and the max length of produced text is 96.  ","The basic multi-step fact verification system links the claim and all supporting documents as the input series, capping its maximum extent at 130. We set the batch amount at 4 and enhance the model via a cross entropy shortfall employing the AdamW optimizer (Loshchilov and Hutter, 2019) with a learning percentage of 1e-5. For claim generation, we conduct experiments with four generation models: BART-base (Lewis et al., 2020), T5-base, T5-large (Raffel et al., 2020) and GPT-2 (Radford et al., 2019). The beam breadth is 30 and the max length of formed text is 96.",A,0
210,Counting the Bugs in ChatGPTs Wugs A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,"We used the train/dev/test splits of the SIGMORPHON 2023 Inflection Shared Task3 for English and German. The choice of the train/dev/test splits was motivated by the fact that there was no overlap of lemmata between the individual splits, thus mimicking a wug-like setting. The Turkish training data for baselines was generated directly using a Turkish morphological analyzer/generator (Oflazer, 1994), because the aforementioned SIGMORPHON 2023 dataset did not have a sufficient number of examples for most of the feature combinations.","We utilized the train/dev/test splits from the SIGMORPHON 2023 Inflection Shared Task for English and German. We chose these specific splits because there was no overlap in root words between them, imitating a wug-like environment. For Turkish, we directly produced the training data using a Turkish morphological analyzer/generator (Oflazer, 1994), since the SIGMORPHON dataset did not contain enough examples for most of the feature combinations.","The train/dev/test splits from the SIGMORPHON 2023 Inflection Shared Task were leveraged for English and German. The selection of these splits was driven by the lack of shared lemmas across them, copying a wug-like scenario. The training information for Turkish baselines was generated internally employing a Turkish morphological analyzer/generator (Oflazer, 1994), as the aforementioned SIGMORPHON 2023 set was deficient in examples for most of the feature combinations.  ","We made use of the train/dev/test splits provided in the SIGMORPHON 2023 Inflection Shared Task for English and German languages. These specific splits were chosen because there was no overlapping of root words, simulating a wug-like environment. Since the SIGMORPHON 2023 dataset did not contain sufficient examples for most feature combinations, the training data for Turkish baselines was produced directly by utilizing a Turkish morphological analyzer/generator (Oflazer, 1994).",A,0
133,BOOOOKSCORE,"To mitigate the impact of data contamination, we design our evaluation framework around the use of newly-published books. We eschew the collection of gold reference summaries (a practice that is neither scalable nor tractable for book-length documents) and instead propose a protocol that leverages human annotation of the coherence of LLM-generated summaries (i.e., their logical connectedness) under different prompting strategies. Our protocol unifies and extends best-practices across disparate works in document understanding and evaluation research, including adoption of fine-grained annotation units (Krishna et al., 2023), use of QA pairs to denote points of confusion (Ko et al., 2020), and a taxonomic breakdown of different coherence errors (Goyal et al., 2022a).","To lessen the effects of tainted data, we construct our assessment framework centered on recently published books. We avoid gathering ideal summary references (a procedure that is neither practical nor feasible for book-sized texts) and rather put forth a protocol harnessing human labeling of the coherence of large language model-created summaries (meaning their logical linkage) under various prompting tactics. Our protocol combines and builds on best practices across different works in document comprehension and evaluation research, encompassing embracing fine-grained annotation components (Krishna et al., 2023), utilizing QA pairs to indicate confusing points (Ko et al., 2020), and a systematic classification of various coherence errors (Goyal et al., 2022a).","To moderate the influence of polluted information, we design our testing framework around newly available books. We steer clear of assembling model summary references (an approach that is neither scalable nor viable for full book content) and instead suggest a protocol leveraging human review of the consistency of large language model-generated summaries (that is their logical connection) under different prompting strategies. Our protocol brings together and expands on best procedures across various efforts in document understanding and assessment research, including taking on precise annotation units (Krishna et al., 2023), applying QA pairs to flag confusing spots (Ko et al., 2020), and a taxonomic division of different coherence mistakes (Goyal et al., 2022a).  ","To reduce the impact of contaminated data, we construct our evaluation framework using freshly printed books. We avoid collecting ideal summaries (a tactic that is neither practical nor feasible for entire books) and instead put forward a protocol utilizing human examination of the cohesion of large language model-created summaries (meaning their logical linkage) under various prompting approaches. Our protocol combines and builds upon best practices across different works in document comprehension and assessment research, including adopting granular annotation components (Krishna et al., 2023), employing QA pairs to highlight confusing areas (Ko et al., 2020), and a systematic categorization of various coherence errors (Goyal et al., 2022a).",A,0
743,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"In our ablation study (Table 2), we find that edit distance and frequency controls are crucial to generate realistic synthetic data, confirming the effectiveness of the error patterns reported in §3.2.1. Critic’s training methods. Following (Yasunaga et al., 2021), we randomly sample 600 grammatical sentences and 600 ungrammatical sentences from GEC validation sets and use the averaged F0.5 score over 5 runs to measure the performance of the critic. We analyze the performance of our critic and compare it to LM-Critic in Table 4. We conduct an ablation study using the following configurations: (1) without employing the self-knowledge distillation method (SKD); (2) without applying the data augmentation approach (DA); and (3) without utilizing the high-confidence subset Dsub (CF). ","Our analysis of the components (Table 2) demonstrates that the use of edit distance and frequency controls are essential for generating realistic synthetic information, which supports the usefulness of the error patterns described in section 3.2.1. Methods for training the critic. As in Yasunaga et al. (2021), we randomly selected 600 grammatically correct sentences and 600 ungrammatical sentences from GEC validation sets and utilized the average F0.5 score over 5 runs to evaluate the critic's performance. We examined our critic's performance and compared it to LM-Critic in Table 4. We performed an ablation study using the following settings: (1) without applying the self-knowledge distillation approach (SKD); (2) without utilizing the data augmentation method (DA); and (3) without using the high-confidence subset Dsub (CF).","Our examination of the different components (Table 2) shows that using edit distance and frequency controls is vital for creating realistic synthetic data, which validates the usefulness of the error patterns described in section 3.2.1. Techniques for training the critic. Similar to Yasunaga et al. (2021), we arbitrarily chose 600 grammatically correct sentences and 600 ungrammatical sentences from GEC validation sets and used the mean F0.5 score over 5 runs to assess the critic's performance. We analyzed our critic's performance and contrasted it with LM-Critic in Table 4. We did an ablation study using these configurations: (1) without applying the self-knowledge distillation technique (SKD); (2) without employing the data augmentation method (DA); and (3) without utilizing the high-confidence subset Dsub (CF).  ","Our analysis of the different elements (Table 2) demonstrates that using edit distance and frequency controls is critical for generating realistic synthetic information, which corroborates the efficacy of the error patterns described in section 3.2.1. Procedures for training the critic. As in Yasunaga et al. (2021), we randomly selected 600 grammatically correct sentences and 600 ungrammatical sentences from GEC validation sets and used the average F0.5 score over 5 runs to gauge the critic's performance. We inspected our critic's performance and compared it against LM-Critic in Table 4. We conducted an ablation analysis using these settings: (1) without applying the self-knowledge distillation technique (SKD); (2) without using the data augmentation approach (DA); and (3) without employing the high-confidence subset Dsub (CF).",A,0
611,RESEE,"Informativeness:  1.  The system’s result is dull, repetitive, and does not have new information.  2.  Choose this score when you are hesitant between score 1 and score 3.  3.  The system’s result contains some new information and it displays a certain level of diversity.  4.  Choose this score when you are hesitant between score 3 and score 5.  5.  The system’s result is very informative and contains novel content.  In addition, it displays a high level of diversity and it is enjoyable to read. ","Dullness: 1. The system's response is monotonous, reiterative, and does not provide novel information. 2. Select this rating if you are uncertain between scores of 1 and 3. 3. The system's response contains some new details and exhibits some diversity. 4. Choose this assessment if you are unsure between ratings of 3 and 5. 5. The system's response is highly enlightening and incorporates original content. Furthermore, it shows a high amount of variety and is pleasant to read.","Interest: 1. The system's answer is boring, repetitive, and does not contribute new knowledge. 2. Pick this evaluation if you are wavering between 1 and 3. 3. The system's answer includes some novel information and displays some diversity. 4. Select this appraisal if you are wavering between 3 and 5. 5. The system's answer is very informative and comprises creative content. Additionally, it exhibits substantial diversity and is enjoyable to peruse. ","Engagement: 1. The system's response is monotonous, reiterative, and does not provide novel information. 2. Choose this rating if you are uncertain between scores of 1 and 3. 3. The system's response contains some new details and shows some variety. 4. Select this evaluation if you are ambivalent between ratings of 3 and 5. 5. The system's response is highly illuminating and incorporates imaginative content. Furthermore, it displays a high degree of diversity and is pleasant to read through.",A,0
55,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Considering slot labels and utterances may vary significantly across different datasets, we further evaluate the proposed method under the cross-dataset scenario, a more challenging setting. Here we introduce another popular slot filling dataset ATIS (Liu et al., 2019a). It is used for the target (source) domain data while the SNIPS for the source (target) domain data, as shown in Table 7. The results confirm that our method still works well in this challenging setting.","Evaluating the suggested approach in situations where the slot tags and utterances differ considerably between datasets poses an additional challenge. To test this, we apply the method to a more demanding cross-dataset setting using the ATIS dataset (Liu et al., 2019a) as the target domain and the SNIPS dataset as the source. Despite the greater difficulty, Table 7 shows our technique remains effective for slot filling across these distinct datasets.","We further assess the proposed approach under a more difficult cross-dataset configuration where slot labels and expressions diverge significantly. Here we utilize the ATIS dataset (Liu et al., 2019a) as the target domain and SNIPS as the source, representing a more demanding evaluation. Even in this challenging cross-domain scenario, the results in Table 7 confirm our method can still perform slot filling effectively between the two distinct datasets. ","To further evaluate the robustness of the proposed technique, we test it in a more challenging cross-dataset setting where slot annotations and utterances differ substantially between corpora. We use ATIS (Liu et al., 2019a) as the target domain data and SNIPS as the source, representing a difficult cross-domain slot filling scenario. Despite the greater complexity, Table 7 shows our approach remains capable of effective slot filling performance across these divergent datasets. This confirms the method's applicability even when slot labels and expressions vary considerably across corpora.",A,0
764,VECHR,"Heri proposes a complete typology encompassing eight types: dependency, state control, victimization, migration, discrimination, reproductive health, unpopular views and intersections thereof. Tab 1 gives a description for each type. VECHR consists of 788 cases under Article 3, which were collected based on Heri’s study of the Court’s case law references of vulnerability. See App B for details on Heri’s case sampling methodology and our post-processing procedures. ","Heri puts forward a comprehensive classification system including eight categories: reliance, government regulation, exploitation, movement, bias, health relating to giving birth, unliked opinions and combinations of these. Table 1 provides a description for each category. VECHR is made up of 788 examples under Article 3, which were gathered based on Heri's examination of the Court's case law mentions of susceptibility. Refer to Appendix B for information on Heri's case sampling methods and our post-processing steps.","Heri presents a full typology with eight kinds: dependence, state authority, mistreatment, migration, unfair treatment, reproductive wellbeing, controversial views and mixtures of those. The first table gives an explanation for each kind. VECHR consists of 788 instances under Article 3, which were identified based on Heri's review of the Court's case law references to vulnerability. See Appendix B for details about Heri's case sampling procedures and our subsequent processing.","Heri puts forth a comprehensive taxonomy with eight varieties: reliance, government control, abuse, relocation, discrimination, procreative health, unpopular opinions and combinations thereof. Table one provides a description of each variety. VECHR is comprised of 788 examples under Article 3, which were collected based on Heri's examination of the Court's case law mentions of susceptibility. Refer to Appendix B for information on Heri's case sampling methodology and our post-processing steps.",A,0
599,RESEE,"The provided datasets are auto-constructed, meaning visual biases brought by online searching are inevitable.  We plan to take our next step to make the dataset more accurate and to include more visual knowledge (e.g., visual knowledge from external document knowledge in WoW) in our multimodal dialogues.  (2) For now, we did not consider a one-to-one mapping between the textual entity and entity images in the model input, more sophisticated relations can also be introduced for better modal interaction and modeling.  (3) Our framework offers a novel way to enhance text-only dialogue system performance by adding extra information from a multimodal perspective.  However, this comes at the cost of extra computational overhead brought by learning visual knowledge. ","The given data was automatically generated, so inherent visual biases from online searching exist. We intend to improve the data's accuracy and incorporate more visual knowledge (e.g. visual knowledge from external document knowledge in WoW) into our multimodal conversations. Currently, we did not consider a one-to-one mapping between textual entities and entity images in the model input, but more complex relationships can also be introduced for better modal interaction and modeling. Our framework provides a new way to boost text-only dialogue system performance by adding extra information from a multimodal viewpoint. However, this comes with the extra computational cost of learning visual knowledge.","The supplied data was auto-created, so visual biases from online searching are unavoidable. We want to make the data more precise and include more visual knowledge (like visual knowledge from external document knowledge in WoW) in our multimodal dialogues. For now, we did not think about a one-to-one mapping between the text entity and entity images in the model input, more intricate relations can also be introduced for superior modal interaction and modeling. Our system offers a novel approach to improve text-only dialogue system performance by incorporating extra information from a multimodal angle. However, this requires extra computational overhead to learn visual knowledge.  ","The given information was machine-generated, so visual biases from web searching are inevitable. We intend to improve the data's precision and incorporate more visual knowledge (for instance visual knowledge from external document knowledge in WoW) into our multimodal conversations. Currently, we did not consider a one-to-one mapping between text entities and entity images in the model input, but more elaborate relationships can also be introduced for superior modal interaction and modeling. Our framework provides a new technique to enhance text-only dialogue system performance by adding supplementary information from a multimodal viewpoint. However, this necessitates extra computational cost to learn visual knowledge.",A,0
388,INSTRUCTSCORE,"Future research can investigate more advanced techniques, such as incorporating human feedback through reinforcement (Ouyang et al., 2022), for more effective integration of feedback into the training pipeline. More sophisticated approach holds promising potential to further boost the performance of this pipeline.","Further studies could look into more complex methods, like adding human input through reinforcement (Ouyang et al., 2022), to better incorporate feedback into the training process. A more advanced approach has promising possibilities to additionally improve the effectiveness of this pipeline.","Upcoming work could explore more sophisticated techniques, such as integrating human critiques via reinforcement learning (Ouyang et al., 2022), to more successfully fuse feedback into the training workflow. A more refined methodology has encouraging potential to further enhance the performance of this pipeline. ","Future investigations could analyze more elaborate procedures, like incorporating human reviews through reinforcement (Ouyang et al., 2022), for superior assimilation of feedback into the training sequence. A more refined system shows promising prospects to additionally boost the capabilities of this pipeline.",A,0
547,Non-autoregressive Text Editing with Copy-aware Latent Alignments,"The output space of vanilla CTC comprises the general vocabulary V as well as the blank token ∅. We can utilize CTC to mimic the edit processes by symbolizing generating a token t P V as ADDt, representing the insertion operation, and ∅ as DELETE, meaning deleting a source token. This satisfies the aforementioned desiderata of learning to edit with a minimal set of operations (Dong et al., 2019), and maintaining enough flexibility by means of marginalizing all latent alignments defined over the entire vocabulary. However, vanilla CTC is still wasteful for text editing as it lacks explicit modeling of the copy behavior. ","The set of possible outputs for standard CTC includes the overall vocabulary V and the blank symbol ∅. We can use CTC to imitate the editing processes by encoding generating a token t in V as ADDt, which represents inserting that token, and using ∅ for DELETE to mean removing a source token. This meets the previously stated criteria of learning to edit with a minimal set of operations (Dong et al., 2019), and keeping enough flexibility by marginalizing over all possible alignments over the whole vocabulary. However, standard CTC is still inefficient for text editing since it does not explicitly model the copying behavior.","The output space of regular CTC is composed of the full vocabulary V and the empty token ∅. We can make CTC mimic editing operations by denoting generating a token t from V as ADDt to represent inserting t, and using ∅ to represent DELETE, which is removing a source token. This fulfills the aforementioned goals of learning to edit with a small set of operations (Dong et al., 2019), and maintaining sufficient flexibility by summing over all potential alignments defined on the complete vocabulary. However, unmodified CTC is still suboptimal for text editing because it does not directly model copying.","The set of outputs for vanilla CTC consists of the general vocabulary V and the blank symbol ∅. We can get CTC to simulate editing steps by encoding generating a token t in V as ADDt to represent inserting t, and using ∅ for DELETE to mean deleting a source token. This satisfies the previously mentioned aims of learning to edit with a minimal set of operations (Dong et al., 2019), and keeping enough flexibility by marginalizing all possible alignments over the full vocabulary. However, plain CTC still wastes effort for text editing since it lacks explicit modeling of copying behavior.",A,0
360,Improved Techniques for Training Consistency Models,"We train all models with the RAdam optimizer (Liu et al., 2019) using learning rate 0.0001. All CIFAR-10 models are trained for 400,000 iterations, whereas ImageNet 64´64 models are trained for 800,000 iterations. For CIFAR-10 models in Section 3, we use batch size 512 and EMA decay rate 0.9999 for the student network. For iCT and iCT-deep models in Table 2, we use batch size 1024 and EMA decay rate of 0.99993 for CIFAR-10 models, and batch size 4096 and EMA decay rate 0.99997 for ImageNet 64´64 models. All models are trained on a cluster of Nvidia A100 GPUs.","We optimize every model using the RAdam optimization algorithm (Liu et al., 2019) with a learning rate of 0.0001. All CIFAR-10 models are optimized for 400,000 iterations, while ImageNet 64x64 models are optimized for 800,000 iterations. For CIFAR-10 models in Section 3, we utilize a batch size of 512 and an EMA decay rate of 0.9999 for the student network. For iCT and iCT-deep models in Table 2, we use a batch size of 1024 and an EMA decay rate of 0.99993 for CIFAR-10 models, and a batch size of 4096 and an EMA decay rate of 0.99997 for ImageNet 64x64 models. We optimize all models on a cluster of Nvidia A100 GPUs.","Every model is trained using the RAdam optimization method (Liu et al., 2019) with a learning rate set to 0.0001. The CIFAR-10 models are trained for 400,000 iterations, while the ImageNet 64x64 models are trained for 800,000 iterations. For the CIFAR-10 models in Section 3, we employ a batch size of 512 and an EMA decay rate of 0.9999 for the student network. For the iCT and iCT-deep models in Table 2, we utilize a batch size of 1024 and an EMA decay rate of 0.99993 for the CIFAR-10 models, and a batch size of 4096 and an EMA decay rate of 0.99997 for the ImageNet 64x64 models. We train all the models on a cluster of Nvidia A100 GPUs.  ","We optimize all models utilizing the RAdam optimization algorithm (Liu et al., 2019) with a learning rate of 0.0001. The CIFAR-10 models are optimized for 400,000 iterations, while the ImageNet 64x64 models are optimized for 800,000 iterations. For the CIFAR-10 models in Section 3, we use a batch size of 512 and an EMA decay rate of 0.9999 for the student network. For the iCT and iCT-deep models in Table 2, we utilize a batch size of 1024 and an EMA decay rate of 0.99993 for the CIFAR-10 models, and a batch size of 4096 and an EMA decay rate of 0.99997 for the ImageNet 64x64 models. We optimize all the models on a cluster of Nvidia A100 GPUs.",A,0
266,DisCo Distilled Student Models Co-training for Semi-supervised Text Mining,"Compared with the FLiText, DisCo improves the average classification accuracy by 1.9% while using a student model with 0.7M fewer parameters than FLiText. FLiText relies heavily on backtranslation models for generating augmented data, similar to UDA. Unfortunately, this strategy fails to eliminate error propagation introduced by the back-translation model and requires additional data pre-processing.","The DisCo model surpasses the FLiText model in classification accuracy by 1.9% while utilizing a student network with 0.7M less parameters. FLiText depends greatly on backtranslation models to synthesize augmented data, analogous to UDA. However, this technique is ineffective at removing error propagation induced by the backtranslation model and necessitates extra data preprocessing.","When compared to FLiText, DisCo improves classification accuracy by 1.9% with a student model containing 0.7M fewer parameters. FLiText is heavily reliant on backtranslation models, like UDA, for generating extra training data. But this approach fails to eliminate the error propagation from the backtranslation model and requires additional data processing.  ","Contrasted with FLiText, DisCo boosts average classification performance by 1.9% using a student network with 0.7M less parameters. FLiText strongly depends on backtranslation models to produce synthetic training data, similar to UDA. Unfortunately, this methodology is unable to remove the error introduction by the backtranslation model and needs supplementary data preparation.",A,0
286,"Explain, Edit, Generate","Furthermore, ensuring the label flipping and linguistic diversity of generated counterfactuals become increasingly difficult with the premise of logical relationships, which are critical factors to assure the quality of the counterfactuals. To address these challenges, we develop a novel pipeline method, RACE (RAtionale-sensitive Counterfactual gEneration), by focusing on the causal features within the rationales extracted from the multi-hop evidence using an explainability method. In specific, for each original instance, the Explainer and Editor modules are employed to produce the counterfactual evidence that logically corresponds to — but factually distinct from — the original claim.","Moreover, guaranteeing the label reversal and linguistic variety of the produced counterfactuals grows more problematic as we try to maintain logical connections, which are key elements for ensuring counterfactual quality. To tackle these problems, we have created a new pipeline approach, RACE (RAtionale-sensitive Counterfactual gEneration), by concentrating on the causal features within the rationales extracted from the multi-hop evidence using an explainability technique. Specifically, for every original example, the Explainer and Editor modules are used to generate counterfactual evidence that is logically aligned with but factually different from the original claim.","In addition, making sure the label flipping and language diversity of the generated counterfactuals becomes more difficult when trying to preserve logical relationships, which are essential factors for guaranteeing counterfactual quality. To address these difficulties, we have developed a new pipeline method called RACE (RAtionale-sensitive Counterfactual gEneration), by focusing on the causal features within the rationales obtained from the multi-hop evidence using an explainability approach. Precisely, for every initial case, the Explainer and Editor modules are utilized to produce counterfactual evidence that is logically consistent with but factually distinct from the original claim.  ","Furthermore, ensuring the label reversal and linguistic variety of the created counterfactuals becomes increasingly challenging with the requirement to maintain logical connections, which are vital elements to ensure the quality of the counterfactuals. To tackle these problems, we have invented a new pipeline technique, RACE (RAtionale-sensitive Counterfactual gEneration), by concentrating on the causal features within the rationales derived from the multi-hop evidence using an explainability method. Specifically, for every starting example, the Explainer and Editor modules are employed to generate counterfactual evidence that logically aligns with but is factually different from the original claim.",A,0
716,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"State-of-the-art grammatical error correction (GEC) systems rely on parallel training data (ungrammatical sentences and their manually corrected counterparts), which are expensive to construct. In this paper, we employ the Break-It-Fix-It (BIFI) method to build an unsupervised GEC system. The BIFI framework generates parallel data from unlabeled text using a fixer to transform ungrammatical sentences into grammatical ones, and a critic to predict sentence grammaticality. We present an unsupervised approach to build the fixer and the critic, and an algorithm that allows them to iteratively improve each other. We evaluate our unsupervised GEC system on English and Chinese GEC. Empirical results show that our GEC system outperforms previous unsupervised GEC systems, and achieves performance comparable to supervised GEC systems without ensemble. ","Modern grammatical error correction systems depend on parallel data of ungrammatical sentences and their manually fixed versions, which are costly to create. In this work, we use the Break-It-Fix-It method to build an unsupervised GEC system. The BIFI framework produces parallel data from unlabeled text by using a fixer to transform ungrammatical sentences into grammatical ones, and a critic to predict sentence grammaticality. We present an unsupervised way to build the fixer and critic, and an algorithm that lets them iteratively enhance each other. We evaluate our unsupervised GEC system on English and Chinese GEC. Experimental results indicate that our GEC system surpasses previous unsupervised GEC systems, and achieves performance comparable to supervised GEC systems without ensemble.","State-of-the-art systems for correcting grammatical errors rely on parallel training information (sentences with errors and their manually corrected versions), which require significant effort to generate. In this paper, we use the Break-It-Fix-It (BIFI) approach to construct an unsupervised GEC system. The BIFI method produces parallel data from unlabeled text by applying a fixer to change ungrammatical sentences into grammatical ones, and a critic to judge sentence grammaticality. We introduce an unsupervised way to build the fixer and critic, and an algorithm that enables them to iteratively enhance each other. We evaluate our unsupervised GEC system on English and Chinese GEC. Empirical results demonstrate that our GEC system is superior to previous unsupervised GEC systems, and achieves performance on par with supervised GEC systems without ensemble.","Current best grammatical error correction systems need parallel training data of incorrect sentences and their manually fixed versions, which take substantial effort to create. In this paper, we use the Break-It-Fix-It framework to build an unsupervised GEC system. The BIFI approach generates parallel data from unlabeled text through a fixer that transforms ungrammatical sentences into grammatical ones, and a critic that predicts sentence grammaticality. We present an unsupervised method to construct the fixer and critic, and an algorithm that allows them to iteratively improve each other. We evaluate our unsupervised GEC system on English and Chinese GEC. Experimental results show our GEC system outperforms previous unsupervised GEC systems, and achieves performance comparable to supervised GEC systems without ensemble.",A,0
60,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Previous studies for slot filling mainly focus on instance-level contrastive learning, which may be sub-optimal for a fine-grained sequence labeling task. Inspired by supervised contrastive learning, we leverage a slot-level contrastive learning scheme for zero-shot slot filling to learn the discriminative representations for domain adaptation. For all existing slot entities within a mini-batch, we regard those with the same type as the positive example pairs and those with different type as negative ones.","Earlier research on slot filling has concentrated mostly on contrastive learning at the instance level, which may not be ideal for a fine-grained sequence labeling task. Drawing inspiration from supervised contrastive learning, we employ a slot-level contrastive learning approach for zero-shot slot filling to learn discriminative representations for domain adaptation. For all slot entities present in a mini-batch, we treat those having the same type as positive example pairs and those having different types as negative ones.","Previous work on slot filling has focused primarily on instance-based contrastive learning, which could be suboptimal for a granular sequence labeling task. Motivated by supervised contrastive learning, we use a slot-level contrastive learning scheme for zero-shot slot filling to acquire discriminative representations for domain adaptation. For all existing slot entities in a mini-batch, we consider those sharing the same type as positive example pairs and those with differing types as negative ones.  ","Earlier slot filling studies have concentrated mainly on contrastive learning at the instance level, which may not be best for a fine-grained sequence labeling task. Inspired by supervised contrastive learning, we utilize a slot-level contrastive learning approach for zero-shot slot filling to learn discriminative representations for domain adaptation. For all slot entities present in a mini-batch, we view those having the same type as positive example pairs and those having different types as negative ones.",A,0
568,RESEE,"We thus propose to explicitly divide the visual standard of a dialogue session into turn-level and entity-level.  (2) Instead of matching photos from existing image sets, we search images on the internet for every entity to obtain accurate and diverse visual representations accordingly.  To justify the advantage of our approach in obtaining pictures with higher quality, we randomly sample 50 entities from existing dialogue data and either search corresponding images from the internet or retrieve them from a large image corpus with over 150K images.1 We further conduct a human evaluation to quantify entity-image relevance. ","Therefore, we suggest clearly separating the visual norms of a conversation session into turn-by-turn and entity-by-entity levels. Rather than pairing images from current image collections, we look on the internet for pictures of each entity to get precise and varied visual illustrations as needed. To prove the benefit of our method for finding higher quality images, we arbitrarily choose 50 entities from existing conversation information and either look up matching images online or extract them from a large image collection with over 150,000 images. We also perform a human assessment to quantify the relevance between entities and images.","As a result, we recommend explicitly dividing the visual principles of a discussion session into segment-by-segment and item-by-item categories. Instead of matching photographs from available image databases, we search for pictures on the web for each item to acquire accurate and diverse visual representations as required. To validate the advantage of our technique for acquiring higher quality pictures, we randomly pick 50 items from current discussion data and either look up corresponding images online or obtain them from a substantial image collection containing over 150,000 images. We further conduct a human evaluation to quantify the relationship between items and images.  ","Consequently, we propose clearly separating the visual guidelines of a chat session into turn-by-turn and element-by-element divisions. Rather than pairing pictures from existing image repositories, we look on the internet for images of each element to acquire precise and varied visual illustrations as needed. To demonstrate the benefit of our approach for finding higher quality visuals, we arbitrarily select 50 elements from current chat data and either search for matching visuals online or extract them from a large image repository containing over 150,000 images. We also perform a human assessment to quantify the connection between elements and visuals.",A,0
362,INSTRUCTSCORE,"Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text. To address this limitation, we present INSTRUCTSCORE, a fine-grained explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report.","Judging the value of computer-generated language is very important. While recently created statistical measures match human opinion well, they don't clarify why they rate text as they do, or connect scores to flaws in the text. To fix this issue, we introduce INSTRUCTSCORE, a detailed explainable way to rate generated text. By using both direct human guidance and the implicit knowledge of GPT-4, we customize a text rating method based on LLaMA, making both a score for the text and a human readable analysis report.","Assessing the excellence of automatically produced language is crucial. Though latest learned gauges have high agreement with human judgment, they don't elucidate their verdict or associate the marks with defects in the generated text. To address this shortcoming, we present INSTRUCTSCORE, a fine-grained accountable metric for evaluating text generation. By leveraging both explicit human instruction and the implicit expertise of GPT-4, we fine-tune a text evaluation measure based on LLaMA, generating both a grade for the text and an interpretable diagnostic report.  ","Determining the merit of machine-made language is imperative. While recently created learned measures have strong correlation with human assessment, they don't explain their rating or connect the scores to flaws in the generated text. To remedy this limitation, we introduce INSTRUCTSCORE, a detailed responsible metric for evaluating generated text. By utilizing both direct human guidance and the implicit knowledge within GPT-4, we customize a text evaluation measure based on LLaMA, producing both a grade for the text and an understandable analysis report.",A,0
250,DisCo Distilled Student Models Co-training for Semi-supervised Text Mining,"Large pre-trained language models (PLMs), such as BERT (Devlin et al., 2019), GPT-3 (Brown et al., 2020), play a crucial role in the development of natural language processing applications, where one prominent training regime is to fine-tune the large and expensive PLMs for the downstream tasks of interest (Jiao et al., 2020). Minimizing the model size and accelerating the model inference are desired for systems with limited computation resources, such as mobile (Liu et al., 2021) and edge (Tambe et al., 2021) devices.","Massive pre-trained language systems (PLSs), like BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020), are very important in creating natural language processing apps. A common training approach is to adjust the large and costly PLSs for specific downstream tasks (Jiao et al., 2020). Reducing the model size and speeding up the model inference are wanted for systems with restricted computing power, like mobile (Liu et al., 2021) and edge (Tambe et al., 2021) devices.","Enormous pre-trained language architectures (PLAs), for instance BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020), play a key role in building natural language processing services, where one popular training methodology is to fine-tune the large and expensive PLAs for the specific downstream tasks of interest (Jiao et al., 2020). Minimizing the model dimensions and accelerating the model deduction are desirable for systems with limited calculation resources, such as mobile (Liu et al., 2021) and edge (Tambe et al., 2021) devices.  ","Massive pre-trained language models (PLMs), like BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020), are critical in developing natural language processing tools, where one common training approach is to adapt the large and costly PLMs for the particular downstream tasks of interest (Jiao et al., 2020). Reducing the model size and speeding up the model inference are beneficial for systems with constrained computing capabilities, such as mobile (Liu et al., 2021) and edge (Tambe et al., 2021) devices.",A,0
209,Counting the Bugs in ChatGPTs Wugs A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,We investigate the efficacy of several baselines for the task of morphological inflection. The chosen baselines encompass both statistical and neural architectures that have shown impressive performance on the morphological generalization task in recent years. We evaluate their performance on the SIGMORPHON 2023 task as well as on our constructed wug test set. The baselines have complementary strengths (see Section 5).,We examine the effectiveness of multiple standard models for the job of morphologically changing words. The selected standard models cover both statistical and neural network designs that have demonstrated remarkable capabilities on the task of morphologically generalizing words in recent times. We assess their capabilities on the SIGMORPHON 2023 challenge and on our created nonce word test set. The standard models have complementary strengths (see Section 5).,We study the usefulness of several baseline systems for the process of inflecting words morphologically. The chosen baseline systems include both statistical and neural network architectures which have shown impressive abilities on the task of morphological generalization in recent years. We evaluate their abilities on the SIGMORPHON 2023 competition and also on our constructed nonce word test set. The baseline systems have complementary advantages (see Section 5). ,We investigate the utility of multiple basic models for the process of morphologically inflecting words. The selected basic models involve both statistical and neural network designs which have exhibited remarkable performance on the task of morphologically generalizing words in recent times. We assess their performance on the SIGMORPHON 2023 competition and also on our created pseudo-word test set. The basic models have complementary strengths (see Section 5).,A,0
428,Eliminating Lipschitz Singularities in Diffusion Models,"Both qualitative and quantitative results confirm that our approach substantially alleviates the large Lipschitz constants near zero point and improves the synthesis performance compared to the DDPM baseline (Ho et al., 2020). We also compare this simple approach with other potential methods to address the challenge of large Lipschitz constants, and find our method outperforms all of these alternative methods. In conclusion, in this work, we theoretically prove and empirically observe the presence of Lipschitz singularities issue near the zero point, advancing the understanding of the diffusion process. Besides, we propose a simple yet effective approach to address this challenge and achieve impressive improvements.","The qualitative and quantitative findings validate that our method greatly reduces the large Lipschitz constants near the zero point and enhances the synthesis capabilities over the DDPM baseline (Ho et al., 2020). We also contrast this straightforward technique with other possibilities to tackle the issue of large Lipschitz constants, and determine our approach surpasses all of these other options. To summarize, in this work, we theoretically demonstrate and empirically notice the existence of Lipschitz singularities problem near the zero point, progressing the comprehension of the diffusion process. Additionally, we suggest a simple but powerful solution to address this issue and accomplish remarkable enhancements.","Both the qualitative and quantitative data support that our approach significantly decreases the large Lipschitz constants close to the zero point and improves the synthesis performance compared to the DDPM baseline (Ho et al., 2020). We also compare this easy approach with other potential ways to handle the challenge of large Lipschitz constants, and find our method outperforms all of these other methods. In conclusion, in this work, we theoretically prove and empirically see the presence of Lipschitz singularities problem near the zero point, advancing the understanding of the diffusion process. Furthermore, we propose a straightforward yet effective solution to tackle this challenge and achieve impressive gains.  ","The qualitative and quantitative analyses confirm that our method considerably reduces the large Lipschitz constants near the zero point and enhances the synthesis capabilities compared to the DDPM baseline (Ho et al., 2020). We also contrast this simple technique with other possible approaches to address the issue of large Lipschitz constants, and determine our approach surpasses all of these alternatives. In summary, in this work, we theoretically demonstrate and empirically detect the presence of Lipschitz singularities issue near the zero point, advancing the comprehension of the diffusion process. In addition, we suggest a straightforward yet powerful solution to tackle this challenge and realize remarkable improvements.",A,0
356,Improved Techniques for Training Consistency Models,"As noted by Kynkäänniemi et al. (2023), this can skew FIDs and lead to inflated sample quality. Methods based on LPIPS suffer from similar issues, as LPIPS is also pre-trained on ImageNet. We include these methods in Tables 2 and 3 for completeness, but we do not consider them as direct competitors to iCT or iCT-deep methods. Several key observations emerge from Tables 2 and 3. First, iCT methods surpass previous diffusion distillation approaches in both one-step and two-step generation on CIFAR-10 and ImageNet 64´64, all while circumventing the need for training diffusion models.","As pointed out by Kynkäänniemi and colleagues (2023), this can distort FIDs and result in overstated sample quality. Approaches relying on LPIPS have comparable problems, since LPIPS is also pre-trained on ImageNet. We have included these methods in Tables 2 and 3 for thoroughness, but we do not view them as direct rivals to iCT or iCT-deep approaches. A few key takeaways emerge from Tables 2 and 3. First, iCT techniques exceed prior diffusion distillation methods in both one-step and two-step generation on CIFAR-10 and ImageNet 64x64, all without needing to train diffusion models.","As indicated by Kynkäänniemi and coauthors (2023), this could skew FIDs and produce inflated sample quality. Methods utilizing LPIPS have similar deficiencies, given that LPIPS is also pre-trained on ImageNet. We have incorporated these methods in Tables 2 and 3 for completeness, however we do not deem them as direct competitors to iCT or iCT-deep techniques. Several major observations result from Tables 2 and 3. Initially, iCT approaches surpass previous diffusion distillation methods in both one-step and two-step generation on CIFAR-10 and ImageNet 64x64, all while avoiding the necessity to train diffusion models. ","As highlighted by Kynkäänniemi and colleagues (2023), this can distort FIDs and yield overstated sample quality. Procedures relying on LPIPS have analogous issues, since LPIPS is also pre-trained on ImageNet. We have included these methods in Tables 2 and 3 for thoroughness, however we do not view them as direct alternatives to iCT or iCT-deep procedures. A few principal takeaways emerge from Tables 2 and 3. Firstly, iCT techniques exceed prior diffusion distillation approaches in both one-step and two-step generation on CIFAR-10 and ImageNet 64x64, all without necessitating training diffusion models.",A,0
626,SOUL,"Annotators are instructed to delve beyond the surface-level content and generate more challenging statements that require a deeper level of sentiment and opinion understanding ability. For instance, simply describing the user does not like the product is discouraged, but statements focusing on mixed sentiments towards various aspects, or the underlying reasons behind opinions are encouraged. In the meantime, the label of each statement is annotated. Unlike traditional natural language inference (NLI) tasks, the primary objective of statement annotation is to extract and label subjective information rather than establish logical connections or entailment between different texts. Besides, we request annotators to provide justifications for their proposed statements. These justifications provide the rationale behind the statement categorization. ","Annotators are told to look past superficial content and create more complex statements needing greater comprehension of sentiments and opinions. For example, just saying a user dislikes a product is discouraged. Instead, focus on mixed feelings about aspects or reasons behind views. Also, label each statement. Unlike usual natural language inference tasks, the main goal here is extracting and tagging subjective data rather than logical links between texts. Additionally, justify your statement labels. These rationales explain the reasoning for the categorization.","Annotators should dig deeper than surface meaning and generate harder statements needing more understanding of sentiments and stances. Simply stating displeasure is discouraged. Rather, highlight mixed opinions on facets or motivations underlying perspectives. Moreover, categorize each statement. Diverging from typical natural language inference, the chief aim is extracting and marking subjective information over logical connections between texts. Annotators should also give reasons for their labels. These justifications provide the thinking behind the categorizations.  ","Annotators are instructed to look past superficial content and create more thought-provoking statements necessitating greater grasp of sentiments and viewpoints. Merely expressing dislike is discouraged. Instead, focus on mixed feelings regarding aspects or rationales behind opinions. Also, classify each statement. Unlike standard natural language inference, the primary goal here is extracting and labeling subjective data rather than logical links between texts. In addition, justify your categorizations. These explanations provide the reasoning behind the classifications.",A,0
213,Counting the Bugs in ChatGPTs Wugs A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,"As a baseline for the 2020 and 2021 SIGMORPHON shared tasks, a simple non-neural system (Liu and Mao, 2016) was implemented that uses edit distance to “discover prefix and suffix rules in training data.”4 At test time, the system modifies a lemma by applying the longest matching suffix rule and most frequently applied prefix rule for a given morphosyntactic description.","To establish a starting point for the 2020 and 2021 SIGMORPHON shared tasks, a basic non-neural system (Liu and Mao, 2016) was put into practice that utilizes edit distance to ""find prefix and suffix patterns in training information."" At test time, the system alters a lemma by applying the longest matching suffix rule and most often used prefix rule for a particular morphosyntactic description.","As a foundation for the 2020 and 2021 SIGMORPHON shared tasks, a simple non-artificial intelligence system (Liu and Mao, 2016) was executed that harnesses edit proximity to ""detect prefix and suffix conventions in training data."" During testing, the system modifies a lemma by administering the longest corresponding suffix principle and most frequently employed prefix principle for a given morphosyntactic portrayal.  ","To form a baseline for the 2020 and 2021 SIGMORPHON shared tasks, an uncomplicated non-neural system (Liu and Mao, 2016) was implemented that leverages edit closeness to ""identify prefix and suffix norms in training material."" At evaluation time, the system alters a lemma by applying the longest fitting suffix norm and most commonly used prefix norm for a specific morphosyntactic illustration.",A,0
630,SOUL,"We can make the following observations: 1) All models exhibit limited sentiment ability, resulting in a performance gap of 17% to 27% compared to human performance. This shows the difficulty of the RC task, and there is still much room for improvement in developing models that can accurately comprehend sentiment and opinion. The challenges may arise from the complexity and diversity of statements that incorporate mixed sentiments, underlying reasons of opinions, and other aspects. 2) Among SLMs, Flan-T5 achieves the best performance, surpassing T5 with the same model size by 1.41%, possibly due to the effectiveness of instruction tuning during its training process. 3) LLMs demonstrate effective zero-shot ability, with Flan-T5XXL achieving the best results even without any training data. ","We can draw the following conclusions: 1) All models display restricted emotional understanding, resulting in a performance gap of 17% to 27% compared to human ability. This highlights the difficulty of the RC task, and there remains ample room for enhancing models to precisely grasp sentiment and viewpoint. The obstacles may stem from the intricacy and variety of statements encompassing blended emotions, fundamental rationales of perspectives, and other facets. 2) Among SLMs, Flan-T5 accomplishes the best results, outperforming T5 with identical model scale by 1.41%, potentially owing to the efficacy of guidance tuning during its training procedure. 3) LLMs exhibit effective zero-shot capacity, with Flan-T5XXL attaining the highest outcomes even lacking any training information.","We can make these observations: 1) All models have limited ability to understand sentiment, lagging human performance by 17% to 27%. This demonstrates the challenge of the RC task, and substantial progress remains to develop models that accurately comprehend sentiment and opinions. Complex and diverse statements with mixed emotions, underlying rationales, and other factors may explain the difficulty. 2) Among SLMs, Flan-T5 performs best, exceeding equally sized T5 by 1.41%, likely due to the usefulness of instruction tuning during training. 3) LLMs show effective zero-shot capacity, with Flan-T5XXL achieving the top results without any training data.  ","These are our takeaways: 1) All models have restricted sentiment comprehension, trailing humans by 17% to 27%, underscoring the difficulty of RC. Much room for improvement remains to create models accurately grasping sentiment and perspectives. Challenges may stem from intricate, varied statements with blended emotions, fundamental opinion rationales, etc. 2) Among SLMs, Flan-T5 excels, topping equally sized T5 by 1.41%, potentially owing to beneficial instruction tuning during training. 3) LLMs demonstrate effective zero-shot ability, with Flan-T5XXL achieving the best results sans training data.",A,0
90,ALDi Quantifying the Arabic Level of Dialectness of Text,"The Sentence ALDi model provides more nuanced scores while also showing strong discrimination between MSA and DA, even for DA variants that are barely present in AOC-ALDi (TUN, MOR, MGR; note that Token DI also has not seen these). It also yields slightly lower scores for the DA versions of the Bible than for the DA tweets, indicating that the informal genre of tweets may be an indicator of stronger dialectness levels.","The Sentence ALDi framework gives more detailed evaluations while also displaying a clear distinction between MSA and DA, including for DA varieties that have very little presence in AOC-ALDi (TUN, MOR, MGR; note that Token DI has also not encountered these). It also produces slightly lower marks for the DA versions of the Bible versus the DA tweets, hinting that the casual style of tweets could signify stronger degrees of dialect usage.","The Sentence ALDi system provides more nuanced rankings and still shows a strong ability to differentiate between MSA and DA, even for DA forms that barely exist in AOC-ALDi (TUN, MOR, MGR; Token DI has also not seen these). It also assigns somewhat lower scores to the DA versions of the Bible compared to the DA tweets, suggesting that the informal nature of tweets may indicate higher levels of dialect usage.  ","The Sentence ALDi approach gives more subtle evaluations and continues to display clear separation between MSA and DA, including for DA varieties that have minimal presence in AOC-ALDi (TUN, MOR, MGR; note Token DI has also not encountered these). It also produces slightly lower ratings for the DA versions of the Bible versus the DA tweets, indicating that the casual genre of tweets could be a sign of stronger dialectness.",A,0
511,Neural Fine-Tuning Search for Few-Shot Learning,"We term our method Neural Fine-Tuning Search (NFTS). NFTS defines a search space that is relevant to both convolutional and transformers architectures, and the choice of which specific adapter modules to consider is a hyperparameter, rather than a hard constraint. Our contributions are summarised as follows: (i) We provide the first systematic Auto-ML approach to finding the optimal adaptation strategy to trade off adaptation flexibility and overfitting in multidomain FSL. (ii) Our novel NFTS algorithm automatically determines which layers should be frozen or adapted, and where new adaptation parameters should be inserted for best few-shot adaptation. (iii) We advance the state-of-the-art in the well-established and challenging Meta-Dataset (Triantafillou et al., 2020), and the more recent and diverse Meta-Album (Ullah et al., 2022) benchmarks.","We call our approach Neural Fine-Tuning Search (NFTS). NFTS characterizes a search area that is applicable to both convolutional and transformer models, and the decision of which particular adapter components to examine is a hyperparameter, not an inflexible requirement. Our key contributions are: (i) We present the first methodical AutoML way to find the best adaptation plan to balance flexibility and overfitting in multidomain few-shot learning. (ii) Our new NFTS algorithm automatically decides which layers to freeze or adapt, and where to insert new adaptation parameters for optimal few-shot adaptation. (iii) We improve the state-of-the-art on the well-established and challenging Meta-Dataset (Triantafillou et al., 2020), and the more recent diverse Meta-Album (Ullah et al., 2022) benchmarks.","Our approach is termed Neural Fine-Tuning Search (NFTS). NFTS defines a search space applicable to convolutional and transformer architectures, where the choice of adapter modules is a hyperparameter rather than a constraint. We make the following key contributions: (i) We introduce the first systematic AutoML method for optimally trading off adaptation flexibility and overfitting in multidomain few-shot learning. (ii) Our novel NFTS algorithm automatically chooses which layers to freeze/adapt and where to insert new adaptation parameters for optimal few-shot adaptation. (iii) We advance the state-of-the-art on the challenging Meta-Dataset (Triantafillou et al., 2020) and diverse Meta-Album (Ullah et al., 2022) benchmarks.  ","We name our technique Neural Fine-Tuning Search (NFTS). NFTS characterizes a search space suitable for convolutional and transformer models, with the selection of adapter modules as a hyperparameter instead of a fixed requirement. Our main contributions are: (i) We provide the first systematic AutoML approach for optimally balancing adaptation flexibility and overfitting in multidomain few-shot learning. (ii) Our new NFTS algorithm automatically determines which layers to freeze/adapt and where to insert new adaptation parameters for best few-shot adaptation. (iii) We push the state-of-the-art on the challenging Meta-Dataset (Triantafillou et al., 2020) and diverse Meta-Album (Ullah et al., 2022) benchmarks.",A,0
181,Copyright Violations and Large Language Models,"What is fair use in language models is also an ethical question. Our study aims to shed light on the extent of verbatim memorization in large language models. Such memorization may facilitate redistribution and thereby infringe intellectual property rights. Is that really fair? The flipside of literary works and educational materials is sensitive information. Here, new risks arise. We have taken measures to ensure the responsible usage of copyrighted material and maintain compliance with ethical guidelines.",The ethical implications of how much verbatim material language models store is an important issue to examine. Our research hopes to clarify the degree to which large language models memorize content word-for-word. This kind of memorization could make it easier to redistribute copyrighted material unlawfully. Is that truly ethical? There are also risks associated with sensitive data being memorized. We have implemented safeguards to promote responsible use of copyrighted content and adherence to ethical principles.,"What represents morally right usage in language models also raises questions of fairness. Our investigation aims to illuminate the extent of literal storage in large language models. This sort of storage may enable illegal sharing and breach intellectual property laws. Is that genuinely just? Copyrighted literary works and educational resources contrast with private information. Here, new hazards emerge. We have instituted protections to ensure lawful employment of copyrighted material and obedience to ethical guidelines.  ","The ethical appropriateness of how much literal material language models retain is an important matter to analyze. Our analysis strives to clarify the amount that large language models store content verbatim. This kind of storage could facilitate unlawful redistribution, violating intellectual property laws. Is that truly fair? In contrast to literary and educational content are sensitive data. Here, new risks materialize. We have implemented safeguards to promote legal use of copyrighted material and compliance with ethical standards.",A,0
394,INTERFAIR Debiasing with Natural Language Feedback for Fair Interpretable Predictions,"Debiasing human written text is an important scientific and social problem that has been investigated by several recent works (Zhang et al., 2018; Jentzsch et al., 2019; Badjatiya et al., 2019; Heindorf et al., 2019; Ravfogel et al., 2020; Gonen and Goldberg, 2019; He et al., 2021). These methods primarily try to eliminate the biased information from the model’s internal representations or from the input itself, disregarding the task performance during the process.","Removing unfair biases from text written by people is a crucial scientific and societal issue that recent studies have looked into (Zhang et al., 2018; Jentzsch et al., 2019; Badjatiya et al., 2019; Heindorf et al., 2019; Ravfogel et al., 2020; Gonen and Goldberg, 2019; He et al., 2021). These approaches mostly attempt to get rid of the prejudiced information from the model's internal representations or the input itself, not considering the task performance in the process.","Eliminating biases from text authored by humans is an important scientific and community dilemma that latest research has investigated (Zhang et al., 2018; Jentzsch et al., 2019; Badjatiya et al., 2019; Heindorf et al., 2019; Ravfogel et al., 2020; Gonen and Goldberg, 2019; He et al., 2021). These techniques largely try to remove the biased data from the model's internal depictions or the input itself, overlooking the task results during the process.  ","Taking away prejudices from writing done by people is a crucial scientific and public issue that current studies have examined (Zhang et al., 2018; Jentzsch et al., 2019; Badjatiya et al., 2019; Heindorf et al., 2019; Ravfogel et al., 2020; Gonen and Goldberg, 2019; He et al., 2021). These approaches primarily attempt to delete the prejudiced information from the model's internal representations or the input itself, not taking into account the task performance during the process.",A,0
259,DisCo Distilled Student Models Co-training for Semi-supervised Text Mining,"In DisCo, two kinds of compressed students (represented by two different colours in Figure 1(a)) are generated by the same teacher. This process allows us to pre-encode the model view specifically for DisCo. Additionally, we duplicate copies of a single student model to receive supervised and unsupervised data individually. In the supervised learning phase, DisCo optimizes two students using labelled samples. In the unsupervised learning phase, each student model concurrently shares the parameters with its corresponding duplicate, which is trained by supervised learning. The subsequent consistency training loss then optimizes the students using unlabeled samples.","In DisCo, two types of condensed students (shown by two separate colors in Figure 1(a)) are produced by the same instructor. This enables pre-encoding the model perspective explicitly for DisCo. Also, we replicate copies of a single student model to get supervised and unsupervised information separately. In the supervised learning stage, DisCo enhances two students using labeled examples. In the unsupervised learning stage, each student model concurrently shares the parameters with its matching duplicate, which is educated by supervised learning. The following consistency training loss then improves the students using unlabeled examples.","In DisCo, two varieties of compacted students (depicted by two distinct colors in Figure 1(a)) are generated by the same educator. This allows pre-configuring the model viewpoint specifically for DisCo. In addition, we duplicate instances of a single student model to obtain supervised and unsupervised data individually. During the supervised learning phase, DisCo optimizes two students utilizing labelled samples. In the unsupervised learning phase, each student model simultaneously shares the parameters with its corresponding copy, which is trained by supervised learning. The subsequent consistency training loss then refines the students utilizing unlabeled samples.  ","In DisCo, two forms of condensed students (represented by two separate colors in Figure 1(a)) are created by the same mentor. This enables pre-setting the model perspective exclusively for DisCo. Furthermore, we replicate versions of a single student model to acquire supervised and unsupervised information separately. In the supervised learning step, DisCo improves two students employing labeled examples. In the unsupervised learning step, each student model concurrently shares the parameters with its matching duplicate, which is coached by supervised learning. The following consistency training loss then enhances the students employing unlabeled examples.",A,0
158,Copyright Violations and Large Language Models,"We present experiments with a range of language models over a collection of popular books and coding problems, providing a conservative characterization of the extent to which language models can redistribute these materials. Overall, this research highlights the need for further examination and the potential impact on future developments in natural language processing to ensure adherence to copyright regulations.","We show tests done with various language models on many well-known books and coding challenges, giving a careful description of how much these language models can reuse these materials. In general, this research emphasizes the need for more inspection and the possible effects on future progress in natural language processing to guarantee compliance with copyright laws.","We provide studies using a variety of language models on a set of widely read books and programming problems, offering a cautious account of the degree to which language models can rework these materials. On the whole, this research underscores the necessity for additional review and the potential consequences on future advancements in natural language processing to ensure conformity with copyright rules. ","We demonstrate experiments utilizing an assortment of language models on a gathering of prevalent books and coding difficulties, giving a moderate portrayal of the degree to which language models can redistribute these materials. By and large, this exploration features the need for further assessment and the potential effect on future advancements in normal language handling to guarantee adherence to copyright guidelines.",A,0
69,ALDi Quantifying the Arabic Level of Dialectness of Text,"Earlier initiatives recognized the presence of such a spectrum (Habash et al., 2008; Zaidan and Callison-Burch, 2011), however the datasets that were developed are either skewed toward more standardized documents with limited code-switching or lack information about the distribution and the quality of the levels of dialectness labels. Consequently, the Level of Dialectness has not yet been adopted as a linguistic variable that is formally recognized in analyzing Arabic text, despite being potentially useful for NLP applications.","Past efforts were aware that this range exists (Habash et al., 2008; Zaidan and Callison-Burch, 2011), but the data created is either focused too much on more standardized texts with minimal language mixing or does not have details about the distribution and quality of the dialectness level labels. Because of this, the Degree of Dialect has not been accepted as an official linguistic feature for examining Arabic writing, even though it could be helpful for NLP systems.","Earlier work noted the presence of this spectrum (Habash et al., 2008; Zaidan and Callison-Burch, 2011). However, the datasets developed either emphasize more standardized documents with limited code-switching or lack information on the distribution and quality of dialectness level tags. As a result, Level of Dialectness has not been adopted as a formal linguistic variable for analyzing Arabic text, despite its potential utility for NLP. ","Previous efforts acknowledged this range exists (Habash et al., 2008; Zaidan and Callison-Burch, 2011). But the data produced either focuses too narrowly on standardized texts with minimal language mixing or lacks details on the distribution and quality of dialectness level labels. Consequently, Degree of Dialect has not been accepted as an official linguistic feature for studying Arabic writing, even though it could benefit NLP.",A,0
676,Standardizing Distress Analysis,"When compared to human performance, the present stateof- the-art models perform poorly, which serves to emphasize the difficulty of the task at hand.  We believe our work will advance multimodal reasoning and comprehension while also assisting in the resolution of a significant real-world problem.  Limitations and Future Scope Due to the low prevalence of hate speech on social media (approximately 3% of messages are hateful), (Fortuna and Nunes, 2018)), we scrape posts by searching for hate words to increase the likelihood of encountering hate-offensive content. ","The current top models are not as good as humans at this task, which highlights how hard it is. We think our work will improve understanding across different modes and help fix a major real world issue. Restrictions and Next Steps Since hate speech is rare on social media (around 3% of posts are hateful, (Fortuna and Nunes, 2018)), we look for hate words to find more offensive content.","Existing best models are worse than people, showing the challenge. Our research can enhance comprehension of multiple types of data and address a big problem today. Limits and Future Directions Hate speech is uncommon online (about 3% of posts, (Fortuna and Nunes, 2018)), so we search for hate terms to get more offensive stuff.  ","Today's leading models underperform compared to humans, underscoring the difficulty. Our efforts can further multimodal reasoning and understanding while tackling a substantial real-world dilemma. Constraints and Next Areas Because hate speech is scarce on social media (roughly 3% of messages, (Fortuna and Nunes, 2018)), we look for hate language to find more hateful content.",A,0
131,BOOOOKSCORE,"Widespread adoption of LLMs outside the research community has driven the development of a more ambitious task: summarizing book-length documents, which we define to be texts longer than 100K tokens. As these documents exceed the context window limits of today’s LLMs (e.g., 8K tokens for GPT-4), summarizing them via prompt-based approaches necessitates heuristics to chunk the input, process each chunk, and then combine and compress the outputs (Wu et al., 2021).","The use of large language models beyond research has led to trying a more ambitious goal: summarizing texts longer than 100,000 words, which we consider book-length. These book-length texts are too long for current LLMs to process all at once (for example, GPT-4 can only see 8,000 words at a time), so summarizing them with prompts requires splitting the input into chunks, summarizing each chunk, and then combining and shortening the outputs (Wu et al., 2021).","Widespread adoption of large language models outside of research has driven attempts at a more ambitious task: summarizing documents longer than 100,000 tokens, which we define as book-length texts. These book-length documents go beyond the context limits of today's LLMs (for instance, GPT-4 can only see 8,000 tokens at once), so summarizing them using prompts necessitates splitting the input into chunks, processing each chunk, and then joining and compressing the outputs (Wu et al., 2021). ","The use of large language models beyond research settings has led to efforts on a more ambitious goal: summarizing texts longer than 100,000 words, which we consider book-length. These book-length texts exceed the context limits of current LLMs (for example, GPT-4 can only process 8,000 words at a time), so summarizing them using prompts requires splitting the input into pieces, summarizing each piece, and then combining and shortening the outputs (Wu et al., 2021).",A,0
44,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"To evaluate the proposed method, we conduct the experiments on the SNIPS dataset for zero-shot settings (Coucke et al., 2018), which contains 39 slot types across seven different domains: AddToPlaylist (ATP), BookRestaurant (BR), GetWeather (GW), PlayMusic (PM), RateBook (RB), SearchCreativeWork (SCW) and SearchScreeningEvent (SSE). Following previous studies (Liu et al., 2020b; Siddique et al., 2021), we choose one of these domains as the target domain never used for training, and the remaining six domains are combined to form the source domain. Then, we split 500 samples in the target domain as the development set and the remainder are used for the test set.","To assess the suggested approach, we run tests on the SNIPS collection for zero-shot configurations (Coucke et al., 2018), which has 39 slot kinds across seven distinct areas: AddToPlaylist (ATP), BookRestaurant (BR), GetWeather (GW), PlayMusic (PM), RateBook (RB), SearchCreativeWork (SCW) and SearchScreeningEvent (SSE). In line with prior research (Liu et al., 2020b; Siddique et al., 2021), we select one of these domains as the target domain never utilized for instruction, and the rest of the six domains are combined to form the source domain. After that, we divide 500 examples in the target domain as the development set and the remainder are employed for the test set.","To appraise the proposed technique, we conduct experiments on the SNIPS data set for zero-shot configurations (Coucke et al., 2018), which encompasses 39 slot types across seven unique domains: AddToPlaylist (ATP), BookRestaurant (BR), GetWeather (GW), PlayMusic (PM), RateBook (RB), SearchCreativeWork (SCW) and SearchScreeningEvent (SSE). Per earlier studies (Liu et al., 2020b; Siddique et al., 2021), we choose one of these domains as the target domain never leveraged for training, and the other six domains are aggregated to constitute the source domain. Subsequently, we split 500 samples in the target domain as the development set and the remainder are utilized for the test set.  ","To evaluate the suggested method, we perform tests on the SNIPS dataset for zero-shot settings (Coucke et al., 2018), containing 39 slot categories across seven distinct areas: AddToPlaylist (ATP), BookRestaurant (BR), GetWeather (GW), PlayMusic (PM), RateBook (RB), SearchCreativeWork (SCW) and SearchScreeningEvent (SSE). As per prior work (Liu et al., 2020b; Siddique et al., 2021), we select one of these domains as the target domain never used for learning, and the other six domains are combined to form the source domain. We then divide 500 examples in the target domain into a development set, using the remainder for the test set.",A,0
70,ALDi Quantifying the Arabic Level of Dialectness of Text,"While Arabs can understand and read this standard language, spontaneously speaking in the standard language is not a natural task for most of them. Variants of DA are generally used in everyday communications, especially in spontaneous situations, and are widely used on social media platforms. DA variants can be grouped on the level of regions (5 major variants: Nile Basin, Gulf, Levant, Maghreb, and Gulf of Aden), countries (more than 20 variants), or cities (100+ variants) (Baimukan et al., 2022). In text, MSA differs from DA in terms of morphemes, syntax, and orthography. These differences form cues of dialectness in code-switched text. In the orthography, distinctive DA terms are written in ways that match the pronunciation.","Although Arabs are able to comprehend and read the standard Arabic language, speaking spontaneously in standard Arabic does not come naturally for most of them. Forms of Dialectal Arabic are generally used in everyday communication, especially in unplanned situations, and are prevalent on social media platforms. Dialectal Arabic variants can be categorized by regions (5 major variants: Nile Valley, Gulf, Levant, Maghreb, and Gulf of Aden), countries (over 20 variants), or cities (100+ variants) (Baimukan et al., 2022). In writing, Modern Standard Arabic differs from Dialectal Arabic in morphemes, syntax, and spelling. These differences create cues of dialectalness in code-switched text. In the spelling, distinctive Dialectal Arabic words are written to match the pronunciation.","While Arabs can grasp and peruse the standard Arabic tongue, extemporaneously talking in the standard Arabic is not a natural errand for a large portion of them. Varieties of Dialectal Arabic are for the most part utilized in regular communications, particularly in unconstrained circumstances, and are broadly utilized via web-based entertainment stages. Dialectal Arabic varieties can be assembled based on districts (5 significant varieties: Nile Valley, Gulf, Levant, Maghreb, and Gulf of Aden), nations (north of 20 varieties), or urban communities (100+ varieties) (Baimukan et al., 2022). In content, Modern Standard Arabic contrasts from Dialectal Arabic as far as morphemes, syntax, and spelling. These distinctions structure signals of dialectalness in code-exchanged content. In the spelling, particular Dialectal Arabic terms are composed such that match the elocution.","Although Arabs can grasp and peruse the standard Arabic language, talking aimlessly in the standard Arabic isn't a characteristic assignment for a large portion of them. Varieties of Colloquial Arabic are generally utilized in regular correspondence, particularly in unconstrained circumstances, and are broadly utilized via online entertainment stages. Colloquial Arabic varieties can be assembled based on districts (5 significant varieties: Nile Basin, Gulf, Levant, Maghreb, and Gulf of Aden), nations (north of 20 varieties), or urban communities (100+ varieties) (Baimukan et al., 2022). In text, Modern Standard Arabic contrasts from Colloquial Arabic as far as morphemes, syntax, and spelling. These distinctions structure signs of colloquialness in code-exchanged text. In the spelling, unmistakable Colloquial Arabic terms are composed such that match the elocution.",A,0
352,Improved Techniques for Training Consistency Models,"As revealed in Fig. 3a, the sample quality of consistency models improves predictably as N increases. Importantly, FID scores relative to N adhere to a precise power law until reaching saturation, after which further increases in N yield diminishing benefits. As noted by Song et al. (2023), while larger N can reduce bias in CT, they might increase variance. On the contrary, smaller N reduces variance at the cost of higher bias. Based on Fig. 3a, we cap N at 1281 in Npkq, which we empirically find to strike a good balance between bias and variance. In our experiments, we set s0 and s1 in discretization curriculums from their default values of 2 and 150 in Song et al. (2023) to 10 and 1280 respectively.","The results shown in Fig. 3a demonstrate that the consistency of the models gets better in a predictable way as N becomes larger. Notably, the FID scores compared to N follow a precise power law until reaching a plateau, after which further increasing N only provides diminishing improvements. As noted by Song et al. (2023), although larger N values can reduce bias in CT, they may also increase variance. In contrast, smaller N lowers variance but increases bias. Based on Fig. 3a, we limit N to 1281 in Npkq, which we empirically determined strikes a good balance between bias and variance. In our experiments, we set s0 and s1 in the discretization curriculums to 10 and 1280 instead of their default values of 2 and 150 used by Song et al. (2023).","The data presented in Fig. 3a shows that as N grows, the consistency of the models improves in a predictable way. Critically, the FID scores relative to N adhere closely to a precise power law until saturation is reached. After saturation, further increasing N only provides diminishing improvements. Song et al. (2023) noted that while larger N values can decrease bias in CT, they may also boost variance. In contrast, smaller N lowers variance but boosts bias. According to Fig. 3a, we constrain N to 1281 in Npkq, which we empirically found strikes a good balance between bias and variance. In our experiments, we set s0 and s1 in the discretization curriculums to 10 and 1280 rather than their defaults of 2 and 150 used in Song et al. (2023).","The data in Fig. 3a demonstrates that as N is increased, the consistency of the models improves in a predictable fashion. Importantly, the FID scores relative to N closely follow a precise power law until reaching a plateau. After the plateau, further increasing N only provides diminishing gains. As Song et al. (2023) noted, while larger N values can reduce bias in CT, they may also amplify variance. Conversely, smaller N decreases variance but amplifies bias. Based on Fig. 3a, we limit N to 1281 in Npkq, which we empirically determined strikes a good balance between bias and variance. In our experiments, we set s0 and s1 in the discretization curriculums to 10 and 1280 rather than their defaults of 2 and 150 used by Song et al. (2023).",A,0
342,Improved Techniques for Training Consistency Models,"Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model.","Coherence models are an emerging group of generative models that can generate high quality data in a single step without requiring adversarial learning. Current coherence models reach ideal sample quality by transferring knowledge from pre-trained diffusion models and using learned metrics like LPIPS. However, transferring knowledge restricts the quality of coherence models to that of the pre-trained diffusion model, and LPIPS introduces unwanted bias in assessment. To address these issues, we put forth enhanced techniques for coherence training, where coherence models learn straight from information without transferring knowledge. We analyze the theory underlying coherence training and pinpoint a previously missed flaw, which we fix by removing Exponential Moving Average from the teacher coherence model.","Consistency prototypes are a new set of generative prototypes that can produce high-grade data in one cycle without needing combative preparation. Present consistency prototypes accomplish optimum specimen quality by assimilating from pre-trained diffusion prototypes and exercising learned gauges like LPIPS. However, assimilation confines the quality of consistency prototypes to that of the pre-trained diffusion prototype, and LPIPS prompts undesirable inclination in appraisal. To address these challenges, we present improved techniques for consistency preparation, where consistency prototypes learn directly from data without assimilation. We delve into the hypothesis behind consistency preparation and identify a previously overlooked defect, which we address by eliminating Exponential Moving Average from the teacher consistency prototype.","Uniformity examples are an emerging collection of generative examples that can sample superior data in one stride without requiring adversarial tutoring. Current uniformity examples achieve best specimen quality by absorbing from pre-trained diffusion examples and applying learned measures like LPIPS. However, absorption limits the quality of uniformity examples to that of the pre-trained diffusion example, and LPIPS produces undesirable prejudice in evaluation. To tackle these difficulties, we present enhanced techniques for uniformity tutoring, where uniformity examples learn straight from data without absorption. We probe the hypothesis behind uniformity tutoring and pinpoint a previously missed flaw, which we address by removing Exponential Moving Average from the teacher uniformity example.",A,0
302,"Explain, Edit, Generate","Table 3 presents an example of the original instance and the counterfactual claims generated by different methods. The words that differ from the original claim are highlighted. It can be observed that RACE generates a linguistically diverse and fluent counterfactual claim, and the original label is successfully flipped. Obviously, the counterfactual claim generated by RACE can be combined with the original evidence to form a valid multi-hop fact verification instance, which is logical and can be verified according to the given evidence.","The table provides an illustration of the first case and the contrary assertions created through various techniques. The terms that diverge from the initial assertion are emphasized. It is evident that RACE produces a linguistically diverse and eloquent contrary assertion, and the original tag is fruitfully inverted. Clearly, the contrary assertion formed by RACE can be paired with the original proof to constitute a logical multi-step fact verification example, which is rational and can be corroborated based on the provided evidence.","The table demonstrates an instance of the original example and the contrasting claims produced by different approaches. The words that are distinct from the original claim are highlighted. One can notice that RACE generates a linguistically varied and fluent contrasting claim, and the original label is successfully changed. Undoubtedly, the contrasting claim created by RACE can be combined with the original evidence to make a valid multi-step fact verification case, which is logical and can be verified per the given evidence.  ","The table shows a case of the first instance and the opposing claims created by various methods. The terms that differ from the initial claim are accentuated. It is visible that RACE produces a linguistically diverse and articulate opposing claim, and the original tag is successfully reversed. Clearly, the opposing claim formed by RACE can be joined with the original proof to make a valid multi-step fact checking example, which is rational and can be corroborated as per the provided evidence.",A,0
516,Neural Fine-Tuning Search for Few-Shot Learning,"The results on Meta-Dataset are shown in Table 2 and Table 3 for the single domain and multi-domain training settings respectively. We can see that NFTS obtains the best average performance across all the competitor methods for both ResNet and ViT architectures. The margins over prior state-of-the-art are often substantial for this benchmark with +1.9% over TSA in ResNet18 single domain, +2.3% in multi-domain and +1.6% over ETT in VIT-small single domain. The increased margin in the multi-domain case is intuitive, as our framework has more data with which to learn the optimal path(s).","The findings from Meta-Dataset are displayed in Table 2 and Table 3 for the individual area and multi-area preparation settings respectively. It is evident that NFTS accomplishes the most remarkable typical execution over all the contender techniques for both ResNet and ViT designs. The edges over past best in class are frequently generous for this benchmark with +1.9% over TSA in ResNet18 single area, +2.3% in multi-area and +1.6% over ETT in VIT-small single area. The expanded edge in the multi-area case is natural, as our structure has more information with which to learn the ideal way(s).","The outcomes on Meta-Dataset are exhibited in Table 2 and Table 3 separately for the single space and multi-space preparing arrangements. We can perceive that NFTS acquires the best normal presentation over all the rival strategies for both ResNet and ViT designs. The edges over past cutting edge are frequently critical for this benchmark with +1.9% over TSA in ResNet18 single space, +2.3% in multi-space and +1.6% over ETT in VIT-small single space. The expanded edge in the multi-space case is instinctive, as our system has more information to learn the ideal way(s).","The consequences for Meta-Dataset are shown in Table 2 and Table 3 for the solitary area and multi-area preparing settings separately. It tends to be seen that NFTS accomplishes the best normal execution over all the contender techniques for both ResNet and ViT designs. The edges over past best in class are frequently generous for this benchmark with +1.9% over TSA in ResNet18 single area, +2.3% in multi-area and +1.6% over ETT in VIT-small single area. The expanded edge in the multi-area case is natural, as our structure has more information to learn the ideal way(s).",A,0
135,BOOOOKSCORE,"Since our human evaluation is expensive, we follow recent work by developing an LLM-based evaluation metric called BOOOOKSCORE that identifies and explains instances of any of our eight established coherence errors in a given summary. Human validation shows that BOOOOKSCORE’s annotations are almost as reliable as those of human annotators, which allows us to automatically evaluate many other book-length summarization configurations. Because BOOOOKSCORE does not rely on gold summaries, it can easily be used to evaluate new LLM summarizers on any collection of newly-published books, ensuring that the metric will remain meaningful for LLMs of the future.","We created an AI evaluation system named BOOOOKSCORE to detect and clarify occurrences of any of the 8 coherence mistakes we defined earlier in summaries. Testing showed BOOOOKSCORE's notes are nearly as accurate as human reviewers' notes, so we can use it to automatically assess many other ways to summarize book-length texts. Since BOOOOKSCORE does not need ideal summaries, we can easily apply it to rate new AI summarizers on any new books, guaranteeing the system will be useful for future AI models.","As human analysis of our work is costly, we made an AI tool called BOOOOKSCORE that spots and explains any cases of the 8 coherence issues we identified before in a summary. Human checking revealed BOOOOKSCORE's comments are almost as dependable as people's comments, enabling automatic evaluation of numerous other book summarization approaches. Because BOOOOKSCORE does not rely on model summaries, it can readily judge new AI summarizers on any newly published books, ensuring the metric remains applicable for future AI.","Given the high cost of human assessment of our work, we developed an AI metric named BOOOOKSCORE that pinpoints and elucidates any instances of the 8 coherence problems we previously defined in a summary. Human validation demonstrated BOOOOKSCORE's remarks are nearly as reliable as human annotators' remarks, permitting automated evaluation of many other book summarization methods. Since BOOOOKSCORE does not depend on ideal summaries, it can easily gauge new AI summarizers on any newly issued books, guaranteeing the metric stays meaningful for future AI.",A,0
618,SOUL,"In order to assess whether a model can truly comprehend the sentiment and accurately interpret intricate emotions, it is essential to adopt a more comprehensive approach that extends beyond merely predicting the polarity of sentiment. To this end, we introduce a new sentiment analysis task, namely Sentiment and Opinion Understanding of Language (SOUL). Our inspiration comes from reading comprehension tasks, which assess human understanding of a passage by asking to judge the validity of a statement. Similarly, we adopt the form of verifying comprehension statements regarding an opinionated review text. ","To really test if a model can fully grasp the feeling and intricacies of emotion in text, we need a more complete method beyond just predicting if the text is positive or negative. We made a new sentiment analysis job called SOUL to do this. It was inspired by reading comprehension tests which see if people understand a passage by asking if a statement about it is true. Similarly, we test comprehension of opinionated reviews by asking if statements about them are correct.","In order to truly evaluate whether a model can comprehend the sentiment and accurately interpret complex emotions, it is vital to take a more holistic approach that goes beyond simply categorizing the polarity of sentiment. For this purpose, we present a new sentiment analysis task, Sentiment and Opinion Understanding of Language (SOUL). The inspiration stems from reading comprehension activities, which measure human understanding of a passage by requiring judgment on the validity of a statement. Analogously, we adopt the format of verifying comprehension statements with regards to an opinionated review text.","To really determine if a model can grasp the feeling and intricacies of emotion in text, a more comprehensive method is needed rather than just predicting if the text is positive or negative. We created a new sentiment analysis task called SOUL to do this. It was modeled after reading comprehension tests which evaluate if people understand a passage by asking if a statement about it is correct. Similarly, we test understanding of opinionated reviews by asking if statements about them are true.",A,0
165,Copyright Violations and Large Language Models,"In a later study, Carlini et al. (2023) attempt to quantify memorization using the GPT-Neo model family and find that the degree of memorization increases with model capacity, duplication of examples, and the amount of context used for prompting. Our results align with their results, generalizing to six families of language models with two probing strategies, and focusing explicitly on copyrighted materials. Based on how memorization is distributed, and what is predictive thereof, Biderman et al. (2023a) consider the problem of predicting memorization. Ozdayi et al. (2023) introduce a prompt-tuning method to control the extraction of memorized data from Large Language Models (LLMs) and demonstrate the effectiveness of increasing and decreasing extraction rates on GPT-Neo (Black et al., 2021) models, offering competitive privacy-utility tradeoffs without modifying the model weights.","In a subsequent study, Carlini and colleagues (2023) try to quantify retention using the GPT-Neo model family and find that the degree of retention grows with model capacity, duplication of instances, and the amount of context utilized for prompting. Our findings align with their findings, generalizing to six families of language models with two probing strategies, and explicitly focusing on copyrighted materials. Based on how retention is distributed, and what predicts it, Biderman and others (2023a) consider the issue of forecasting retention. Ozdayi and colleagues (2023) introduce a prompt-tuning technique to regulate the extraction of retained data from Large Language Models (LLMs) and demonstrate the efficacy of increasing and decreasing extraction rates on GPT-Neo (Black et al., 2021) models, providing competitive privacy-utility tradeoffs without altering the model weights.","A later study by Carlini and coauthors (2023) attempts to quantify memorization using the GPT-Neo model family and finds that the degree of memorization increases with model size, duplication of examples, and the amount of context used for prompting. Our results align with their results, generalizing across six families of language models using two probing strategies, and specifically focusing on copyrighted content. Based on how memorization is distributed and what predicts it, Biderman and colleagues (2023a) examine predicting memorization. Ozdayi and coauthors (2023) introduce a prompt-tuning approach to control extracting memorized data from Large Language Models (LLMs) and show effectiveness at increasing and decreasing extraction rates on GPT-Neo (Black et al., 2021) models, providing good privacy-utility tradeoffs without changing model weights.","A subsequent study by Carlini and coworkers (2023) tries to quantify retention using the GPT-Neo model family and finds the degree of retention grows with model capacity, duplication of examples, and amount of context for prompting. Our findings agree with theirs, generalizing across six language model families using two probing strategies, and focusing on copyrighted material. Based on how retention is distributed and predicted, Biderman et al. (2023a) examine predicting retention. Ozdayi et al. (2023) introduce prompt-tuning to control extracting retained data from Large Language Models (LLMs) and demonstrate increasing and decreasing extraction rates on GPT-Neo (Black et al., 2021) models, offering good privacy-utility tradeoffs without changing model weights.",A,0
164,Copyright Violations and Large Language Models,"Memorization in neural language models is not directly controlled, and as we show below, verbatim memorization – not just the capacity for verbatim memorization, but actual verbatim memorization – seems to grow near-linearly with model size. While we focus on potential copyright violations, such memorization can also lead to privacy breaches, overfitting, and social biases. Carlini et al. (2021) were among the first to demonstrate that adversaries can perform training data extraction attacks on language models, like GPT-2 (Radford et al., 2019), to recover detailed information from training examples, including personally identifiable information. They also found that larger models are more vulnerable to such attacks.","Neural language models do not have direct control over memorization. As we demonstrate below, word-for-word memorization - not just the ability for verbatim memorization, but actual verbatim memorization - appears to increase almost linearly as the model gets bigger. Although we focus on potential copyright violations, such memorization can also lead to privacy breaches, overfitting, and societal prejudices. Carlini et al. (2021) were among the first to show that adversaries can carry out training data extraction attacks on language models, like GPT-2 (Radford et al., 2019), to obtain detailed information from training examples, including personally identifiable information. They also discovered that larger models are more susceptible to such attacks.","Neural language models lack direct control of memorization. As we exhibit below, literal memorization - not just the capacity for literal memorization, but actual literal memorization - seems to grow almost linearly as the model becomes larger. While we concentrate on potential copyright infringements, such memorization can also result in privacy violations, overfitting, and social biases. Carlini et al. (2021) were among the first to demonstrate that enemies can execute training data extraction attacks on language models, like GPT-2 (Radford et al., 2019), to recover detailed information from training examples, including personally identifiable information. They also found that bigger models are more vulnerable to such attacks.  ","Neural language models do not have direct governance over memorization. As we present below, word-for-word memorization - not just the potential for verbatim memorization, but actual verbatim memorization - appears to increase almost proportionally as the model becomes larger. Although we focus on potential copyright violations, such memorization can also lead to privacy breaches, overfitting, and prejudices in society. Carlini et al. (2021) were among the first to prove that adversaries can implement training data extraction attacks on language models, like GPT-2 (Radford et al., 2019), to obtain detailed information from training examples, including personally identifiable information. They also discovered that larger models are more susceptible to such attacks.",A,0
28,A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Moreover, our approach leads to significant improvement over random sampling (see Appendix D.2 for details on significance). Overall, these findings prove that interpretability scores, here apron,prof , can serve as a reliable signal to make fairer translations. We highlight how such improvements are enabled by a simple solution that requires no fine-tuning and only four human-written examples.","Furthermore, our method results in considerable enhancement compared to arbitrary selection (see Appendix D.2 for information on meaningfulness). In summary, these discoveries demonstrate that understandability metrics, in this case apron,prof, can function as a dependable indicator to generate more impartial translations. We emphasize how such advancements are made possible by a straightforward solution that necessitates no fine-tuning and merely four human-authored samples.","In addition, our approach leads to notable betterment versus haphazard picking (refer to Appendix D.2 for specifics on significance). On the whole, these findings prove that lucidity scores, here apron,prof, are able to serve as a reliable signal to create more even-handed translations. We highlight how such refinements are enabled by a simple solution that calls for no fine-tuning and only four examples written by humans. ","Moreover, our approach results in considerable enhancement over accidental selection (see Appendix D.2 for information about importance). All in all, these discoveries show that intelligibility metrics, in this case apron,prof, can act as a dependable indicator to generate more fair translations. We point out how such improvements are made possible by a straightforward solution that needs no fine-tuning and just four samples authored by humans.",A,0
393,INTERFAIR Debiasing with Natural Language Feedback for Fair Interpretable Predictions,"In one setup, users, by interacting with test examples, further decreased bias in the explanations (5-8%) while maintaining the same prediction accuracy. In the other setup, human feedback was able to disentangle associated bias and predictive information from the input leading to superior bias mitigation and improved task performance (4-5%) simultaneously.","In one arrangement, people, by engaging with trial instances, additionally reduced prejudice in the clarifications (5-8%) while keeping up with the same expectation precision. In the other arrangement, human input had the option to separate related predisposition and prescient data from the information prompting better bias moderation and improved assignment execution (4-5%) all the while.","In one configuration, clients, by cooperating with test models, further decreased one-sidedness in the elucidations (5-8%) while keeping up with a similar expectation exactness. In the other arrangement, human criticism had the option to disentangle related predisposition and prescient data from the information prompting unrivaled predisposition relief and improved errand presentation (4-5%) simultaneously. ","In one system, end clients, by cooperating with exploratory delineations, additionally diminished inclination in the clarifications (5-8%) while keeping up with a similar expectation precision. In the other system, human input had the option to detach related predisposition and prescient information from the information prompting better inclination moderation and improved assignment execution (4-5%) simultaneously.",A,0
193,Counting the Bugs in ChatGPTs Wugs A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,"More recently, there have been a few studies examining the morphological capabilities of language models (Edmiston, 2020; Hofmann et al., 2020a), but they focus on smaller language models such as BERT (Devlin et al., 2019). By contrast, we examine ChatGPT, a model whose parameter count is three orders of magnitude larger, and we analyze its zero-, one-, and few-shot capabilities, an approach fully neglected by prior work.","In the past, some research has looked at the ability of language models to understand word structure and formation (Edmiston, 2020; Hofmann et al., 2020a). However, these studies focused on smaller models like BERT (Devlin et al., 2019). Our study is different because we look at ChatGPT, which is much bigger with 1,000 times more parameters. We also thoroughly test its capabilities with no examples, one example, and a few examples, which no previous study has done.","Recently, a couple studies have investigated the morphological skills of AI language models (Edmiston, 2020; Hofmann et al., 2020a). But they only examined smaller models such as BERT (Devlin et al., 2019). In contrast, our research analyzes ChatGPT, a far larger model with 1,000 times more parameters. Additionally, we comprehensively evaluate its abilities with zero, one, and a few demonstrations, an approach that past work has not taken.  ","In the past few years, a small number of papers have looked at the morphological abilities of language models (Edmiston, 2020; Hofmann et al., 2020a). However, they focused on smaller models like BERT (Devlin et al., 2019) rather than massive models. Our study is novel because we examine ChatGPT, which dwarfs previous models with 1,000x more parameters. Also, we thoroughly test its skills with no examples, one example, and a few examples - an analysis completely neglected by prior studies.",A,0
707,"Unnatural Error Correction, GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text","For ScrRec, RR of GPT-4 is constantly above 95% for all setups.  For ScrQA, GPT-4 also constantly performs best with very limited accuracy drop, as the difficulty of scramble types increases.   Effect of different scramble rates:  Figure 3 illustrates the relationship between the scramble rates (i.e., the percentages of randomly scrambled words in text) and the performance on ScrRec with scrambled RealtimeQA.  As the scramble rates increases, RR decreases for text-davinci- 003, Falcon-180b and Llama-2-70b.  RR of GPT- 3.5-turbo and GPT-4 does not change significantly.  GPT-4 outperforms other models by a wide margin, with higher than 95% RR for most setups (except for 20% scramble rate). Similarly, Figure 4 plots RPG against the scramble rates for different models on ScrQA with scrambled RealtimeQA. ","For ScrRec, the recall rate of GPT-4 is always over 95% across all configurations. For ScrQA, GPT-4 also consistently has the best performance with very little drop in accuracy, even as the difficulty of scramble types increases. Effect of varying scramble percentages: Figure 3 shows the relationship between the percentages of randomly scrambled words in text and the performance on ScrRec with scrambled RealtimeQA. As the scramble percentage increases, the recall rate decreases for text-davinci-003, Falcon-180b and Llama-2-70b. The recall rate of GPT-3.5-turbo and GPT-4 does not change significantly. GPT-4 outperforms the other models by a wide margin, with over 95% recall rate for most configurations (except for 20% scramble rate). Similarly, Figure 4 plots the RPG against the scramble percentages for different models on ScrQA with scrambled RealtimeQA.","For ScrRec, GPT-4's recall rate stays above 95% for all settings. For ScrQA, GPT-4 also consistently performs the best with very little drop in accuracy, even as the difficulty of scramble types increases. Effect of varying scramble frequency: Figure 3 shows the relationship between the frequency of randomly scrambled words in text and the performance on ScrRec with scrambled RealtimeQA. As the scramble frequency increases, the recall rate decreases for text-davinci-003, Falcon-180b and Llama-2-70b. The recall rate of GPT-3.5-turbo and GPT-4 does not change much. GPT-4 outperforms the other models by a wide margin, with over 95% recall rate for most settings (except for 20% scramble frequency). Similarly, Figure 4 plots the RPG against the scramble frequency for different models on ScrQA with scrambled RealtimeQA.","For ScrRec, GPT-4 maintains a recall rate above 95% across all configurations. For ScrQA, GPT-4 also consistently achieves the highest performance with minimal decrease in accuracy, even as the difficulty of scramble types rises. Effect of varying scramble proportions: Figure 3 illustrates the relationship between the proportions of randomly scrambled words in text and the performance on ScrRec with scrambled RealtimeQA. As the scramble proportion increases, the recall rate decreases for text-davinci-003, Falcon-180b and Llama-2-70b. The recall rate of GPT-3.5-turbo and GPT-4 does not change substantially. GPT-4 outperforms the other models by a wide margin, with over 95% recall rate for most configurations (except for 20% scramble proportion). Similarly, Figure 4 plots the RPG against the scramble proportions for different models on ScrQA with scrambled RealtimeQA.",A,0
512,Neural Fine-Tuning Search for Few-Shot Learning,"As explained in Section 1, our goal is to search for the best-performing one, but the main challenge is related to the fact that we do not know what data is going to be used for adaptation at test time. One extreme approach would be to search for a single solution during training and simply use it throughout the entire test, regardless of the potential domain shift. Another, would be to defer the search and perform it from scratch each time a new support set is given to us at test time. However, both have their shortcomings. As such, we propose a hybrid, where searching is split into two phases – one during training, and a subsequent one during testing.","As described in Part 1, our aim is to look for the top-performing option, but the primary difficulty is associated with not knowing the data that will be utilized for customization when we are assessing performance. One extreme tactic would be to find one solution while training and just use that for the whole test, notwithstanding any potential changes in the domain. Another approach would be to put off the search and do it from the beginning every time we get a new support set during testing. However, both have their downsides. Therefore, we suggest a combined method, where searching is divided into two stages - one during training, and a subsequent one during testing.","As explained in the first section, our objective is to find the best-performing choice, but the main obstacle is that we don't know what data will be used for adaptation when we evaluate the system. One very aggressive strategy would be to identify one solution during training then use that same one for the entire test, ignoring any possible shifts in the domain. Another strategy would be to delay the search and redo it from scratch whenever we get a new support set during testing. However, both strategies have their flaws. As a result, we propose a hybrid approach, where the search is split into two phases - one during training, and a second one during testing.","As described in Part 1, our goal is to identify the top-performing selection, but the primary challenge relates to not knowing the data that will be employed for customization when we assess performance. One extreme plan would be to pinpoint one solution during training then utilize that for the whole test, disregarding any potential domain changes. Another plan would be to put off the search and conduct it again from the start whenever we obtain a new support set during testing. However, both plans have their weaknesses. Therefore, we suggest a combined approach, where the search is separated into two stages - one during training, and a subsequent one during testing.",A,0
767,VECHR,"The indomain train/val/test of VECHR are all from the same text topic cluster of Article 3. The OOD VECHRchallenge consists of non-Article 3 cases from different topic clusters (e.g. Article 10: freedom of expression), which involves different legal concepts and language usage.4 3.2 Vulnerability Type Annotation We follow the typology and methodology presented by Heri 2021. She considered cases as “vulnerablerelated”, only when “vulnerability had effectively been employed by the Court in its reasoning”. These cases are further coded according to the trait or situation (vulnerable type) giving rise to the vulnerability. In situations where the Court considered that multiple traits contributed to the vulnerability, she coded the case once for each relevant category. ","The in-domain train/validate/test sets for VECHR all originate from the same textual topic cluster of Article 3. The out-of-domain VECHR challenge contains non-Article 3 instances from varying topic groups (for example, Article 10: freedom of speech), which entail distinct legal ideas and language usage. VECHR annotated cases as ""vulnerability-linked"" solely when ""vulnerability had been actively utilized by the Court in its rationale"". These examples were further encoded based on the characteristic or circumstance (vulnerable category) producing the vulnerability. When the Court deemed multiple attributes added to the vulnerability, she classified the case once for every suitable type.","The training, validation, and test data for VECHR inside the domain all come from the same text topic group of Article 3. The out-of-domain VECHR challenge has non-Article 3 examples from different topic clusters (like Article 10: freedom of expression), which involve separate legal concepts and language use. VECHR marked cases as ""vulnerability-related"" only if ""vulnerability had been actively used by the Court in its reasoning"". These cases were additionally coded per the trait or situation (vulnerable type) causing the vulnerability. When the Court considered multiple characteristics contributed to the vulnerability, it classified the case once for each relevant category.  ","The in-domain train, validate, and test sets for VECHR are all sourced from the same textual topic cluster around Article 3. The out-of-domain VECHR challenge contains non-Article 3 cases from varying topic groups (such as Article 10: freedom of speech), which include distinct legal principles and language usage. VECHR labeled cases as ""vulnerability-connected"" only when ""vulnerability had been actively employed by the Court in its reasoning"". These cases were further encoded by the characteristic or circumstance (vulnerable type) producing the vulnerability. When the Court deemed multiple attributes added to the vulnerability, it classified the case once per each applicable category.",A,0
136,BOOOOKSCORE,"We use BOOOOKSCORE to evaluate the impact of several critical design decisions on the coherence of generated summaries, including the choice of prompting strategy, base LLM, and chunk size, a study that altogether cost $9K in LLM API calls. Our findings include (1) hierarchical merging generally results in more coherent summaries but reduced level of detail compared to incremental updating; (2) GPT-4 and Claude 2 produce the most coherent summaries, while LLaMA 2 is substantially worse and fails to follow instructions; (3) increasing the chunk size does not improve hierarchical merging but does substantially benefit Claude 2 when using incremental updating; and (4) summary-level preference judgments are highly subjective and do not correlate with BOOOOKSCORE.","We utilize BOOOOKSCORE to assess the influence of multiple important blueprint selections on the lucidity of created synopses, encompassing the determination of prompting tactic, foundational LLM, and fragment magnitude, an inspection that altogether amounted to $9K in LLM API invocations. Our discoveries entail (1) hierarchical combining commonly culminates in more coherent summaries albeit reduced particularity vis-a-vis incremental modernizing; (2) GPT-4 and Claude 2 beget the most coherent summaries, whilst LLaMA 2 is substantially more atrocious and neglects instructions; (3) expanding the fragment size does not ameliorate hierarchical merging but does substantially profit Claude 2 when employing incremental updating; and (4) synopsis-level preference judgments are highly subjective and do not correlate with BOOOOKSCORE.","We harness BOOOOKSCORE to gauge the impact of several pivotal design choices on the cohesion of generated abstracts, including the selection of prompting methodology, underlying LLM, and chunk extent, a study that totally cost $9K in LLM API calls. Our conclusions embrace (1) hierarchical coalescing usually yields more cohesive summaries albeit decreased granularity compared to incremental revamping; (2) GPT-4 and Claude 2 spawn the most cohesive summaries, while LLaMA 2 is substantially more dismal and disregards instructions; (3) expanding the chunk extent does not enhance hierarchical merging but does substantially benefit Claude 2 when leveraging incremental revamping; and (4) summary-level preference verdicts are highly subjective and do not correlate with BOOOOKSCORE.  ","We utilize BOOOOKSCORE to evaluate the effect of multiple pivotal design decisions on the coherence of produced summaries, including the election of prompting strategy, foundational LLM, and segment size, an examination that totally amounted to $9K in LLM API requests. Our determinations entail (1) hierarchical consolidation generally results in more coherent summaries albeit reduced specificity compared to incremental renovation; (2) GPT-4 and Claude 2 beget the most coherent summaries, while LLaMA 2 is substantially more dismal and disregards directives; (3) expanding the segment size does not ameliorate hierarchical consolidation but does substantially profit Claude 2 when harnessing incremental renovation; and (4) summary-level preference judgments are highly subjective and do not correlate with BOOOOKSCORE.",A,0
572,RESEE,"We search for both visual concepts from either a very large image pool or the internet.  In detail, we construct multimodal datasets extended from Wizard of Wikipedia (WoW) (Dinan et al., 2018), a knowledge-grounded dialogue dataset, and the commonly used Daily Dialogue (DD) (Li et al., 2017).  One dialogue turn is a single exchange of conversation between two speakers (e.g., a question and an answer).  Intuitively, turn-level visual knowledge is helpful when there are more than one topic related to a dialogue session with multiple turns, and the turn-level visual knowledge should be highly relevant to the current ongoing conversation turn. ","We look for visual ideas from a huge collection of images or the web. Specifically, we build multimodal datasets that expand on Wizard of Wikipedia (WoW) (Dinan et al., 2018), a knowledge-based dialogue set, and the popular Daily Dialogue (DD) (Li et al., 2017). One dialogue turn is a single conversation exchange between two speakers (e.g., a question and an answer). Intuitively, turn-level visual information is useful when there are multiple topics related to a dialogue with multiple turns, and the turn-level visual knowledge should be highly relevant to the current conversation turn happening.","We hunt for visual concepts either from an extremely large image bank or the internet. In particular, we construct multimodal sets extended from Wizard of Wikipedia (WoW) (Dinan et al., 2018), a knowledge-anchored dialogue collection, and the commonly utilized Daily Dialogue (DD) (Li et al., 2017). One dialogue turn is a single conversation swap between two speakers (e.g., a question and an answer). Intuitively, turn-level visual knowledge is helpful when there are multiple subjects related to a dialogue with multiple turns, and the turn-level visual knowledge should be highly pertinent to the current ongoing conversation turn.","We seek visual ideas from either a very big image trove or the web. Specifically, we build multimodal collections expanded from Wizard of Wikipedia (WoW) (Dinan et al., 2018), a knowledge-based dialogue set, and the popularly used Daily Dialogue (DD) (Li et al., 2017). One dialogue turn is a single exchange of conversation between two speakers (e.g., a question and an answer). Intuitively, turn-level visual information is useful when there are multiple topics related to a dialogue with multiple turns, and the turn-level visual knowledge should be highly relevant to the current conversation turn happening.",A,0
481,Model Tells you what to Discard - Adaptive KV Cache Compression for LLMs,"Based on this intuition, we propose FastGen to accelerate the generative inference by adaptively compressing the KV cache on the fly. First, we employ an efficient profiling algorithm to recognize the structural patterns for attention modules. Under the guidance of this profiling, we then construct the KV cache for various modules adaptively. With this diagnose-before-compress approach, FastGen effectively reduces the memory footprint of KV cache while preserving the model quality.","Guided by this insight, we put forward FastGen to speed up the generative deduction through adaptively condensing the KV store in real-time. First, we utilize an effective profiling algorithm to identify the structural patterns for attention components. With the direction of this profiling, we then build the KV store for different modules adaptively. With this analyze-before-compress method, FastGen successfully decreases the memory size of KV store while keeping the model value.","Motivated by this understanding, we present FastGen to accelerate the generative conclusion by flexibly shrinking the KV database on the fly. Initially, we use an efficient profiling technique to recognize the structural forms for attention units. Under the steering of this profiling, we then construct the KV database for various units adaptively. With this diagnose-before-shrink approach, FastGen effectively lessens the memory footprint of KV database while retaining the model quality. ","Driven by this insight, we bring forth FastGen to expedite the generative inference through nimbly compacting the KV repository in real-time. Firstly, we employ an effective profiling algorithm to identify the structural patterns for attention modules. Under the guidance of this profiling, we then build the KV repository for different modules adaptively. With this analyze-before-compact approach, FastGen successfully reduces the memory size of KV repository while maintaining the model value.",A,0
140,BOOOOKSCORE,"The only existing public dataset for book-length summarization is BookSum (Kryscinski et al., 2022), which contains famous books from the Project Gutenberg public-domain repository along with reference summaries scraped from popular websites such as CliffNotes and GradeSaver. Both the source books and reference summaries are in the pretraining data of existing LLMs: Chang et al. (2023) confirm that many books in the BookSum held-out split (e.g., The Adventures of Huckleberry Finn, The Picture of Dorian Gray) are among the most-memorized books by GPT-4 and ChatGPT, and we were able to auto-complete several reference BookSum summaries by prompting GPT-4 with a short prefix of the summary.","The sole publicly available dataset for summarizing entire books is BookSum (Kryscinski et al., 2022). It has famous books from Project Gutenberg, a public domain repository, along with summaries taken from well-known websites like CliffNotes and GradeSaver. Both the source books and reference summaries were part of the pretraining data for existing large language models. Chang et al. (2023) found that many books in the BookSum held-out split (such as The Adventures of Huckleberry Finn and The Picture of Dorian Gray) are among the books best memorized by GPT-4 and ChatGPT. We were also able to autocomplete several BookSum reference summaries just by prompting GPT-4 with a short prefix of the summary.","BookSum (Kryscinski et al., 2022) is the only public dataset available for summarizing full-length books. It contains renowned books from the public domain Project Gutenberg repository and reference summaries scraped from popular sites like CliffNotes and GradeSaver. The source books and reference summaries were included in the pretraining data for current large language models. Chang et al. (2023) showed that many books in the BookSum held-out split (The Adventures of Huckleberry Finn, The Picture of Dorian Gray, etc.) are among the books best memorized by GPT-4 and ChatGPT. We found we could autocomplete several BookSum reference summaries just by giving GPT-4 a short prefix of the summary.  ","The sole existing public data set for summarizing entire books is BookSum (Kryscinski et al., 2022). It has famous books from the public domain Project Gutenberg repository along with reference summaries taken from well-known websites including CliffNotes and GradeSaver. The source books and reference summaries were part of the pretraining data for current large language models. Chang et al. (2023) demonstrated that many books in the BookSum held-out split (such as The Adventures of Huckleberry Finn and The Picture of Dorian Gray) are among the books best remembered by GPT-4 and ChatGPT. We found we could auto-complete several BookSum reference summaries simply by providing GPT-4 a short prefix of the summary.",A,0
521,Neural Fine-Tuning Search for Few-Shot Learning,"We first summarise results of the entire search space in terms of which layers are preferential to fine-tune or not, and which layers are preferential to insert adapters or not in Figure 2a. The blocks indicate layers (columns) and adapters/fine-tuning (rows), with the color indicating whether that architectural decision was positively (green) or negatively (red) correlated with validation performance. We can see that the result is complex, without a simple pattern, as assumed by existing work (Hu et al., 2022; Li et al., 2022; Xu et al., 2022). That said, our NAS does discover some interpretable trends. For example, adapters should be included at early/late ResNet-18 layers and not at layers 5-9.","We begin by outlining the findings across the full set of options considered regarding which layers are best to fine-tune or leave static, and which are best to augment with adapters versus leave alone, as shown in Figure 2a. The grid depicts layers as columns and adapters/fine-tuning as rows, with green and red indicating positive and negative correlations respectively with validation accuracy. It is evident the relationship is intricate, lacking the straightforward pattern presumed by prior work (Hu et al., 2022; Li et al., 2022; Xu et al., 2022). However, our NAS is able to uncover some understandable tendencies. For instance, adapters should be added to early and late ResNet-18 layers, but avoided for layers 5-9.","We first summarize the results across the entire search space in terms of which layers are better to fine-tune or freeze, and which are better to insert adapters into or leave as-is, as visualized in Figure 2a. The grid shows layers as columns and adapters/fine-tuning as rows, with green and red indicating positive and negative correlations with validation performance respectively. We see the relationship is complex, without the simple pattern assumed by previous work (Hu et al., 2022; Li et al., 2022; Xu et al., 2022). Still, our NAS is able to identify some interpretable patterns. For example, adapters should be added to early and late ResNet-18 layers, but not layers 5-9.","Initially, we review the findings for the full set of options regarding which layers are optimal to fine-tune versus freeze, and which benefit most from inserting adapters versus leaving unmodified, as depicted in Figure 2a. The matrix shows layers as columns and adapters/fine-tuning as rows, with green and red denoting positive and negative correlations with validation accuracy respectively. Evidently the relationship is intricate, lacking the straightforward pattern posited by earlier work (Hu et al., 2022; Li et al., 2022; Xu et al., 2022). However, our NAS can discern some understandable tendencies. Specifically, adapters should be incorporated into early and late ResNet-18 layers, but avoided for layers 5-9.",A,0
6,A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We hence propose a few-shot learning-based debiasing approach, in which we use interpretability scores to select the incontext exemplars. Figure 1 shows an example of the resulting approach. The solution is simple-yet-effective, leading to significantly fairer translations with as few as four human-translated exemplars. Overall, our findings prove interpretability as a valuable tool for studying and mitigating bias in language models, both as a diagnostic tool and a signal driving bias mitigation approaches. We release code and data artifacts hoping to foster future research in this direction.","Therefore, we suggest a few-shot learning method to reduce bias, where we utilize interpretability metrics to choose the in-context examples. Figure 1 illustrates this approach. The solution is straightforward but successful, resulting in much fairer translations using only four human-translated examples. In summary, our discoveries show interpretability as a useful tool for analyzing and reducing bias in language models, both for diagnosis and guiding bias reduction methods. We publish code and data to hopefully encourage more research in this area.","As a result, we put forward a few-shot learning procedure for debiasing, in which interpretability rankings are leveraged to select the in-context samples. The figure depicts an instance of this technique. It is an easy yet potent approach, generating significantly more impartial translations using just four human-translated samples. On the whole, our results establish interpretability as a valuable asset for inspecting and alleviating bias in language models, serving both as an analytical instrument and a signal steering bias mitigation approaches. We make code and data available in hopes of promoting future work in this direction.  ","Consequently, we propose a few-shot learning-centric debiasing method, where we harness interpretability metrics to choose the in-context examples. The figure shows one case of this tactic. The solution is simple but mighty, producing much more unbiased translations using only four human-translated examples. In summary, our findings position interpretability as an invaluable tool for probing and reducing bias in language models, functioning as both a diagnostic lens and a guiding force for bias mitigation approaches. We release code and data in aspirations of propelling further research here.",A,0
375,INSTRUCTSCORE,"INSTRUCTSCORE assesses the quality of generated texts based on an explainable diagnostic report. Building upon this report, INSTRUCTSCORE provides an intuitive way to comprehend a model’s generation capability, resulting in easier comparison among different models. In particular, we begin by extracting concise yet representative explainable knowledge from a large-scale instruction following model, which is then utilized to train our Exp-Generator. After carefully analyzing the diagnostic reports produced by our Exp-Generator, we summarize common failure modes in diagnostic report and ask GPT-4 to identify them.","INSTRUCTSCORE judges the value of created texts using an understandable diagnostic document. Leveraging this document, INSTRUCTSCORE gives an intuitive method to grasp a model's generation talent, enabling simpler comparisons between different models. Specifically, we start by taking brief yet illustrative understandable information from a large-scale instruction obeying model, which we then use to educate our Exp-Generator. After thoroughly analyzing the diagnostic reports created by our Exp-Generator, we summarize common flawed modes in diagnostic report and request GPT-4 to pinpoint them.","INSTRUCTSCORE evaluates the excellence of produced texts based on a clear diagnostic account. Capitalizing on this account, INSTRUCTSCORE provides an instinctive way to comprehend a model's generation capability, resulting in easier contrasts between different models. In particular, we commence by deriving concise yet exemplary comprehensible knowledge from a widespread instruction following model, which is then employed to develop our Exp-Generator. After intently examining the diagnostic reports generated by our Exp-Generator, we summarize prevalent failure types in diagnostic report and ask GPT-4 to identify them. ","INSTRUCTSCORE appraises the caliber of authored texts utilizing an intelligible diagnostic chronicle. Leveraging this chronicle, INSTRUCTSCORE furnishes an intuitive fashion to grasp a model's generation faculty, enabling simpler comparisons amid distinct models. Specifically, we inaugurate by gleaning succinct yet illustrative intelligible erudition from a prevalent instruction heeding model, which is then exerted to educate our Exp-Generator. After studiously scrutinizing the diagnostic reports spawned by our Exp-Generator, we encapsulate commonplace miscarriage modes in diagnostic report and petition GPT-4 to pinpoint them.",A,0
628,SOUL,"Dataset Statistics The SOUL dataset comprises 15,028 statements related to 3,638 reviews, resulting in an average of 4.13 statements per review. To create training, development, and test sets, we split the reviews in a ratio of 6:1:3, respectively. Detailed statistics can be found in Table 1. 3 Experiments 3.1 Setup Models We benchmark SOUL with several widely used Small Language Models with the complete training set, including Roberta (Liu et al., 2019), T5 (Raffel et al., 2020), and Flan-T5 (Chung et al., 2022). We adopt the base version for each model type. In addition, we extend our analysis to two representative LLMs from the Flan and GPT model families, namely Flan-T5XXL (13B) (Raffel et al., 2020) and ChatGPT1, respectively. ","Dataset Information
The SOUL dataset has 15,028 utterances about 3,638 reviews, which is around 4.13 utterances per review on average. To make training, development, and test sets, we divided the reviews in a ratio of 6:1:3. See Table 1 for more detailed information. ","Data Breakdown  
The SOUL dataset contains 15,028 statements pertaining to 3,638 reviews, averaging to 4.13 statements for each review. We split the reviews into training, development, and test sets using a ratio of 6:1:3. Specific statistics can be found in Table 1.","Dataset Details
The SOUL dataset is comprised of 15,028 statements across 3,638 reviews, resulting in roughly 4.13 statements per review on average. We created training, development, and test sets by splitting the reviews in a 6:1:3 ratio. More precise statistics are provided in Table 1.",A,0
73,ALDi Quantifying the Arabic Level of Dialectness of Text,"Only a very few works have considered this distinction. One is Zaidan and CallisonBurch (2011), who collected sentence-level dialectness annotations in the Arabic Online Commentary data set. Although the dataset has been released, there has been no published description or analysis of these annotations that we know of, and (perhaps for this reason) no follow-up work using them3 . Our work aims to remedy this. An earlier project that annotated dialectness was Habash et al. (2008), who proposed a word-level annotation scheme consisting of four levels: (1) Pure MSA, (2) MSA with non-standard orthography, (3) MSA with dialect morphology, and (4) Dialectal lexeme.","Just a handful of studies have examined this differentiation. One such study is by Zaidan and CallisonBurch (2011), who obtained annotations at the sentence level regarding dialect usage in the Arabic Online Commentary dataset. While the dataset is available, to our knowledge there has been no published report or examination of these annotations, and (possibly due to this) no subsequent work leveraging them. Our research aims to address this gap. An earlier project that annotated dialect usage was by Habash et al. (2008), who proposed a word-level annotation system with four levels: (1) Pure MSA, (2) MSA with nonstandard spelling, (3) MSA with dialect morphology, and (4) Dialectal word.","Only a few analyses have considered this distinction. Zaidan and CallisonBurch (2011) is one example, collecting annotations about dialectness on a per-sentence basis in the Arabic Online Commentary dataset. Though the dataset is out there, we're not aware of any published description or review of these annotations, and (perhaps because of this) no follow-on work using them. Our study seeks to remedy this situation. An earlier effort that annotated dialectness was Habash et al. (2008), proposing a word-level annotation scheme with four tiers: (1) Pure MSA, (2) MSA with nonstandard orthography, (3) MSA with dialect morphology, and (4) Dialectal lexeme.","A very small number of studies have examined this differentiation. One is the work by Zaidan and CallisonBurch (2011), who obtained sentence-level annotations of dialect usage in the Arabic Online Commentary dataset. While the dataset is available publicly, there has been no published analysis or review of these annotations that we know of, and (possibly for this reason) no subsequent work leveraging them. Our research aims to address this gap. An earlier project annotating dialect usage was by Habash et al. (2008), who proposed a word-level annotation system with four levels: (1) Pure MSA, (2) MSA with nonstandard spelling, (3) MSA with dialect morphology, and (4) Dialectal word.",A,0
329,Fifty Shades of Bias,"Thus far, the NLP community has leveraged the BWS annotation framework for various tasks. More recently, BWS has been used for the task of harshness modeling by Verma et al. (2022), determining degrees of offensiveness by Hada et al. (2021), and quantifying intimacy in language by Pei and Jurgens (2020). In the past, BWS has been used for tasks such as relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018). Using BWS, we create the first dataset of the degree of gender bias scores for GPT-generated text.","So far, the NLP community has utilized the BWS tagging system for various jobs. Most recently, BWS has been leveraged for the task of harshness modeling by Verma et al. (2022), determining levels of offensiveness by Hada et al. (2021), and quantifying intimacy in language by Pei and Jurgens (2020). Previously, BWS has been leveraged for tasks like relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018). Using BWS, we generate the first dataset of the degree of gender bias scores for GPT-produced text.","Up until now, the NLP community has made use of the BWS labeling framework for various jobs. Most recently, BWS has been applied to the task of harshness modeling by Verma et al. (2022), determining levels of offensiveness by Hada et al. (2021), and quantifying intimacy in language by Pei and Jurgens (2020). In the past, BWS has been applied to tasks such as relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018). Employing BWS, we construct the first dataset of the degree of gender bias scores for GPT-generated text.  ","Up to this point, the NLP community has made use of the BWS annotation system for various tasks. Most recently, BWS has been deployed for the task of harshness modeling by Verma et al. (2022), determining levels of offensiveness by Hada et al. (2021), and quantifying intimacy in language by Pei and Jurgens (2020). Previously, BWS has been deployed for tasks such as relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018). Utilizing BWS, we assemble the first dataset of the degree of gender bias scores for GPT-generated text.",A,0
254,DisCo Distilled Student Models Co-training for Semi-supervised Text Mining,"We present DisCo, a novel co-training approach aimed at enhancing the SSL performances by using distilled small models and few labelled data. The student models in the DisCo acquire complementary information from multiple views, thereby improving the generalization ability despite the small model size and limited labelled samples. we introduce two types of view diversities for co-training: i) model view diversity, which leverages diversified initializations for student models in the cohort, ii) data view diversity, which incorporates varied noisy samples for student models in the cohort.","We introduce DisCo, a new co-training method intended to boost SSL results by utilizing distilled small models and a small amount of labeled data. The student models in DisCo obtain complementary information from multiple perspectives, thereby enhancing generalizability despite the limited model size and labeled data. We present two kinds of view diversity for co-training: i) model view diversity, which uses varied initializations for the student models in the cohort, ii) data view diversity, which includes diverse noisy examples for the student models in the cohort.","We put forward DisCo, an original co-training technique focused on improving SSL performance through the use of distilled compact models and scarce labeled data. The student models in DisCo gain complementary knowledge from multiple viewpoints, thus increasing generalizability despite the constrained model scale and labeled samples. We bring in two forms of view diversity for co-training: i) model view diversity, which harnesses varied initializations for the student models in the group, ii) data view diversity, which incorporates assorted noisy instances for the student models in the group.  ","We introduce DisCo, an innovative co-training approach targeting enhanced SSL results utilizing distilled small models and minimal labeled data. The student models in DisCo obtain complementary information from multiple perspectives, thereby boosting generalizability despite the limited model size and labeled data. We present two varieties of view diversity for co-training: i) model view diversity, which leverages diverse initializations for the student models in the cohort, ii) data view diversity, which includes varied noisy examples for the student models in the cohort.",A,0
145,BOOOOKSCORE,"Given a span from a summary marked as containing an error, along with questions highlighting the confusion, we ask annotators (1) whether they think the span is confusing; and (2) whether the corresponding questions highlight the central confusion. We use the same four annotators hired before for this task, but make them validate human and (and later GPT-4) annotations for 25 books that they did not annotate in the first task. Overall, we validated 1,659 annotations for a total cost of $418.90, and we discover that 79.7% of annotated spans are validated as legitimate through this task. More details on our validation can be found in Appendix J","We provided parts of summaries that were labeled as confusing, as well as questions pointing out the confusion, to annotators. We asked them (1) if they thought the part was perplexing; and (2) if the matching questions emphasized the main puzzle. We utilized the same four annotators employed previously for this job, but had them check human and (later GPT-4) annotations for 25 books they did not annotate originally. In total, we confirmed 1,659 annotations for a total price of $418.90, and we found that 79.7% of the annotated sections were validated as legitimate through this task. More information on our confirmation can be found in Appendix J.","We gave annotators segments from summaries that were marked as having an error, along with questions highlighting the misunderstanding. We asked them (1) if they thought the segment was baffling; and (2) if the related questions underlined the key bewilderment. We used the same four annotators contracted earlier for this work, but had them validate human and (subsequently GPT-4) annotations for 25 books they did not annotate at first. Altogether, we verified 1,659 annotations for a total expenditure of $418.90, and we discovered that 79.7% of the annotated portions were validated as genuine through this job. More details on our verification can be found in Appendix J.  ","We provided annotators sections from summaries that were labeled as problematic, along with inquiries emphasizing the confusion. We queried them (1) if they considered the portion perplexing; and (2) if the associated questions stressed the principal puzzle. We utilized the same four annotators engaged previously for this task, but had them authenticate human and (afterwards GPT-4) annotations for 25 books they did not annotate initially. In total, we corroborated 1,659 annotations for a total outlay of $418.90, and we ascertained that 79.7% of the annotated segments were validated as legitimate through this work. More information on our confirmation can be found in Appendix J.",A,0
507,Neural Fine-Tuning Search for Few-Shot Learning,"In this cross-domain problem variant, customising the feature extractor for novel domains is important, and several studies address this through dynamic feature extractors (Bateni et al., 2020; Requeima et al., 2019) or ensembles of features (Dvornik et al., 2020a; Li et al., 2021; Liu et al., 2021a). Another group of studies employ heuristically motivated fine-tuning strategies for adaptation (Dhillon et al., 2020; Hu et al., 2022; Li et al., 2022; Xu et al., 2022). Thus, an important question that arises from previous work is: How can one design the optimal adaptation strategy? In this paper, we take a step towards answering this question.","In this issue spanning multiple areas, adjusting the characteristic extractor for new domains is crucial, and several studies tackle this through flexible characteristic extractors (Bateni et al., 2020; Requeima et al., 2019) or collections of attributes (Dvornik et al., 2020a; Li et al., 2021; Liu et al., 2021a). Another set of studies utilize heuristically driven fine-tuning tactics for acclimation (Dhillon et al., 2020; Hu et al., 2022; Li et al., 2022; Xu et al., 2022). Therefore, an important inquiry that emerges from preceding work is: How can one formulate the best adaptation approach? In this paper, we make a stride towards replying to this question.","In this problem extending across fields, customizing the feature extractor for unfamiliar domains is vital, and several works address this through adaptable feature extractors (Bateni et al., 2020; Requeima et al., 2019) or assortments of features (Dvornik et al., 2020a; Li et al., 2021; Liu et al., 2021a). Another collection of studies use heuristically guided fine-tuning strategies for acclimatization (Dhillon et al., 2020; Hu et al., 2022; Li et al., 2022; Xu et al., 2022). Hence, an important query arising from previous work is: How can one design the optimal adaptation methodology? In this paper, we take a step toward answering this query.","In this issue traversing domains, tailoring the feature extractor for novel areas is crucial, and several efforts tackle this through flexible feature extractors (Bateni et al., 2020; Requeima et al., 2019) or assemblies of attributes (Dvornik et al., 2020a; Li et al., 2021; Liu et al., 2021a). Another set of efforts employ heuristically directed fine-tuning tactics for acclimatization (Dhillon et al., 2020; Hu et al., 2022; Li et al., 2022; Xu et al., 2022). Therefore, an important question arising from prior work is: How can one formulate the optimal adaptation strategy? In this paper, we take a step toward resolving this question.",A,0
292,"Explain, Edit, Generate","Our RACE focuses on identifying the causal features within rationales that can be perturbed. To this end, we use CURE (Si et al., 2023a), a multigranular rationale extraction method, to simultaneously extract sentence rationales Rs and token rationales Rt from the multi-hop evidence E for both SUP and REF instances. In essence, the token rationales Rt reflect the logical correlation within the evidence (blue words in Table 1) and the factual relationship between the claim and the evidence (red words in Table 1). Considering the causal relationship of the rationales to the prediction label (Wu et al., 2022), we regard the extracted rationales as the causal features that are to be further processed. The detailed algorithm can be found in Si et al. (2023a).","Our work concentrates on pinpointing the causal elements within justifications that can be altered. For this purpose, we utilize CURE (Si et al., 2023a), a multi-granular rationale extraction technique, to concurrently extract sentence rationales Rs and token rationales Rt from the multi-hop proof E for both SUP and REF examples. Fundamentally, the token rationales Rt demonstrate the logical association within the evidence (blue words in Table 1) and the factual connection between the claim and the evidence (red words in Table 1). Considering the causal linkage of the rationales to the prediction result (Wu et al., 2022), we view the extracted rationales as the causal components that require further processing. The detailed algorithm is available in Si et al. (2023a).","Our project focuses on identifying the causal facets within explanations that are mutable. To accomplish this, we harness CURE (Si et al., 2023a), a multi-grained rationale extraction approach, to simultaneously derive sentence rationales Rs and token rationales Rt from the multi-hop validation E for both SUP and REF cases. At its core, the token rationales Rt exhibit the logical coherence within the evidence (blue words in Table 1) and the factual bond between the claim and the evidence (red words in Table 1). Accounting for the causal tie of the rationales to the prediction outcome (Wu et al., 2022), we regard the extracted rationales as the causal factors necessitating additional processing. The step-by-step algorithm is present in Si et al. (2023a).  ","Our effort centers on pinpointing the causal components within explanations that are modifiable. To do so, we employ CURE (Si et al., 2023a), a multi-level rationale extraction technique, to simultaneously derive sentence rationales Rs and token rationales Rt from the multi-hop substantiation E for both SUP and REF instances. At its essence, the token rationales Rt exhibit the logical connection within the evidence (blue words in Table 1) and the factual association between the claim and the evidence (red words in Table 1). Considering the causal link of the rationales to the prediction result (Wu et al., 2022), we view the extracted rationales as the causal features necessitating further handling. The detailed procedure is available in Si et al. (2023a).",A,0
383,INSTRUCTSCORE,"Surprisingly, INSTRUCTSCORE even outperforms prior supervised learned metrics that trained over direct assessment data (DA), leading BLEURT20 in 6 out of 9 directions. Compared to GPT4 baseline, INSTRUCTSCORE outperforms GEMBA-GPT4 with 0.021 in Kendall and 0.145 in Pearson correlation. The larger gap in Pearson correlation can be explained by a large set of ties that GEMBA-GPT4 is producing. This will lead to false positive in Kendall correlation. Lastly, we demonstrate that INSTRUCTSCORE can achieve close performance to the supervised learned metrics, MATESE, COMET22 and Metric XXL, that have trained over comprehensive human rating data (DA and MQM), with average 0.012 gap in Kendall correlation and 0.045 in Pearson correlation.","Amazingly, INSTRUCTSCORE surpasses even previously trained evaluation metrics that were trained on direct human ratings. It beats BLEURT20 in 6 out of 9 cases. Compared to GPT4, INSTRUCTSCORE is better than GEMBA-GPT4 by 0.021 Kendall and 0.145 Pearson correlation. The bigger Pearson gap is because GEMBA-GPT4 has many ties, causing false positives in Kendall. Finally, INSTRUCTSCORE gets very close to supervised metrics like MATESE, COMET22, and Metric XXL that trained on extensive human scores. On average it is only 0.012 lower in Kendall and 0.045 lower in Pearson correlation.","It is remarkable that INSTRUCTSCORE even exceeds prior metrics that were supervised trained on direct assessments. It is superior to BLEURT20 in 6 of 9 directions. Versus GPT4, INSTRUCTSCORE surpasses GEMBA-GPT4 by 0.021 Kendall and 0.145 Pearson correlation. The larger Pearson gap is due to many ties from GEMBA-GPT4, causing false positives in Kendall. Lastly, INSTRUCTSCORE approaches the performance of supervised metrics like MATESE, COMET22, and Metric XXL that trained on comprehensive human evaluations. On average it is just 0.012 lower in Kendall and 0.045 lower in Pearson correlation.  ","Shockingly, INSTRUCTSCORE outdoes previous metrics that were supervised trained on direct human assessments. It tops BLEURT20 in 6 out of 9 cases. In comparison to GPT4, INSTRUCTSCORE beats GEMBA-GPT4 by 0.021 Kendall and 0.145 Pearson correlation. The bigger Pearson gap stems from numerous ties by GEMBA-GPT4, generating false positives in Kendall. Finally, INSTRUCTSCORE nears the performance of supervised metrics like MATESE, COMET22, and Metric XXL that trained on extensive human ratings. On average it is only 0.012 lower in Kendall and 0.045 lower in Pearson correlation.",A,0
132,BOOOOKSCORE,"Despite the promise that LLMs hold for long-context tasks, the research community still lacks a principled and systematic approach to evaluate their capabilities on book-length summarization. Our paper identifies three open challenges with evaluation: (1) data contamination, in which existing benchmarks such as BookSum (Kryscinski et al., 2022) are in the pretraining data of modern LLMs; (2) an unexplored error distribution, as most prior summarization research centers around short source documents and fails to capture coherence errors that are exacerbated by the “chunk and combine” book-length summarization setting; and (3) a lack of any reliable automatic metric, which requires careful design and validation against human annotations.","Although LLMs appear capable of summarizing book-length content, the research community lacks a principled and methodical way to assess their abilities on this long-context task. Our paper spotlights three unresolved issues around evaluation: (1) test data contamination, since existing benchmarks like BookSum (Kryscinski et al., 2022) are found in the pretraining data of modern LLMs; (2) an uninvestigated error distribution, because most prior summarization research revolves around short source documents and overlooks coherence errors amplified by the ""chunk and combine"" book-length summarization approach; and (3) an absence of any dependable automatic metric, necessitating careful design and validation against human annotations.","While LLMs seem promising for summarizing book-length texts, researchers still need a systematic and theoretical approach for gauging their capabilities on this long-context task. Our paper highlights three open evaluation challenges: (1) corrupted test data, with current benchmarks like BookSum (Kryscinski et al., 2022) appearing in LLMs' pretraining; (2) an uncharted error pattern, since most prior summarization work examines short texts, overlooking coherence errors magnified by ""chunk and fuse"" book summarizing; and (3) no reliable automatic metric, requiring meticulous design and human validation.  ","Although LLMs look capable of summarizing entire books, researchers lack a logical and organized way to evaluate their skills on this long-context task. Our paper underlines three unsettled evaluation issues: (1) contaminated test data, as existing benchmarks such as BookSum (Kryscinski et al., 2022) are found in modern LLMs' pretraining; (2) an unmapped error distribution, since most prior summarization research analyzes short texts, missing coherence errors enlarged by ""chunk and merge"" book summarizing; and (3) no sound automatic metric, needing careful design and human verification.",A,0
361,Improved Techniques for Training Consistency Models,"In Fig. 5, we provide additional analysis for the Pseudo-Huber metric proposed in Section 3.3. We show the shapes of squared ℓ2 metric, as well as Pseudo-Huber losses with various values of c in Fig. 5a, illustrating that Pseudo-Huber losses smoothly interpolates between the ℓ1 and squared ℓ2 metrics. In Fig. 5b, we plot the ℓ2 norms of parameter updates retrieved from the Adam optimizer for models trained with squared ℓ2 and Pseudo-Huber metrics. We observe that the Pseudo-Huber metric has lower variance compared to the squared ℓ2 metric, which is consistent with our hypothesis in Section 3.3.","Figure 5 provides supplementary review of the Pseudo-Huber measure suggested in Part 3.3. The forms of the squared l2 standard, plus Pseudo-Huber losses with multiple c values are exhibited in Fig. 5a, demonstrating that Pseudo-Huber losses seamlessly shift between the l1 and squared l2 measures. In Fig. 5b, we chart the l2 norms of parameter refreshes from the Adam enhancer for models prepared with squared l2 and Pseudo-Huber measures. We notice that the Pseudo-Huber measure has lower difference compared to the squared l2 measure, which aligns with our theory in Section 3.3.","Additional inspection of the Pseudo-Huber metric proposed in Segment 3.3 is given in Figure 5. The shapes of the squared l2 norm, and Pseudo-Huber losses with various c values are shown in Fig. 5a, revealing that Pseudo-Huber losses smoothly transition between the l1 and squared l2 norms. In Fig. 5b, we plot the l2 norms of parameter updates obtained from the Adam optimizer for models trained with squared l2 and Pseudo-Huber metrics. We see that the Pseudo-Huber metric has lower variance versus the squared l2 metric, agreeing with our hypothesis in Segment 3.3.  ","Supplementary analysis of the Pseudo-Huber metric suggested in Portion 3.3 is provided in Fig. 5. The forms of the squared l2 standard, in addition to Pseudo-Huber losses with multiple c values are displayed in Fig. 5a, exhibiting that Pseudo-Huber losses seamlessly interpolate between the l1 and squared l2 standards. In Fig. 5b, we graph the l2 norms of parameter refreshes from the Adam enhancer for models prepared with squared l2 and Pseudo-Huber standards. We discern that the Pseudo-Huber metric has lower fluctuation compared to the squared l2 metric, which concurs with our theory in Portion 3.3.",A,0
319,Fifty Shades of Bias,"To ground the generation, we use a list of carefully curated seeds. This serves two purposes: (1) we can navigate data sparsity issues while still grounding GPT generations to real-world data, and (2) we can understand the biases GPT-3.5-Turbo can propagate via its generations. Studying (2) becomes increasingly relevant given that models have been shown to represent opinionation as a by-product of being trained on poorly representative data (Santurkar et al., 2023) . This paper introduces a novel dataset consisting of 1000 GPT-generated English text annotated for its degree of gender bias. The dataset includes fine grained, real-valued scores ranging from 0 (least negatively biased) to 1 (most negatively biased) – normative gender bias ratings for the statements.","To anchor the creation, we utilize a list of carefully chosen seeds. This has two purposes: (1) we can maneuver around data scarcity problems while still anchoring GPT generations to real-world information, and (2) we can comprehend the prejudices GPT-3.5-Turbo can spread through its creations. Examining (2) becomes increasingly important given that models have been displayed to portray opinion as a byproduct of being trained on poorly representative information (Santurkar et al., 2023). This paper presents a new dataset containing 1000 GPT-created English text annotated for its level of gender bias. The dataset incorporates fine-grained, real-valued scores ranging from 0 (least negatively biased) to 1 (most negatively biased) – normative gender bias evaluations for the statements.","To establish the production, we employ a list of thoughtfully selected origins. This serves two functions: (1) we can steer around data scarcity dilemmas while still establishing GPT generations to real-world evidence, and (2) we can grasp the prejudices GPT-3.5-Turbo can spread through its productions. Investigating (2) becomes increasingly vital given that models have been exhibited to depict partiality as a side-effect of being trained on poorly illustrative data (Santurkar et al., 2023). This paper unveils a novel dataset comprising 1000 GPT-created English text annotated for its level of gender bias. The dataset encompasses fine-grained, real-valued scores ranging from 0 (least negatively biased) to 1 (most negatively biased) – normative gender bias evaluations for the statements.","To base the invention, we utilize a list of prudently chosen seeds. This serves two aims: (1) we can maneuver around data scarcity hurdles while still basing GPT inventions on real-world proof, and (2) we can grasp the prejudices GPT-3.5-Turbo can spread via its inventions. Probing (2) becomes increasingly key given that models have been exhibited to portray opinion as a byproduct of being trained on poorly exemplary data (Santurkar et al., 2023). This paper presents a novel dataset encompassing 1000 GPT-created English text annotated for its degree of gender bias. The dataset includes fine-grained, real-valued scores ranging from 0 (least negatively biased) to 1 (most negatively biased) – normative gender bias ratings for the statements.",A,0
423,Eliminating Lipschitz Singularities in Diffusion Models,"The rapid development of diffusion models has been witnessed in image synthesis (Ho et al., 2020; Song et al., 2020; Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022; Zhang & Agrawala, 2023; Hoogeboom et al., 2023) in the past few years. Concretely, diffusion models construct a multi-step process to destroy a signal by gradually adding noises to it. That way, reversing the diffusion process (i.e., denoising) at each step naturally admits a sampling capability. In essence, the sampling process involves solving a reverse-time stochastic differential equation (SDE) through integrals (Song et al., 2021b).","The quick growth of diffusion models has been observed in image generation (Ho et al., 2020; Song et al., 2020; Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022; Zhang & Agrawala, 2023; Hoogeboom et al., 2023) over the last few years. Specifically, diffusion models build a multi-step process to corrupt a signal by slowly introducing noise to it. Therefore, reversing the diffusion process (i.e., removing noise) at each step naturally allows sampling. Fundamentally, the sampling process consists of solving a reverse-time stochastic differential equation (SDE) through integrals (Song et al., 2021b).","The rapid expansion of diffusion models has been seen in image synthesis (Ho et al., 2020; Song et al., 2020; Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022; Zhang & Agrawala, 2023; Hoogeboom et al., 2023) recently. In particular, diffusion models construct a multi-phase process to degrade a signal by gradually incorporating noise into it. As a result, reversing the diffusion process (i.e., denoising) at each phase naturally provides a sampling ability. At its core, the sampling process involves solving a reverse-time stochastic differential equation (SDE) through integrals (Song et al., 2021b).  ","The fast progression of diffusion models has been noticed in image generation (Ho et al., 2020; Song et al., 2020; Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022; Zhang & Agrawala, 2023; Hoogeboom et al., 2023) in recent times. Specifically, diffusion models build a multi-step process to deteriorate a signal by slowly adding noise to it. Thus, inverting the diffusion process (i.e., removing noise) at each step naturally enables sampling. In essence, the sampling process consists of solving a reverse-time stochastic differential equation (SDE) through integrals (Song et al., 2021b).",A,0
737,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"In the next phase, we leverage f0 and Dm to derive pseudo labels and train a RoBERTa-based critic, as described in §3.3. By utilizing this critic, we segregate Dm into grammatically correct (Dgm ) and incorrect (Dug m ) subsets. We then use the BIFI mechanism to generate realistic parallel data that is then employed to train a new fixer f1. We subsequently substitute f0 with f1 and repeat this procedure until the fixer achieves satisfactory performance. Following prior work (Awasthi et al., 2019; Grundkiewicz et al., 2019), we use the combination of WMT NewsCrawl corpus (Bojar et al., 2018) and One-Billion-Word corpus (Chelba et al., 2014) as the seed monolingual corpus Dseed m . ","In the next stage, we use f0 and Dm to create pseudo labels and teach a RoBERTa-based critic, as described in section 3.3. By using this critic, we separate Dm into correct (Dgm) and incorrect (Dugm) groups. We then utilize the BIFI system to generate realistic parallel information that we then use to train a new fixer f1. We then replace f0 with f1 and repeat this process until the fixer has good performance. As in prior work (Awasthi et al., 2019; Grundkiewicz et al., 2019), we use the combination of the WMT NewsCrawl corpus (Bojar et al., 2018) and One-Billion-Word corpus (Chelba et al., 2014) as the initial monolingual corpus Dseedm.","In the following phase, we harness f0 and Dm to make pseudo labels and educate a RoBERTa-based critic, as explained in section 3.3. By employing this critic, we separate Dm into proper (Dgm) and improper (Dugm) subsets. We then utilize the BIFI process to generate realistic parallel data that we then use to train a new fixer f1. We then substitute f0 with f1 and repeat this procedure until the fixer has satisfactory performance. As in prior work (Awasthi et al., 2019; Grundkiewicz et al., 2019), we use the combination of the WMT NewsCrawl corpus (Bojar et al., 2018) and One-Billion-Word corpus (Chelba et al., 2014) as the initial monolingual corpus Dseedm.  ","In the next part, we take advantage of f0 and Dm to derive pseudo tags and educate a RoBERTa-based reviewer, as stated in section 3.3. By using this reviewer, we divide Dm into accurate (Dgm) and inaccurate (Dugm) groups. We then employ the BIFI process to generate realistic parallel information which we then use to train a new fixer f1. We then swap f0 with f1 and repeat this procedure until the fixer has good performance. As in previous work (Awasthi et al., 2019; Grundkiewicz et al., 2019), we use the combination of the WMT NewsCrawl corpus (Bojar et al., 2018) and One-Billion-Word corpus (Chelba et al., 2014) as the starting monolingual corpus Dseedm.",A,0
412,INTERFAIR Debiasing with Natural Language Feedback for Fair Interpretable Predictions,"Test-time improvement of task performance and bias with a frozen model indicates that 1) full-text-based training suffers from spurious correlation or noise that hampers task performance, and 2) interactive debiasing is superior to no feedback since it produces better quality human feedback to refine task performance while eliminating bias. This phenomenon can be seen as a proxy for data augmentation leading to a superior disentanglement of original task performance and bias. Finally, since test-time interactions modify task rationales, we check their faithfulness using comprehensiveness and sufficiency scores, measured as defined in (DeYoung et al., 2020).","The better task performance and lower bias from using a frozen model at test time shows that 1) training on full text leads to irrelevant correlations or noise that makes the task worse, and 2) getting human feedback interactively is better than no feedback, since it gives higher quality feedback to improve the task while removing bias. This can be seen as similar to data augmentation, which leads to better separation of the original task performance and bias. Lastly, since interacting at test time changes the task rationales, we evaluate their faithfulness using comprehensiveness and sufficiency scores, as defined in (DeYoung et al., 2020).","The improvements in task performance and bias reduction when using a frozen model at test time indicates that 1) training with full text introduces spurious correlations or noise that hinders task performance, and 2) interactive human feedback is superior to no feedback, as it provides better feedback to enhance task performance and eliminate bias. This is analogous to data augmentation resulting in better disentanglement of the original task performance and bias. Finally, since test time interactions alter task rationales, we assess their faithfulness using comprehensiveness and sufficiency metrics, as specified in (DeYoung et al., 2020).  ","The better task results and lower bias from utilizing a frozen model at test time shows that 1) full text training introduces irrelevant correlations or noise that impedes task performance, and 2) interactive human feedback is better than no feedback, since it furnishes superior feedback to refine task performance while removing bias. This is similar to data augmentation producing better separation of the original task performance and bias. Lastly, since interacting at test time modifies task rationales, we check their faithfulness using comprehensiveness and sufficiency measures, as defined in (DeYoung et al., 2020).",A,0
41,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Recent line of works have investigated the instance-level contrastive learning by template regularization (Shah et al., 2019; Liu et al., 2020b; He et al., 2020; Wang et al., 2021). As slots with the same types tend to have the semantically similar contexts, inspired by Das et al. (2022), we propose to use the slot-level contrastive learning to facilitate the discriminative slot representations that may contribute to adaptation robustness.","A number of recent studies have looked at contrastive learning at the instance level using template regularization (Shah et al., 2019; Liu et al., 2020b; He et al., 2020; Wang et al., 2021). Since slots of the same type often have semantically similar contexts, building on Das et al. (2022), we put forward using contrastive learning at the slot level to promote discriminative representations of slots that could add to adaptation robustness.","Several recent works have investigated contrastive learning on individual instances by template regularization (Shah et al., 2019; Liu et al., 2020b; He et al., 2020; Wang et al., 2021). Because slots of the same type tend to have contextually similar semantics, inspired by Das et al. (2022), we suggest employing contrastive learning on slots to encourage distinctive slot representations that may improve robustness to adaptation.  ","A series of recent studies have explored contrastive learning on the instance level through template regularization (Shah et al., 2019; Liu et al., 2020b; He et al., 2020; Wang et al., 2021). Since slots of the same type frequently share semantic similarities in their contexts, building on Das et al. (2022), we put forward the use of contrastive learning on slots to promote discriminative slot representations that could contribute to robustness in adaptation.",A,0
742,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"competing approaches. As demonstrated in Table 3, the erroneous sentences generated by the competing methods tend to either be grammatically correct or change the intended meaning of the original sentences. This observation explains the better performance of our method relative to these competing approaches. Notably, Sun et al. (2022) implements an approach similar to ours, which also generates replacement errors by inserting masks and then uses XLM to predict the mask. The difference is that they use translation pairs to guide the creation of candidate tokens, while our method relies on edit distance and frequency information. ","Different methods. As shown in Table 3, the incorrect sentences made by the other ways often are either grammatically right or alter what the original sentences were trying to say. This clarifies why our method did better than these other approaches. Notably, Sun et al. (2022) uses a method like ours, which also makes replacement mistakes by adding masks and then uses XLM to guess the mask. The difference is they use translation pairs to help choose possible tokens, while our method depends on edit distance and frequency data.","Alternative approaches. As exhibited in Table 3, the flawed sentences produced by the competing techniques tend to be syntactically accurate or change the intent of the initial sentences. This accounts for the superior performance of our method compared to these alternative approaches. Significantly, Sun et al. (2022) implements a technique similar to ours, which also generates substitution errors by inserting masks and then utilizes XLM to predict the mask. The distinction is that they employ translation pairs to guide the creation of candidate tokens, while our method relies on edit distance and frequency information.","Contrasting techniques. As shown in Table 3, the inaccurate sentences created by the opposing processes are often grammatically correct or alter the meaning of the original sentences. This explains the better results of our method versus these contrasting approaches. Importantly, Sun et al. (2022) uses an approach like ours, which also produces replacement mistakes by adding masks and then uses XLM to determine the mask. The difference is they utilize translation pairs to direct the selection of possible tokens, while our method depends on edit distance and frequency data.",A,0
108,Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering,"To minimize Eq.4, we need to obtain the predicted probability of the source and adapted model. However, this requires at least two forward propagation and one back propagation for each sample, which undoubtedly dramatically increases the cost of practical application. To break this dilemma, we propose an efficient side block, which is plugged into the backbone as the adapted model so that we only need one forward propagation to obtain the two outputs simultaneously.","In order to reduce Eq.4, we must find the predicted probability of both the original and adapted models. But doing so necessitates at least two forward passes and one backward pass per sample, greatly increasing the cost for real-world use. To resolve this problem, we introduce an efficient side module that connects to the backbone as the adapted model, allowing us to get both outputs in one forward pass.","To minimize Eq.4, we have to calculate the predicted probabilities from the source and adapted systems. However, that requires a minimum of two forward computations and one backward computation per example, substantially raising the cost for practical use. To break this impasse, we present a fast side unit, attached to the backbone as the adapted system, so we only need one forward pass to concurrently produce both outputs. ","In order to reduce Eq.4, the predicted likelihoods from the original and adapted models must be determined. But that entails at least two forward propagations and one backpropagation per instance, greatly increasing the expense for real applications. To break this deadlock, we put forth an efficient side block, connected to the backbone as the adapted model, allowing both outputs to be generated in one feedforward pass.",A,0
332,Fifty Shades of Bias,"To ground the generations of GPT, we first curate a list of 500 seeds. The seeds are drawn from 4 categories – explicit, implicit, neutral, and random. Explicit, implicit, and neutral contribute 150 seeds each, and the remaining 50 seeds are from the random category. We select the seeds as follows: Explicit: These are sentences that have explicit mentions of gender and stereotypical associations. We select these seeds from StereoSet (Nadeem et al., 2021). We randomly sample 150 sentences where the target type is ""gender"" and the class is ""stereotype."" The sentences are uniformly distributed between males and females.","We carefully choose 500 starting phrases to establish the lineages of GPT. The phrases are taken from 4 groups - overt, implied, unbiased, and arbitrary. Overt, implied, and unbiased each provide 150 phrases, and the remaining 50 phrases are from the random group. We pick the phrases like this: Overt: These are sentences that plainly state gender and stereotypical links. We take these seeds from StereoSet (Nadeem et al., 2021). We randomly choose 150 sentences where the goal category is ""gender"" and the class is ""stereotype."" The sentences are evenly split between males and females.","To lay the groundwork for the generations of GPT, we first assemble a list of 500 initiating statements. The statements originate from 4 categories - explicit, suggested, neutral, and random. Explicit, suggested, and neutral each furnish 150 statements, and the remaining 50 statements come from the random category. We select the statements as follows: Explicit: These are sentences that transparently mention gender and stereotypical connections. We obtain these seeds from StereoSet (Nadeem et al., 2021). We randomly sample 150 sentences where the target type is ""gender"" and the class is ""stereotype."" The sentences are evenly distributed between men and women.","In order to establish a foundation for the lineages of GPT, we first gather together 500 originating expressions. The expressions are gathered from 4 groups - unambiguous, hinted, nonpartisan, and arbitrary. Unambiguous, hinted, and nonpartisan each provide 150 expressions, while the remaining 50 expressions come from the arbitrary group. We choose the expressions in the following manner: Unambiguous: These are sentences that openly refer to gender and stereotypical links. We take these seeds from StereoSet (Nadeem et al., 2021). We randomly select 150 sentences where the focus type is ""gender"" and the class is ""stereotype."" The sentences are evenly balanced between males and females.",A,0
56,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"In recent years, zero-shot slot filling has received increasing attention. A dominating line of research is the metric-learning method, where the core idea is to learn a prototype representation for each category and classify test data based on their similarities with prototypes (Snell et al., 2017). For slot filling, the semantic embeddings of textual slot descriptions usually serve as the prototype representations (Bapna et al., 2017; Lee and Jha, 2019; Zhu et al., 2020). Shah et al. (2019) utilize both the slot description and a few examples of slot values to learn semantic representations of slots.  ","Over the past few years, zero-shot slot filling has become more popular. A major area of research is the metric-learning approach, where the main concept is to learn a prototype representation for each type and categorize test data based on how similar they are to the prototypes (Snell et al., 2017). For slot filling, the semantic embeddings of textual slot descriptions often act as the prototype representations (Bapna et al., 2017; Lee and Jha, 2019; Zhu et al., 2020). Shah et al. (2019) use both the slot description and some examples of slot values to learn semantic representations of slots.","In the last several years, zero-shot slot filling has attracted growing attention. A leading line of work is the metric-learning method, which centers on learning a prototypical representation for each class and classifying test data according to their resemblance to the prototypes (Snell et al., 2017). For slot filling, the semantic embeddings of textual slot descriptions commonly function as the prototype representations (Bapna et al., 2017; Lee and Jha, 2019; Zhu et al., 2020). Shah et al. (2019) leverage both the slot description and a few slot value examples to learn semantic representations of slots.","Recently, zero-shot slot filling has become increasingly popular. A dominant research direction is the metric-learning approach, where the core concept is learning a prototype representation for each category and categorizing test data based on similarity to the prototypes (Snell et al., 2017). For slot filling, semantic embeddings of textual slot descriptions often act as prototype representations (Bapna et al., 2017; Lee and Jha, 2019; Zhu et al., 2020). Shah et al. (2019) use both slot descriptions and some example slot values to learn semantic representations of slots.",A,0
244,Cultural Concept Adaptation on Multimodal Reasoning,"To attack the difficulties of data annotation and scarcity, we propose an annotation-free cultural adaptation method and design a novel cultural concept-based multi-modal data augmentation to generate the new data example. By training the model on the augmented dataset, key results indicate that our methods consistently and statistically outperform the baselines. In the future, we plan to apply our method to more downstream tasks related to culture. Employing curriculum learning and designing more refined training strategies according to the difficulty of different languages and cultural concepts is also worth exploring. At the same time, how to further extend our method to make it more applicable to multi-modal models based on auto regressive generation, such as GPT-4-V 6 , is also highly worthwhile to explore.","To tackle the problems of lacking labeled data and sparse annotations, we put forward a technique that does not need annotations and a new way to increase data using cultural ideas across modalities. By teaching the model with the larger dataset, key results show our approaches are consistently and statistically superior to baseline methods. Moving forward, we intend to use our technique for more downstream tasks involving culture. It is also worth investigating curriculum learning and crafting more nuanced training strategies per the difficulty of different languages and cultural concepts. Likewise, extending our technique to make it more useful for multi-modal models like GPT-4-V that generate text iteratively is highly worthwhile.","To address the challenges of limited annotated data, we present an annotation-free method of cultural adaptation and an innovative multi-modal data augmentation approach leveraging cultural concepts to synthesize new examples. Critical findings demonstrate that training on the augmented dataset leads our techniques to consistently and significantly surpass baselines. Looking ahead, we plan to apply our method to additional downstream tasks related to culture. It is also promising to explore curriculum learning and devise more refined training techniques tailored to the complexity of different languages and cultural concepts. Concurrently, broadening our method's applicability to multi-modal auto-regressive generation models such as GPT-4-V merits investigation.","To tackle the problems of scarce labeled data and annotation, we put forward an annotation-free cultural adaptation approach and design a novel data augmentation technique using cultural concepts across modalities to create new examples. Vital results show our methods consistently and statistically beat baselines when training the model on the augmented dataset. Moving forward, we intend to employ our method on further downstream tasks involving culture. It is also worthwhile to explore curriculum learning and craft more nuanced training plans based on the difficulty of different languages and cultural concepts. Similarly, extending our method's applicability to multi-modal auto-regressive models like GPT-4-V warrants investigation.",A,0
351,Improved Techniques for Training Consistency Models,"When training consistency models, we minimize the discrepancy between models evaluated at adjacent noise levels. Recall from Section 2 that the model with the lower noise level is termed the teacher network, and its counterpart the student network. While Song et al. (2023) maintains EMA parameters for both networks with potentially varying decay rates, we present a theoretical argument indicating that the EMA decay rate for the teacher network should always be zero for CT, although it can be nonzero for CD. We revisit the theoretical analysis in Song et al. (2023) to support our assertion and provide empirical evidence that omitting EMA from the teacher network in CT notably improves the sample quality of consistency models.","While teaching consistency models, we reduce the difference between models judged at nearby noise amounts. Remember from Section 2 that the model with the lower noise amount is called the teacher network, and its partner the student network. Although Song et al. (2023) keeps EMA parameters for both networks with potentially varying decay rates, we provide a theoretical contention showing that the EMA decay rate for the teacher network should always be zero for CT, despite being nonzero for CD. We re-examine the theoretical examination in Song et al. (2023) to back our claim and give empirical proof that excluding EMA from the teacher network in CT significantly improves the sample quality of consistency models.","When instructing consistency models, we minimize the incongruity between models appraised at adjoining noise magnitudes. Recall from Section 2 that the model with the lower noise magnitude is termed the teacher network, and its counterpart the student network. While Song et al. (2023) maintains EMA parameters for both networks with potentially varying decay rates, we present a theoretical argument indicating that the EMA decay rate for the teacher network should always be zero for CT, although it can be nonzero for CD. We revisit the theoretical analysis in Song et al. (2023) to support our assertion and provide empirical evidence that omitting EMA from the teacher network in CT notably improves the sample quality of consistency models.","During training of consistency models, we reduce the divergence between models evaluated at nearby noise levels. Remember from Section 2 that the model with the lower noise level is called the teacher network, and its partner the student network. Although Song et al. (2023) retains EMA parameters for both networks with potentially varying decay rates, we give a theoretical contention showing the EMA decay rate for the teacher network should always be zero for CT, despite being non-zero for CD. We re-examine the theoretical analysis in Song et al. (2023) to support our claim and provide empirical evidence that leaving out EMA from the teacher network in CT significantly improves the sample quality of consistency models.",A,0
396,INTERFAIR Debiasing with Natural Language Feedback for Fair Interpretable Predictions,"However, a user can potentially further tune the model’s belief on the bias, leading to a correct prediction while minimally using biased information. While interactive NLP models recently focused on model debugging (Tandon et al., 2021, 2022), improving explainability in QA (Li et al., 2022b), machine teaching (Dalvi et al., 2022), critiquing for personalization (Li et al., 2022a), and dialog as a more expressive form of explanations (Lakkaraju et al., 2022; Slack et al., 2022), we focus on an under-explored paradigm of model debiasing using user interactions.","Nevertheless, a person has the potential to additionally adjust the model's assumption regarding the prejudice, resulting in an accurate forecast while barely utilizing biased details. Although interactive NLP models have recently concentrated on model debugging (Tandon et al., 2021, 2022), enhancing interpretability in QA (Li et al., 2022b), machine teaching (Dalvi et al., 2022), critiquing for customization (Li et al., 2022a), and dialogue as a more expressive form of clarifications (Lakkaraju et al., 2022; Slack et al., 2022), we concentrate on an under-investigated paradigm of model debiasing utilizing user interactions.","However, an end user could possibly further tune the model's view on the bias, leading to a correct prediction while minimally using biased data. While interactive NLP models have recently focused on model debugging (Tandon et al., 2021, 2022), improving explainability in question answering (Li et al., 2022b), machine teaching (Dalvi et al., 2022), critiquing for personalization (Li et al., 2022a), and conversation as a more expressive form of explanations (Lakkaraju et al., 2022; Slack et al., 2022), we focus on an under-explored paradigm of model debiasing using user interactions.","Though, a person could potentially further adjust the model's perspective on the bias, resulting in an accurate forecast while barely leveraging biased information. Despite interactive NLP models recently concentrating on model debugging (Tandon et al., 2021, 2022), enhancing interpretability in QA (Li et al., 2022b), machine teaching (Dalvi et al., 2022), critiquing for customization (Li et al., 2022a), and dialogue as a more expressive form of clarifications (Lakkaraju et al., 2022; Slack et al., 2022), we concentrate on an under-investigated paradigm of model debiasing utilizing user interactions.",A,0
227,Cultural Concept Adaptation on Multimodal Reasoning,"While code-switching operates on sentences, mixup methods are utilized in a variety of contexts, such as mixup (Zhang et al., 2017), cutmix (Yun et al., 2019), attentive cutmix (Walawalkar et al., 2020), and alignmixup (Venkataramanan et al., 2022). Hao et al. (2023) introduced a joint data augmentation method, which generates new image-text pairs while maintaining semantic coherence through image interpolation and text concatenation. In contrast to these approaches, we substitute the target portion of the image with one that corresponds to a low-resource cultural concept.","Although code-switching works on sentences, mixup techniques are used in various settings, like mixup (Zhang et al., 2017), cutmix (Yun et al., 2019), attentive cutmix (Walawalkar et al., 2020), and alignmixup (Venkataramanan et al., 2022). Hao et al. (2023) presented a collaborative data augmentation approach, which produces new image-text pairs while keeping semantic consistency through image interpolation and text joining. Unlike these methods, we replace the target area of the image with one that matches a low-resource cultural idea.","While code-switching functions at the sentence level, mixup approaches have applications in diverse contexts, including mixup (Zhang et al., 2017), cutmix (Yun et al., 2019), attentive cutmix (Walawalkar et al., 2020), and alignmixup (Venkataramanan et al., 2022). Hao et al. (2023) developed a joint data enhancement technique, generating novel image-text pairs with semantic coherence via image mixing and text combining. In contrast, we swap the intended image region with one exemplifying an under-resourced cultural concept.  ","Although code-switching acts on sentences, mixup techniques have utility across settings, like mixup (Zhang et al., 2017), cutmix (Yun et al., 2019), attentive cutmix (Walawalkar et al., 2020), and alignmixup (Venkataramanan et al., 2022). Hao et al. (2023) devised a collaborative data expansion method, creating new image-text instances while maintaining semantic consistency through image blending and text fusion. Divergently, we substitute the targeted image area with one emblematic of an underrepresented cultural idea.",A,0
212,Counting the Bugs in ChatGPTs Wugs A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,"Similarly, for Tamil, we split the data into train and dev sets. Since we have a limited amount of Tamil data, we kept the split ratio at around 4:1 between train and dev sets. We report the results of all baselines in Table 4. Baselines generally perform as expected, validating our usage of them. It should be noted that MinGen and AED are evaluated in IPA/feature space and may therefore be at a disadvantage compared to baselines operating directly in orthography. The training data was converted from orthography into IPA using Epitran (Mortensen et al., 2018).","Likewise, for Tamil, we divided the information into training and development sets. Given that we only had a small amount of Tamil data, we maintained a ratio of around 4:1 between the training and development sets. We present the outcomes of all baseline models in Table 4. The baseline models generally acted as anticipated, confirming that we used them appropriately. It merits noting that MinGen and AED are assessed in IPA/feature space and may thus be at a drawback relative to baselines working directly in orthography. The training information was changed from orthography into IPA utilizing Epitran (Mortensen et al., 2018).","Similarly, for the Tamil language, we separated the data into groups for training and evaluating the model. Since our Tamil data was limited, we kept an approximate split of 4:1 between the training and evaluation data. We show the performance of all baseline systems in Table 4. The baseline systems tended to work as expected, validating our use of them. It's worth noting that MinGen and AED are evaluated in a IPA/feature representation and may be disadvantaged compared to baselines working directly with orthography. The training data was converted from orthography to IPA using the Epitran tool (Mortensen et al., 2018).  ","In the same vein, for Tamil, we partitioned the information into training and validation subsets. Given the small amount of Tamil data available, we maintained a division of around 4:1 between the training and validation subsets. We present the outcomes of all baseline approaches in Table 4. The baseline approaches generally performed as anticipated, confirming our application of them. It should be noted that MinGen and AED are assessed in an IPA/feature space and may thus be at a disadvantage relative to baselines operating directly in orthographic form. The training data was transformed from orthography into IPA using Epitran (Mortensen et al., 2018).",A,0
585,RESEE,"On DD dataset, we incorporate a strong multimodal dialogue system VISAD (Shen et al., 2021), which considers words extracted from dialogue context and their corresponding images into generation.  Note that, RESEE (SHARE) is similar to MARIA (Liang et al., 2021), which considers similar training paradigm.  However, MARIA takes only one image per dialogue session, we thus consider our RESEE (SHARE) as an extension of MARIA.  See Appendix A.2, C for more model details.  We present evaluation results of models with separate or shared encoder-decoder over two datasets in Table 3.  (1) Our model with separate encoder-decoder (RESEE (SEP.)) performs better than the model with shared encoderdecoder (RESEE (SHARE)). ","On the DD dataset, we include a powerful multimodal dialogue framework called VISAD (Shen et al., 2021). This framework takes into account words from the dialogue context and their matching images for text generation. Note that RESEE (SHARE) is similar to MARIA (Liang et al., 2021), which utilizes a comparable training method. However, MARIA only considers one image per dialogue session, so we view our RESEE (SHARE) as an extension of MARIA. See Appendix A.2, C for more model specifics. We present evaluation results of models with separate or shared encoder-decoder across two datasets in Table 3. (1) Our model with separate encoder-decoder (RESEE (SEP.)) is superior to the model with shared encoder-decoder (RESEE (SHARE)).","In the DD dataset, we implement a robust multimodal dialogue platform named VISAD (Shen et al., 2021). This platform analyzes words extracted from dialogue history and their related images for text creation. Note that, RESEE (SHARE) resembles MARIA (Liang et al., 2021), which employs a similar training procedure. However, MARIA utilizes only one image per dialogue session, so we consider our RESEE (SHARE) as an augmentation of MARIA. See Appendix A.2, C for additional model information. We present evaluation results of models with distinct or shared encoder-decoder across two datasets in Table 3. (1) Our model with distinct encoder-decoder (RESEE (SEP.)) is superior to the model with shared encoder-decoder (RESEE (SHARE)).","Within the DD dataset, we deploy a strong multimodal dialogue application called VISAD (Shen et al., 2021). This application examines words pulled from dialogue background and their associated images for text generation. Note that, RESEE (SHARE) is comparable to MARIA (Liang et al., 2021), which uses a similar training process. However, MARIA utilizes only one image per dialogue session, so we view our RESEE (SHARE) as an enhancement of MARIA. See Appendix A.2, C for more model specifics. We present evaluation results of models with independent or shared encoder-decoder across two datasets in Table 3. (1) Our model with independent encoder-decoder (RESEE (SEP.)) is superior to the model with shared encoder-decoder (RESEE (SHARE)).",A,0
679,Standardizing Distress Analysis,"Table 8 shows some keywords used for crawling posts from Twitter and Gab to develop the DCaM dataset.  Initially, we randomly crawled around 5000 posts each for a period of 1 week from both Twitter and Gab and performed topic modeling to fetch the trending topics.  We randomly use a subset of these topics to crawl posts for our dataset.  From the collected posts, we create a bag of frequently occurring hashtags and use the generated set to crawl further posts.  We take care of nonrepetition in the collected posts by maintaining the post IDs.  Lastly, to supplement the lack of offensive posts being crawled, we use the synonyms of the words ’hate’, and ’offensive’ and use them as tags (like for the word ’offensive’ an example synonym could be ’insult’.","Table 8 displays some key search terms used to find posts from Twitter and Gab to build the DCaM dataset. At first, we arbitrarily gathered around 5000 posts each for 1 week from both Twitter and Gab and did topic analysis to identify the trending subjects. We randomly utilized a subset of these topics to find posts for our dataset. From the collected posts, we made a collection of commonly occurring hashtags and used the generated set to find more posts. We ensured no repetition in the gathered posts by keeping the post IDs. Lastly, to make up for the lack of offensive posts being found, we used the synonyms of the words 'hate' and 'offensive' and used them as tags (for example, a synonym for 'offensive' could be 'insult').","Table 8 exhibits some important keywords utilized to scrape posts from Twitter and Gab to construct the DCaM dataset. Initially, we haphazardly scraped around 5000 posts each for 1 week from both Twitter and Gab and executed topic modeling to obtain the trending themes. We arbitrarily utilized a subset of these themes to scrape posts for our dataset. From the collected posts, we constructed a bag of frequently happening hashtags and utilized the generated set to scrape additional posts. We ensured no duplication in the gathered posts by maintaining the post IDs. Finally, to compensate for the lack of offensive posts being scraped, we utilized the synonyms of the words 'hate' and 'offensive' and used them as tags (for instance, a synonym for 'offensive' could be 'insult').  ","Table 8 shows some vital search terms used to extract posts from Twitter and Gab to assemble the DCaM dataset. At first, we randomly extracted around 5000 posts each for 1 week from both Twitter and Gab and implemented topic modeling to obtain the popular topics. We arbitrarily used a portion of these topics to extract posts for our dataset. From the gathered posts, we created a collection of commonly occurring hashtags and utilized the generated set to extract more posts. We ensured no repetition in the collected posts by keeping the post IDs. Lastly, to make up for the shortage of offensive posts being extracted, we used the synonyms of the words 'hate' and 'offensive' and used them as tags (for example, a synonym for 'offensive' could be 'insult').",A,0
24,A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Interestingly, models attend to the source pronoun sensibly less when wrongly translating female referents (-14% in both anti-stereotypical and stereotypical cases), but the same is not valid for male cases. All these results support the use of ad-hoc interpretability methods for discovering word attribution scores associations with desirable (or undesirable) behavior, thereby serving as proxies for subsequent interventions.","Remarkably, models pay noticeably less attention to the source pronoun when inaccurately translating female referents (-14% in both anti-stereotypical and stereotypical instances), however this does not apply for male cases. These findings advocate utilizing specialized interpretability techniques to uncover word attribution correlations with favorable (or unfavorable) actions, hence acting as stand-ins for succeeding interventions.","Interestingly, models focus substantially less on the source pronoun when incorrectly translating female referents (-14% in both non-stereotypical and stereotypical situations), but this does not hold true for male cases. All these results endorse the utilization of custom interpretability approaches to detect word attribution scores links with positive (or negative) conduct, thereby functioning as proxies for subsequent involvements. ","Notably, models concentrate perceptibly less on the source pronoun when erroneously translating female referents (-14% in both counter-stereotypical and stereotypical circumstances), however the same cannot be said for male cases. These discoveries promote the employment of tailored interpretability procedures to identify word attribution correlations with desirable (or undesirable) behaviors, therefore serving as substitutes for following interventions.",A,0
139,BOOOOKSCORE,"It is possible that since hierarchical merging necessitates summarizing portions of the input document without complete context, it may introduce more coherence errors. For example, in the first level, chunks towards the end of the book will be summarized without knowledge of what came before, which can lead to incoherent summaries especially for non-linear or multi-perspective narratives. We thus explore an alternate prompting strategy—incremental updating (Figure 1, right)— that iterates through each chunk in order while continuously updating a global summary with salient information.","One potential issue with hierarchical merging is that it requires condensing parts of the input document without full context, which could introduce more incoherence errors. Specifically, in the first level, segments near the end of the book will be summarized without knowing the preceding content. This can produce incoherent summaries, particularly for nonlinear or multiperspective narratives. Therefore, we examine an alternative prompting approach—incremental updating—that goes through each chunk sequentially while continuously updating a comprehensive summary with relevant details.","Since hierarchical merging needs to summarize sections of the source text without complete context, it may bring in more incoherence mistakes. For instance, in the initial level, passages close to the conclusion of the book will be summarized without awareness of prior material, which can cause inconsistent summaries, especially for nonlinear or multi-viewpoint stories. Hence, we check out a different prompting tactic—step-by-step updating—that walks through each portion in order while steadily enhancing a universal summary with significant information. ","Because hierarchical merging requires condensing parts of the source document without full context, it could introduce more incoherence errors. Specifically, in the first tier, snippets near the end of the book will be summarized without knowledge of preceding content, which can lead to inconsistent summaries, particularly for nonlinear or multi-perspective narratives. Therefore, we explore an alternative prompting methodology—incremental enhancement—that goes through each section sequentially while continuously improving a comprehensive summary with salient details.",A,0
275,DisCo Distilled Student Models Co-training for Semi-supervised Text Mining,"This substantiates our claim that our co-training framework is superior in distilling knowledge encapsulated in unsupervised data. Furthermore, the performance across most tasks experiences a decline after the augmentation technique alteration. As stipulated in (Xie et al., 2020), the UDA/FLiText framework necessitates that augmented data maintain ‘similar semantic meanings’ thereby making back-translation a more suitable for UDA/FLiText, compared to the AD augmentation we incorporated.","This validates our assertion that our co-training system is better at extracting knowledge from unlabeled data. Also, the performance on most tasks gets worse after changing the augmentation method. As stated in (Xie et al., 2020), the UDA/FLiText framework requires augmented data to keep 'comparable semantic meanings', making back-translation more appropriate for UDA/FLiText than the AD augmentation we used.","This supports our claim that our co-training approach is superior at making use of the knowledge in unsupervised information. Furthermore, the performance on most tasks declines after altering the augmentation technique. As described in (Xie et al., 2020), the UDA/FLiText framework needs augmented data to maintain 'similar semantic meanings', which makes back-translation more fitting for UDA/FLiText compared to the AD augmentation we utilized. ","This corroborates our contention that our co-training system is better at harnessing knowledge present in unlabeled data. Additionally, the performance across most tasks suffers after changing the augmentation method. As laid out in (Xie et al., 2020), the UDA/FLiText framework requires that augmented data keep 'comparable semantic meanings', thereby making back-translation more suitable for UDA/FLiText than the AD augmentation we employed.",A,0
338,Fifty Shades of Bias,"Drawing from previous work, our annotation task defined gender bias as ""the systematic, unequal treatment based on one’s gender."" Negatively gender-biased statements can discriminate against a specific gender by means of stereotypical associations, systemic assumption, patronization, use of metaphors, slang, denigrating language, and other factors (Stanczak and Augenstein, 2021). We encouraged annotators to trust their instincts.","Building on prior research, our annotation project characterized gender bias as ""the regular, unequal treatment depending on someone's gender."" Statements exhibiting negative gender bias can discriminate against a particular gender through stereotypical links, systemic assumptions, condescension, use of metaphors, slang, derogatory language, and other elements (Stanczak and Augenstein, 2021). We told annotators to rely on their intuitions.","Leveraging previous studies, our annotation effort defined gender bias as ""the consistent, unequal treatment based on an individual's gender."" Statements with negative gender bias can marginalize a certain gender through stereotypical connections, systemic presumptions, patronization, use of metaphors, slang, disparaging language, and other factors (Stanczak and Augenstein, 2021). We encouraged annotators to trust their gut instincts.  ","Building upon earlier work, our annotation project characterized gender bias as ""the systematic, unequal treatment contingent on someone's gender."" Statements exhibiting negative gender bias can discriminate against a specific gender through stereotypical associations, systemic suppositions, condescension, use of metaphors, slang, derogatory language, and other elements (Stanczak and Augenstein, 2021). We told annotators to rely on their intuitive judgments.",A,0
367,INSTRUCTSCORE,"It achieves the best results for the unseen keyword-to-dialogue generation task. Surprisingly, INSTRUCTSCORE surpasses the supervised BLEURT in 6 out of 9 directions and closely matches state-of-the-art COMET22 in machine translation. Furthermore, we identify a range of failure modes and design an automatic pipeline to pinpoint explanation failures. Our refinement step improves human score by 13.7%, leading to a more accurate alignment with human judgment.","This approach attains the most outstanding performance on the unforeseen keyword-to-conversation generation challenge. Remarkably, INSTRUCTSCORE exceeds the supervised BLEURT in 6 out of 9 aspects and nearly equals cutting-edge COMET22 in machine translation. Moreover, we determine multiple failure methods and construct an automated pipeline to locate explanation failures. Our refinement process enhances human score by 13.7%, resulting in a more precise match with human evaluation.","It realizes the best outcomes for the unseen keyword-to-chat creation task. Unexpectedly, INSTRUCTSCORE outdoes the supervised BLEURT in 6 out of 9 directions and closely equals state-of-the-art COMET22 in machine translation. Furthermore, we pinpoint a variety of failure modes and design an automated workflow to identify explanation failures. Our refinement step boosts human score by 13.7%, leading to a more accurate alignment with human judgment.  ","This method produces the most optimal results on the novel keyword-to-conversation generation challenge. Shockingly, INSTRUCTSCORE surpasses the supervised BLEURT in 6 out of 9 aspects and nearly matches cutting-edge COMET22 in machine translation. Additionally, we identify multiple failure mechanisms and construct an automated pipeline to locate explanation failures. Our refinement process increases human score by 13.7%, resulting in a more precise match with human assessment.",A,0
689,Standardizing Distress Analysis,"A.5.2 Human Evaluation-based Metrics 1.  Fluency:   This determines whether or not the extracted span is fluent and natural.  Natural and regular answers get a score of 5, whereas inarticulate ones receive a 0.  2.  Knowledge consistency:   This determines whether or not the produced answer has used the appropriate knowledge.  If the model generates responses based on irrelevant information, it must get a score of 0, while the selection of pertinent knowledge must receive a score of 5.  3.  Informativeness:   This metric is used to assess how informative the produced replies are.  Here, a score of 0 means that the replies are uninformative, and a score of 5 means that they are. ","A.5.2 Human Appraisal Metrics 1. Fluency: This gauges if the chosen section flows well and sounds natural. Responses that are coherent and ordinary get a mark of 5, while disjointed ones receive a 0. 2. Knowledge relevance: This evaluates if the model's response utilizes suitable information. If the model produces answers using irrelevant details, it earns a 0, while drawing on pertinent knowledge gets a 5. 3. Informativeness: This metric judges how enlightening the model's responses are. Here, a 0 means the replies are unrevealing, and a 5 means they are illuminating.","A.5.2 Human-based Evaluation Metrics 1. Eloquence: This determines if the selected passage is articulate and natural sounding. Well-spoken and typical responses get a score of 5, while incoherent ones receive a 0. 2. Knowledge applicability: This assesses if the model used appropriate knowledge in its response. If the model gives answers based on irrelevant information, it gets a 0, while using relevant knowledge earns a 5. 3. Insightfulness: This metric evaluates how insightful the model's responses are. Here, a score of 0 means the replies are unenlightening, and a 5 means they are perceptive. ","A.5.2 Human Ranking Metrics 1. Coherence: This judges if the extracted section flows logically and sounds natural. Logical and ordinary responses get a 5, while disjointed ones receive a 0. 2. Knowledge pertinence: This evaluates if the model used suitable knowledge in its response. If the model gives answers using irrelevant details, it gets a 0, while utilizing relevant knowledge earns a 5. 3. Revelatory nature: This metric assesses how revealing the model's responses are. Here, a 0 means the replies are uninformative, and a 5 means they provide useful insights.",A,0
110,Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering,"When a sample is given, since the backbone and side block are parallel, only one forward propagation is needed to obtain the output of the source model and the adapted model. During back propagation, the backbone is frozen and only the parameters of the efficient side block are updated, which prevents gradient propagation in the backbone, thus significantly accelerating the backpropagation speed.","Since the backbone and side block are parallel in a given sample, just one forward pass is required to get the output of both the source model and adapted model. During backpropagation, the backbone is fixed and only the efficient side block's parameters are changed. This stops the gradient from spreading in the backbone, greatly speeding up backpropagation.","With a provided sample, as the backbone and side block are parallel, a single forward propagation suffices to get the source model and adapted model's output. When backpropagating, the backbone is static and only the side block's efficient parameters are refreshed, avoiding gradient flow in the backbone and substantially quickening backpropagation. ","When a sample is presented, the backbone and side block being parallel means one forward pass gives the output for the source and adapted models. On backpropagation, the backbone is frozen and only the efficient side block's parameters get updated, blocking the gradient in the backbone, drastically accelerating backpropagation.",A,0
82,ALDi Quantifying the Arabic Level of Dialectness of Text,"The second example shows code-switching between MSA and Egyptian DA, but an Egyptian can still naturally pronounce the MSA portion abiding by the phonetic rules of Egyptian Arabic. This might be the reason why one of the annotators labeled the sentence as mostly dialectal (see Parkinson (1991), who observed the same relation between pronunciation and perceived levels of dialectness). For the third example, all the tokens except for the first one show dialectal features, which made it easy for the three annotators to classify it as most dialectal.","The next illustration displays language mixing between formal Arabic and Egyptian colloquial Arabic. However, an Egyptian person can still organically utter the formal Arabic part while following the phonetic conventions of Egyptian Arabic. This might clarify why one of the reviewers marked the sentence as largely colloquial (see Parkinson (1991), who noticed the same link between speech and perceived degrees of dialect). For the third case, all the words except the first exhibit colloquial traits, which allowed the three reviewers to easily categorize it as most informal.","The following case demonstrates code-switching between standard Arabic and Egyptian conversational Arabic. But an Egyptian could still naturally speak the standard Arabic portion while adhering to the sound patterns of Egyptian Arabic. This could explain why one of the labelers classified the sentence as mostly vernacular (see Parkinson (1991), who observed the same connection between pronunciation and perceived levels of dialectalness). For the third instance, all the terms except the first display dialectal features, which enabled the three labelers to easily identify it as most dialectal.  ","The next example exhibits language mixing between formal Modern Standard Arabic and Egyptian daily Arabic. However, an Egyptian person can still fluently say the formal portion while following the phonology of Egyptian Arabic. This might make clear why one of the annotators marked the sentence as largely colloquial (see Parkinson (1991), who found the same relationship between speech and perceived degrees of dialectalness). For the third case, all the words except the first show informal traits, which allowed the three annotators to readily categorize it as most dialectal.",A,0
51,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Here we explore several typical distance metric functions (including Cosine, MSE, Smooth L1, and KLdivergence) for the slot-level contrastive objective, and we also consider the influence of temperature τ . Figure 4 reveals that the temperature value directly affects the final performance. Also, it shows better results overall at around τ = 0.5 for each metric function we take. We select the cosine similarity function as our desired distance metric function, due to its relatively good performance.","In this section we analyze various common distance metric functions (such as Cosine, MSE, Smooth L1, and KL divergence) for the slot-level contrastive goal, and we also examine the impact of the temperature τ. Figure 4 shows that the temperature value directly influences the final performance. It also indicates better overall results around τ = 0.5 for each metric function we use. We choose the cosine similarity function as our preferred distance metric function, because of its relatively good performance.","We investigate several typical distance metric functions here (Cosine, MSE, Smooth L1, and KL divergence among them) for the slot-level contrastive objective, and we also consider the effect of the temperature τ. Figure 4 makes clear that the temperature value has a direct bearing on the final outcome. It also demonstrates superior overall findings around τ = 0.5 for every metric function we utilize. We opt for the cosine similarity function as our desired distance metric function, owing to its relatively strong performance.  ","In this section multiple common distance metric functions are explored (such as Cosine, MSE, Smooth L1, and KL divergence) for the slot-level contrastive goal, and the influence of the temperature τ is also considered. Figure 4 shows that the final performance is directly affected by the temperature value. It also indicates overall better results around τ = 0.5 for each of the metric functions we use. We choose the cosine similarity function as our preferred distance metric function, because of its relatively good performance.",A,0
467,LLM-enhanced Self-training for Cross-domain Constituency Parsing,"From the observation, we find that the optimal selection strategy is not the same for the five target domains. In the Dialogue and Literature domains, the selection based on GRsConf apparently obtained the best performance . We also noticed that the Forum and Review domains exhibit only slight variations across the four pseudo-data selection criteria. However, for the Law domain, employing only the confidence-based criteria is the best choice to achieve self-training improvements. The token-based selection criteria do not demonstrate a significant advantage; they still improved the constituency parser by 0.33 compared to the model transfer. Looking at the average performance, it becomes evident that the selection strategy GRsConf is relatively superior compared to other approaches. ","After examining the data, we determined that the best pseudo-data selection approach differs across the five target domains. For Dialogue and Literature, choosing samples based on GRsConf confidence scores gave the best results. We also saw that the Forum and Review domains had only small differences between the four selection methods tested. However, using confidence scores alone worked best for the Law domain to get self-training gains. The token-based approaches didn't show a major benefit, though they still improved the parser by 0.33 over model transfer. Looking at the average scores makes it clear that overall, GRsConf outperformed the other selection strategies.","Our analysis revealed that the optimal pseudo-data selection strategy varies for the five domains under study. In Dialogue and Literature, selection using GRsConf confidence was clearly superior. We observed minimal variation between the four selection criteria on Forum and Review. But for Law, relying solely on confidence measures was the best approach for achieving self-training improvements. Token-based selection did not demonstrate a significant advantage, though it still improved the parser by 0.33 compared to model transfer. On average, GRsConf emerged as the relatively superior selection method versus the others.  ","The investigation showed the best pseudo-data selection approach is not uniform across the five target domains. For Dialogue and Literature, GRsConf confidence-based selection achieved the top performance. We found only slight differences between the four selection criteria on Forum and Review. However, confidence measures alone worked optimally for Law to obtain self-training gains. Token-based selection did not provide a major benefit, though it still outperformed model transfer by 0.33. Overall, GRsConf was relatively superior to the other selection strategies on average.",A,0
426,Eliminating Lipschitz Singularities in Diffusion Models,"During the training period, large Lipschitz constants near the zero point have an influence on the training of other parts due to the smooth nature of the network, resulting in instability and inaccuracy. Moreover, during the inference period, which requires a smooth network for integration purposes, the large Lipschitz constants probably have a substantial impact on accuracy, particularly for faster samplers. Therefore, the mitigation of Lipschitz singularities holds great potential for enhancing the performance of diffusion models. Fortunately, there is a simple yet effective alternative solution: by sharing the timestep conditions in the interval with large Lipschitz constants, the Lipschitz constants can be set to zero.","The training phase is impacted by the sizable Lipschitz values around zero because neural networks are smooth by design. This instability and imprecision spreads to other areas. Furthermore, inference needs a smooth network for integration. So the large Lipschitz values likely hurt accuracy, especially for faster sampling rates. Thus, handling the Lipschitz spikes could really improve diffusion models. There is a basic but powerful fix: sharing timestep terms where the spikes occur neutralizes the Lipschitz values.","During training, the big Lipschitz numbers close to zero disrupt learning across the smooth neural network, causing shakiness and mistakes. Additionally, inference requires a smooth network for integration, so the large Lipschitz numbers probably substantially reduce accuracy, particularly with quicker sampling. Therefore, managing the Lipschitz peaks has great potential to enhance diffusion model performance. Fortunately, there's a simple yet effective solution: sharing timestep conditions where the spikes happen removes the Lipschitz numbers. ","The sizable Lipschitz constants near zero impact training across the inherently smooth neural network, leading to unsteadiness and imprecision. Moreover, inference needs a smooth network for integration, so the large Lipschitz constants likely significantly hurt accuracy, especially with faster sampling. Thus, mitigating the Lipschitz spikes holds great promise for improving diffusion models. Thankfully, there is a straightforward but powerful fix: sharing timestep terms where the spikes occur eliminates the Lipschitz constants.",A,0
721,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"Using these patterns, we propose a synthetic data generation method based on a masked language model (MLM) to build a fixer. Subsequently, we use this fixer as a basis for building our critic. The critic is trained using grammaticality labels obtained from high-confidence fixer predictions. To address the data scarcity problem that arises from high-confidence filtering, we propose a masking based approach and a self-knowledge distillation method for data augmentation. The unsupervised GEC system is trained using the BIFI framework, with the fixer and the critic being refined repeatedly in iterations. We evaluate the performance of our system on both English and Chinese GEC tasks. ","Employing these patterns, we put forward a synthetic information creation technique founded on a masked language prototype (MLM) to construct a corrector. Afterward, we employ this corrector as a basis for building our reviewer. The reviewer is educated utilizing grammaticality names acquired from high-certainty corrector guesses. To address the information scarcity issue that emerges from high-certainty sifting, we propose a veiling based methodology and a self-information refining strategy for information expansion. The unsupervised GEC framework is prepared utilizing the BIFI structure, with the corrector and the pundit being refined over and over in cycles. We assess the exhibition of our framework on both English and Chinese GEC errands.","Utilizing these examples, we recommend a manufactured information age strategy in light of a covered language model (MLM) to assemble a rectifier. From that point forward, we utilize this rectifier as a reason for building our analyst. The pundit is prepared utilizing syntactic rightness marks got from high-certainty rectifier expectations. To address the information lack issue that emerges from high-certainty sifting, we propose a covering based methodology and a self-information refining technique for information expansion. The unsupervised GEC framework is prepared utilizing the BIFI structure, with the rectifier and the pundit being refined over and over in cycles. We survey the exhibition of our framework on both English and Chinese GEC assignments. ","Harnessing these patterns, we put forward an artificial data creation approach founded on a masked language archetype (MLM) to construct an amender. Subsequently, we leverage this amender as a cornerstone for erecting our assessor. The assessor is cultivated exploiting grammaticality appellations acquired from high-confidence amender surmisings. To accost the data paucity quandary that springs from high-confidence filtering, we propose a obfuscation grounded avenue and a self-wisdom distillation routine for data proliferation. The unsupervised GEC scheme is inculcated wielding the BIFI framework, with the amender and the assessor being honed repeatedly in rounds. We gauge the performance of our scheme on both English and Chinese GEC charges.",A,0
634,SOUL,"Nevertheless, there is still room for improvement in terms of overall accuracy, as well as enhancing originality and conciseness in the JG task. We include examples of justifications generated by these models in Appendix A.3 for detailed illustration. Moreover, the high agreement between human evaluators and GPT-4 suggests that automated evaluation using GPT-4 is a more viable approach than similarity evaluation. 3.4 Comparison with NLI Furthermore, we conduct an inference on SOUL test set using a widely used NLI model, namely the NLI-RoBERTa model2, trained on the SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) datasets, to demonstrate the focus of SOUL on subjective information rather than logical connections. ","There is still opportunity to further develop the total precision and originality and compactness of the justification generation task, as shown in the examples in Appendix A.3. Also, the high match between human raters and GPT-4 indicates automated scoring with GPT-4 is more practical than similarity scoring. Furthermore, we tested an NLI-RoBERTa model on the SOUL test set to highlight SOUL's emphasis on subjective knowledge over logical links.","The justification generation task can still be enhanced regarding overall accuracy, uniqueness, and brevity, as the examples in Appendix A.3 demonstrate. Additionally, the strong agreement between human evaluators and GPT-4 suggests automated evaluation with GPT-4 is a more viable method than similarity evaluation. We also ran an inference using the NLI-RoBERTa model trained on SNLI and MNLI on the SOUL test set to show SOUL's focus on subjective information rather than logical connections.","There remains room for progress on total accuracy, originality, and concision for the justification generation task, as shown by the examples in Appendix A.3. Also, the high correlation between human raters and GPT-4 indicates automated scoring by GPT-4 is more practical than similarity scoring. We further conducted an inference using the NLI-RoBERTa model trained on SNLI and MNLI datasets on the SOUL test set to highlight SOUL's emphasis on subjective knowledge instead of logical relationships.",A,0
198,Counting the Bugs in ChatGPTs Wugs A Multilingual Investigation into the Morphological Capabilities of a Large Language Model,"Statistics for the amount of data in train, dev, and test for the baselines, as well as the number of wug test words, are given in Table 1. We report the accuracy of one annotator at a time against the judgments of all other annotators in Table 2.","The numerical values showing the quantity of information in the preparation, development, and evaluation sets for the reference points, and also the count of wug evaluation words, are provided in Table 1. We present the precision of one labeler versus the labels of all other labelers in Table 2.","The figures displaying the volume of data in the training, validation, and testing datasets for the benchmarks, and also the number of wug testing terms, are listed in Table 1. We give the correctness of one rater compared to the ratings of all other raters in Table 2. ","The statistics indicating the amount of data in the coaching, checking, and assessing collections for the standards, and also the amount of wug assessing words, are included in Table 1. We provide the accuracy of one annotator against the annotations of all other annotators in Table 2.",A,0
551,Non-autoregressive Text Editing with Copy-aware Latent Alignments,"Following FELIX and EDIT5 (Mallinson et al., 2020, 2022), we evaluate our model by conducting experiments on two text editing tasks: grammatical error correction (GEC) and sentence fusion, both of which are representative and have sufficient data for training. We plan to conduct examinations on more tasks in future work due to space limitations. The task of grammatical error correction involves detecting and correcting the grammatical errors in a given sentence. Setup For English, we adopt a 3-stage training strategy to train our GEC models (Zhang et al., 2022a): 1) pretrain the model on CLANG-8 (Rothe et al., 2021), a cleaned version of the LANG-8 data; 2) finetune the pretrained model on the combination of three datasets, namely FCE (Yannakoudakis et al., 2011), NUCLE (Dahlmeier et al., 2013) and W&I+LOCNESS (Bryant et al., 2019); 3) finally, we further finetune the model on the high-quality W&I+LOCNESS. ","We assess our model's performance by running experiments on two text editing tasks: fixing grammatical mistakes (GEC) and combining sentences, which are representative and have sufficient training data. Due to length constraints, we plan to test on more tasks in the future. The goal of GEC is to identify and correct grammatical errors in a sentence. For English, we use a 3-step method to train our GEC models (Zhang et al., 2022a): 1) pre-train the model on CLANG-8 (Rothe et al., 2021), a cleaned version of LANG-8 data; 2) fine-tune the pre-trained model on a combination of three datasets - FCE (Yannakoudakis et al., 2011), NUCLE (Dahlmeier et al., 2013) and W&I+LOCNESS (Bryant et al., 2019); 3) further fine-tune the model on high-quality W&I+LOCNESS data.","We evaluate our model's capabilities by running tests on two representative text editing tasks with ample training data: fixing grammatical errors (GEC) and merging sentences. Due to space limits, we plan to experiment with more tasks later. GEC involves detecting and correcting grammatical mistakes in sentences. For English, we use a 3-step training approach for our GEC models (Zhang et al., 2022a): 1) pre-train on CLANG-8 (Rothe et al., 2021), a cleaned LANG-8 dataset; 2) fine-tune the pre-trained model on a mix of three datasets - FCE (Yannakoudakis et al., 2011), NUCLE (Dahlmeier et al., 2013) and W&I+LOCNESS (Bryant et al., 2019); 3) further fine-tune on high-quality W&I+LOCNESS data.  ","We evaluate our model via experiments on two representative text editing tasks with sufficient training data: fixing grammatical errors (GEC) and combining sentences. We plan to test more tasks later due to space constraints. GEC involves identifying and correcting grammatical mistakes in sentences. For English, we use a 3-step training process for our GEC models (Zhang et al., 2022a): 1) pre-train on cleaned LANG-8 data CLANG-8 (Rothe et al., 2021); 2) fine-tune the pre-trained model on a combination of three datasets - FCE (Yannakoudakis et al., 2011), NUCLE (Dahlmeier et al., 2013) and W&I+LOCNESS (Bryant et al., 2019); 3) further fine-tune on high-quality W&I+LOCNESS data.",A,0
380,INSTRUCTSCORE,"Specifically, we prompt GPT-4 to parse the explanation into incorrect and correct phrase pairs and extract the error span from the error location. To address hallucinations from error location (M3) and explanation (M4), we verify if our parsed error span is present in the candidate sentence. If one error annotation contains multiple incorrect-correct phrase pairs, it indicates multiple errors in one error location (G4).","In particular, we instruct GPT-4 to analyze the explanation by dividing it into inaccurate and accurate phrase pairs and pinpoint the error span using the error location. To handle illusions stemming from the error location (M3) and explanation (M4), we check if our extracted error span exists in the candidate sentence. If one error annotation has multiple inaccurate-correct phrase pairs, it signifies multiple mistakes in one error location (G4).","Specifically, we prompt GPT-4 to break down the explanation into wrong and right phrase pairs and isolate the error span based on the error location. To tackle deceptions from the error location (M3) and explanation (M4), we verify whether our parsed error span appears in the candidate sentence. When one error annotation has multiple incorrect-correct phrase pairs, it denotes various errors in a single error location (G4).  ","In particular, we direct GPT-4 to decompose the explanation into erroneous and correct phrase pairs and extract the error span using the error location. To address illusions from the error location (M3) and explanation (M4), we confirm whether our extracted error span is present in the candidate sentence. If there are multiple incorrect-accurate phrase pairs in one error annotation, it indicates multiple mistakes in one error location (G4).",A,0
613,RESEE, ,"The writer indicates that they have a strong desire to travel to foreign places and experience new cultures. They envision journeying to distant lands, immersing themselves in unfamiliar customs, tasting exotic cuisines, and encountering fascinating people. The writer conveys great enthusiasm for embracing the unknown and having transformative adventures abroad.  ","The author expresses an eager longing to go on trips overseas and become familiar with ways of life different from their own. They picture themselves voyaging to far-off countries, becoming absorbed in unique traditions, sampling flavorful ethnic foods, and meeting intriguing individuals. The author communicates intense interest in welcoming the unfamiliar and undergoing enriching quests in other nations.","The author conveys an avid wish to embark on international excursions and learn about cultures distinct from their own. They envision traveling to remote destinations, adapting to interesting customs, eating flavorful foreign dishes, and befriending fascinating foreigners. The author exhibits zeal for plunging into the unfamiliar and having mind-expanding intercultural experiences abroad.",A,0
485,Model Tells you what to Discard - Adaptive KV Cache Compression for LLMs,"Nevertheless, these methods can only be applied to non-autoregressive models and typically require an additional re-training phrase, making them less suitable for large LLMs like ChatGPT and LLaMa. Recognizing this gap, researchers started examining the potential of pruning tokens within the KV cache of autogressive LLMs. Mu et al. (2023) learns to compress the prompts into a few special tokens to reduce memory pressure during caching. However, the token prediction requires training and could be an expensive overhead during inference. Meanwhile, several concurrent methods propose to leverage accumulated attention score as the criteria to identify important tokens in the KV cache (Sheng et al., 2023; Zhang et al., 2023; Liu et al., 2023a). Our work, instead of investigating a specific eviction policy, aims to synergistically coordinate diverse eviction policies, adapting them to align more closely with model-specific attributes.","However, those techniques are only applicable to non-autoregressive models and typically need an extra retraining phase, making them less suitable for large LLMs like ChatGPT and LLaMa. Recognizing this gap, researchers started looking at the potential of pruning tokens within the KV cache of autoregressive LLMs. Mu et al. (2023) learns to compress the prompts into a few special tokens to reduce memory pressure during caching. However, the token prediction requires training and could be an expensive overhead during inference. Meanwhile, several concurrent methods propose to leverage accumulated attention score as the criteria to identify important tokens in the KV cache (Sheng et al., 2023; Zhang et al., 2023; Liu et al., 2023a). Our work, instead of investigating a specific eviction policy, aims to synergistically coordinate diverse eviction policies, adapting them to align more closely with model-specific attributes.","However, those approaches can only be used for non-autoregressive models and typically need an extra retraining step, making them less suitable for large LLMs like ChatGPT and LLaMa. Recognizing this limitation, researchers started looking into the potential of removing tokens within the KV cache of autoregressive LLMs. Mu et al. (2023) learns to compress the prompts into a few special tokens to reduce memory pressure during caching. However, the token prediction requires training and could be an expensive overhead during inference. Meanwhile, several concurrent methods propose to leverage accumulated attention score as the criteria to identify important tokens in the KV cache (Sheng et al., 2023; Zhang et al., 2023; Liu et al., 2023a). Our work, instead of investigating a specific removal policy, aims to synergistically coordinate diverse removal policies, adapting them to align more closely with model-specific attributes. ","However, those techniques can only be used with non-autoregressive models and typically require additional retraining, making them less suitable for large LLMs like ChatGPT and LLaMa. Recognizing this limitation, researchers began examining the potential of pruning tokens within the KV cache of autoregressive LLMs. Mu et al. (2023) develops a method to compress prompts into a few special tokens to reduce memory pressure during caching. However, token prediction requires training and could be expensive during inference. Meanwhile, several concurrent methods propose using accumulated attention scores to identify important tokens in the KV cache (Sheng et al., 2023; Zhang et al., 2023; Liu et al., 2023a). Our work, rather than investigating a specific removal policy, aims to synergistically coordinate diverse removal policies, adapting them to align with model-specific attributes.",A,0
323,Fifty Shades of Bias,"Templatized sentences usually have artificial structures and thus have limited applicability to downstream tasks with more natural language. Some prior works mine data from web sources like Wikipedia and Common Crawl (Webster et al., 2018; Emami et al., 2019) to create a dataset for coreference resolution. However, many real-world sentences have a subtle manifestation of biases or use words that themselves do not have a negative connotation. Hence, rule-based sentence mining may not be able to capture the more implicit biases that humans have (Blodgett et al., 2021).","Pre-defined sentences commonly have unnatural forms and therefore have restricted usefulness for later jobs needing more everyday language. Prior research extracts data from web resources such as Wikipedia and Common Crawl (Webster et al., 2018; Emami et al., 2019) to build a dataset for coreference resolution. However, numerous real-world sentences subtly exhibit biases or utilize words that by themselves do not have a negative meaning. Thus, rule-based sentence extraction may be unable to capture the more subtle biases people have (Blodgett et al., 2021).","Sentences using templates often have artificial constructions and thus are of limited value for downstream tasks requiring more natural language. Some previous work obtains data from web sources including Wikipedia and Common Crawl (Webster et al., 2018; Emami et al., 2019) to generate a dataset for coreference resolution. But many real-life sentences subtly display biases or employ words that on their own do not convey a negative connotation. Therefore, rule-based sentence mining may be unable to capture the more implicit biases humans hold (Blodgett et al., 2021).  ","Sentences following a template tend to have unnatural structures and so have restricted applicability to later tasks needing more everyday language. Prior studies extract data from web resources such as Wikipedia and Common Crawl (Webster et al., 2018; Emami et al., 2019) to build a dataset for coreference resolution. However, many sentences from the real world subtly exhibit biases or use words that in themselves do not convey a negative meaning. Thus, rule-based mining of sentences may be unable to capture the more subtle biases people have (Blodgett et al., 2021).",A,0
281,"Explain, Edit, Generate","In this paper, we overcome this limitation by developing a rationale-sensitive method to generate linguistically diverse and label-flipping counterfactuals while preserving logical relationships. In specific, the diverse and fluent counterfactuals are generated via an Explain-Edit-Generate architecture. Moreover, the checking and filtering modules are proposed to regularize the counterfactual data with logical relations and flipped labels. Experimental results show that the proposed approach outperforms the SOTA baselines and can generate linguistically diverse counterfactual data without disrupting their logical relationships.","This research presents a new technique to create linguistically varied and class-changing counterfactual data while maintaining logical connections. Specifically, the method uses an Explain-Edit-Generate structure to produce diverse and fluent counterfactuals. Additionally, new checking and filtering components are introduced to regulate the counterfactual data to have logical relationships and altered labels. Tests showed the proposed approach is better than existing leading methods and can generate linguistically diverse counterfactual examples without disrupting their logical links.","In this work, we address this limitation by developing a rationale-aware technique for generating linguistically diverse and label-flipping counterfactuals while keeping logical associations intact. The key innovation is an Explain-Edit-Generate pipeline that produces varied and natural counterfactuals. We also introduce checking and filtering modules to enforce logical relationships and inverted labels on the counterfactual data. Experiments demonstrate that our approach surpasses state-of-the-art baselines and can generate linguistically diverse counterfactuals without breaking their logical connections. ","Here we tackle this shortcoming by creating a rationale-cognizant approach for producing linguistically varied and label-inverting counterfactuals while retaining logical ties. Specifically, we employ an Explain-Edit-Generate architecture to yield diverse and fluent counterfactuals. Furthermore, we propose checking and filtering components to impose logical associations and flipped labels on the counterfactual data. Tests show our method beats existing top baselines and can generate linguistically diverse counterfactual examples without disrupting their logical bonds.",A,0
18,A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Table 1 reports the zero-shot performance of Flan-T5 and mT0 compared to supervised baseline Marian NMT models (Junczys-Dowmunt et al., 2018). Flan-T5 and mT0 slightly underperform supervised baselines. However, they show competitive zero-shot performance as measured by COMET-22 and BERTScore. COMET-20 quality estimation metric show less encouraging results, especially for Flan-T5 (see Table 9 for a full breakdown). Overall, these results suggest that zero-shot translation with instruction-tuned models is almost as valid as specialized supervised models, further motivating their adoption in real use cases.","The data in Table 1 displays the zero-shot translation abilities of Flan-T5 and mT0 in comparison to supervised Marian NMT models (Junczys-Dowmunt et al., 2018). Flan-T5 and mT0 are slightly below the supervised baselines. However, they exhibit competitive zero-shot performance based on COMET-22 and BERTScore metrics. COMET-20 quality estimates are less positive, especially for Flan-T5 (refer to Table 9 for a full analysis). In summary, these findings imply that zero-shot translation with instruction-fine-tuned models is nearly as effective as specialized supervised models, providing further motivation for adopting them in real-world applications.","The statistics in Table 1 show the zero-shot translation performance of Flan-T5 and mT0 versus supervised Marian NMT models (Junczys-Dowmunt et al., 2018) as a baseline. Flan-T5 and mT0 are slightly worse than the supervised baselines. However, they have competitive zero-shot performance according to COMET-22 and BERTScore. COMET-20 quality evaluation metrics are less encouraging, particularly for Flan-T5 (see Table 9 for a full breakdown). In general, these results indicate that zero-shot translation with instruction-tuned models is nearly as good as specialized supervised models, further supporting their use in real use cases.","The numbers in Table 1 demonstrate the zero-shot translation capabilities of Flan-T5 and mT0 compared to supervised Marian NMT models (Junczys-Dowmunt et al., 2018) as a benchmark. Flan-T5 and mT0 are slightly inferior to the supervised baselines. However, they exhibit competitive zero-shot performance per COMET-22 and BERTScore. COMET-20 quality assessment metrics are less positive, especially for Flan-T5 (refer to Table 9 for a complete analysis). Overall, these findings suggest that zero-shot translation with instruction-fine-tuned models is almost as effective as specialized supervised models, providing additional motivation for employing them in real-world settings.",A,0
576,RESEE,"(3) Image Searching:  Finally, we use two online search engines3 to search images for the entity-level visual knowledge.  Since we leverage two searching engines i.e., Qwant, Pixabay in this process, we make sure that there is at least one valid image for every extracted entity.  The proposed datasets are advantageous in comparing prior works by providing fine-grained and more accurate images related to the dialogue context.  This is because (1) we explicitly split the visual knowledge into turn-level and entity-level; (2) we use a large image pool as well as online searching engines to acquire images.  We additionally present examples and detailed statistics of RESEE-WoW and RESEE-DD in Appendix B. ",(3) Finding Pictures Online: We also use two internet search engines to find pictures related to the specific things mentioned. Using two search sites - Qwant and Pixabay - ensures we get at least one good photo for each thing. Our datasets are useful for comparing to previous work because the pictures closely match the dialogue. This is because (1) we separate visual knowledge into turn-level and entity-level; (2) we use a large collection and internet searches to get the photos. We show examples and details of RESEE-WoW and RESEE-DD in Appendix B.,"(3) Online Image Lookup: Additionally, we utilize two web-based image search tools to find visual representations of the entities. Since we employ two search engines, Qwant and Pixabay, we guarantee there is a minimum of one valid image for every entity extracted. Our proposed datasets have the advantage over prior works of providing more precise, fine-grained images related to the dialogue context. This results from (1) explicitly dividing the visual knowledge into turn-level and entity-level; (2) using a substantial image pool and online searches to obtain images. We present examples and detailed statistics of RESEE-WoW and RESEE-DD in Appendix B.  ","(3) Web Picture Searching: We also use two internet image search sites to find visual depictions of the entity-level knowledge. By tapping into two search tools - Qwant and Pixabay - we ensure at least one good image exists for each extracted entity. Our datasets have benefits over previous works by supplying more accurate, granular images tied to the dialogue context. This stems from (1) explicitly separating the visual knowledge into turn-level and entity-level; (2) utilizing a large image collection and web searches to acquire images. We provide examples and in-depth statistics of RESEE-WoW and RESEE-DD in Appendix B.",A,0
264,DisCo Distilled Student Models Co-training for Semi-supervised Text Mining,"We evaluate DisCo on extractive summarization and text classification tasks, as shown in Table 1. For extractive summarization, we use the CNN/DailyMail (Hermann et al., 2015) dataset, training the model with 10/100/1000 labeled examples. Regarding text classification, we evaluate on semi-supervised datasets: Agnews (Zhang et al., 2015) for News Topic classification, Yahoo!Answers (Chang et al., 2008) for Q&A topic classification, and DBpedia (Mendes et al., 2012) for WikiPedia topic classification. The models are trained with 10/30/200 labeled data per class and 5000 unlabeled data per class. Further details on the evaluation methodology are in Appendix A.3.","We assess DisCo on summarization and categorization tasks, as displayed in Table 1. For summarization, we utilize the CNN/DailyMail dataset, teaching the model with 10/100/1000 marked samples. For categorization, we evaluate on semi-supervised data: Agnews for news grouping, Yahoo!Answers for Q&A grouping, and DBpedia for WikiPedia grouping. The models are educated with 10/30/200 labeled information per type and 5000 unlabeled information per type. More subtleties on the assessment technique are in Appendix A.3.","We appraise DisCo on condensing and characterization errands, as shown in Table 1. For condensing, we use the CNN/DailyMail dataset, preparing the model with 10/100/1000 named models. Regarding characterization, we survey semi-supervised information: Agnews for News Topic ordering, Yahoo!Answers for Q&A Topic ordering, and DBpedia for WikiPedia Topic ordering. The models are prepared with 10/30/200 marked information per class and 5000 unlabeled information per class. Further subtleties on the assessment philosophy are in Appendix A.3. ","We evaluate DisCo on summarizing and grouping tasks, as exhibited in Table 1. For summarizing, we employ the CNN/DailyMail dataset, teaching the model with 10/100/1000 marked cases. Concerning grouping, we assess on semi-supervised information: Agnews for News Subject classification, Yahoo!Answers for Q&A Subject classification, and DBpedia for WikiPedia Subject classification. The models are prepared with 10/30/200 named information per class and 5000 unlabeled information per class. More subtleties on the assessment system are in Appendix A.3.",A,0
83,ALDi Quantifying the Arabic Level of Dialectness of Text,"Estimation Task Before describing case studies demonstrating possible uses of automatic ALDi estimation, we first show that a model trained to predict ALDi is competitive with a DI system in discriminating between dialects (including dialects barely represented in AOC-ALDi), while providing more nuanced dialectness scores. We then consider several specific features of Egyptian Arabic, and again show that the ALDi regression model is more sensitive to these than the baseline approaches.","Before illustrating example uses of automated ALDi approximation, we first display that a model educated to foresee ALDi is on par with a DI structure in telling apart dialects (consisting of dialects barely depicted in AOC-ALDi), while giving more subtle dialectness ratings. We then think about a few distinct attributes of Egyptian Arabic, and again prove that the ALDi regression model is more receptive to these than the baseline moves toward.","In advance of portraying case investigations showing potential employments of programmed ALDi evaluation, we initially exhibit that a model prepared to anticipate ALDi is serious with a DI framework in recognizing between vernaculars (incorporating vernaculars scarcely addressed in AOC-ALDi), while giving more nuanced dialectness scores. We then think about a few explicit highlights of Egyptian Arabic, and indeed show that the ALDi relapse model is more delicate to these than the benchmark methodologies. ","Before illustrating contextual analyses showing conceivable uses of computerized ALDi appraisal, we initially show that a model prepared to foresee ALDi is cutthroat with a DI framework in separating between lingos (including lingos barely addressed in AOC-ALDi), while giving more nuanced dialectness scores. We then consider a few particular attributes of Egyptian Arabic, and indeed show that the ALDi relapse model is more touchy to these than the standard methodologies.",A,0
61,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"In this paper, we tackle the problem of generalized zero-shot slot filling by the proposed end-toend metric learning based scheme. We propose a cascade-style multi-task learning framework to efficiently detect the slot entity from a target domain utterance. The context-aware soft label embeddings are shown to be superior to the widely-used discrete ones. Regarding domain adaptation robustness, we propose a slot level contrastive learning scheme to facilitate the discriminative representations of slot entities.","This research addresses the issue of broad zero-shot slot filling through a proposed end-to-end metric learning system. We put forward a cascade-style multi-task learning structure to efficiently identify the slot entity from a target domain utterance. The context-aware soft label embeddings are demonstrated to be superior to the commonly used discrete ones. For domain adaptation robustness, we propose a slot level contrastive learning approach to enable the discriminative representations of slot entities.","In this work, we take on the challenge of general zero-shot slot filling via our proposed end-to-end metric learning framework. We introduce a cascade-style multi-task learning architecture to effectively detect the slot entity from a target domain utterance. The context-aware soft label embeddings are shown to surpass the widely-used hard labels. Regarding adaptation across domains, we put forward a slot level contrastive learning scheme to facilitate distinctive representations of slot entities.","This paper addresses the issue of broad zero-shot slot filling through our proposed end-to-end metric learning scheme. We present a cascade-style multi-task learning model to efficiently locate the slot entity from a target domain utterance. The context-aware soft label embeddings outperform the commonly used discrete labels. For robustness across domains, we introduce a slot level contrastive learning approach to enable discriminative representations of slot entities.",A,0
513,Neural Fine-Tuning Search for Few-Shot Learning,"We evaluate NFTS on the extended version of Meta-Dataset (Requeima et al., 2019; Triantafillou et al., 2020), currently the most commonly used benchmark for few-shot classification, consisting of 13 publicly available datasets: FGVC Aircraft, CU Birds, Describable Textures (DTD), FGVCx Fungi, ImageNet, Omniglot, QuickDraw, VGG Flowers, CIFAR10/100, MNIST, MSCOCO, and Traffic Signs. There are 2 evaluation protocols: single domain (SD) learning and multi-domain (MD) learning. In the single domain setting, only ImageNet is seen during training and meta-training, while in the multi-domain setting the first eight datasets are seen (FGVC Aircraft to VGG Flower). For meta-testing, 600 episodes are sampled for each domain, following the evaluation protocol proposed by Triantafillou et al. (2020).","We test NFTS using the expanded adaptation of Meta-Dataset (Requeima et al., 2019; Triantafillou et al., 2020). This is currently the most popular benchmark for few-shot classification. It contains 13 publicly accessible datasets: FGVC Aircraft, CU Birds, Describable Textures (DTD), FGVCx Fungi, ImageNet, Omniglot, QuickDraw, VGG Flowers, CIFAR10/100, MNIST, MSCOCO, and Traffic Signs. There are 2 assessment protocols: single domain (SD) learning and multi-domain (MD) learning. In the single domain setting, only ImageNet is seen during training and meta-training. In the multi-domain setting, the first eight datasets are seen (FGVC Aircraft to VGG Flower). For meta-testing, 600 episodes are sampled for each domain, following the evaluation procedure proposed by Triantafillou et al. (2020).","We evaluate NFTS using an expanded version of Meta-Dataset (Requeima et al., 2019; Triantafillou et al., 2020). This is the most widely used benchmark currently for few-shot classification. It has 13 publicly available datasets: FGVC Aircraft, CU Birds, Describable Textures (DTD), FGVCx Fungi, ImageNet, Omniglot, QuickDraw, VGG Flowers, CIFAR10/100, MNIST, MSCOCO, and Traffic Signs. There are 2 evaluation methods: single domain (SD) learning and multi-domain (MD) learning. In single domain, only ImageNet is seen during training and meta-training. In multi-domain, the first eight datasets are seen (FGVC Aircraft to VGG Flower). For meta-testing, 600 episodes are sampled for each domain, following the evaluation protocol by Triantafillou et al. (2020).  ","We assess NFTS using an extended version of Meta-Dataset (Requeima et al., 2019; Triantafillou et al., 2020). This is the most common benchmark now for few-shot classification. It has 13 public datasets: FGVC Aircraft, CU Birds, Describable Textures (DTD), FGVCx Fungi, ImageNet, Omniglot, QuickDraw, VGG Flowers, CIFAR10/100, MNIST, MSCOCO, and Traffic Signs. There are 2 evaluation approaches: single domain (SD) learning and multi-domain (MD) learning. In single domain, only ImageNet is seen during training and meta-training. In multi-domain, the first eight datasets are seen (FGVC Aircraft to VGG Flower). For meta-testing, 600 episodes are sampled for each domain, following the evaluation protocol proposed by Triantafillou et al. (2020).",A,0
433,Eliminating Lipschitz Singularities in Diffusion Models,"With the development of fast sampling algorithms, it is crucial that E-TSDM can be effectively combined with classic fast samplers, such as DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b). To this end, we incorporate both DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b) into E-TSDM for fast sampling in this section. It is worth noting that the presence of large Lipschitz constants can have a more detrimental impact on the efficiency of fast sampling compared to full-timestep sampling, as numerical solvers typically depend on the similarity between function values and their derivatives on adjacent steps. When using fast sampling algorithms with larger discretization steps, it becomes necessary for the functions to exhibit better smoothness, which in turn corresponds to smaller Lipschitz constants. Hence, it is anticipated that the utilization of ETSDM will lead to an improvement in the generation performance of fast sampling methods.","As fast sampling techniques like DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b) have matured, integrating E-TSDM with these classic fast sampling methods has become imperative. We have incorporated both DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b) into E-TSDM to enable rapid sampling in this section. It's important to note that large Lipschitz constants can be more detrimental to fast sampling efficiency compared to full-timestep sampling, since numerical solvers rely on the similarity between function values and derivatives across adjacent steps. When utilizing larger discretization steps with fast sampling algorithms, the functions need to exhibit better smoothness and smaller Lipschitz constants. Therefore, employing E-TSDM is expected to improve the generation capabilities of fast sampling approaches.","With the maturation of fast sampling algorithms like DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b), combining E-TSDM effectively with these established fast samplers has become crucial. We have integrated both DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b) into E-TSDM to enable fast sampling in this section. Notably, large Lipschitz constants can impair fast sampling efficiency more than full-timestep sampling, since numerical solvers depend on the closeness of function values and derivatives across adjacent steps. When leveraging larger discretization steps with fast sampling algorithms, the functions must demonstrate better smoothness and smaller Lipschitz constants. Therefore, utilizing E-TSDM should enhance the generation performance of fast sampling techniques.","With the development of rapid sampling algorithms such as DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b), seamlessly combining E-TSDM with these well-established fast samplers is now critical. We have incorporated both DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b) into E-TSDM to facilitate fast sampling here. Significantly, large Lipschitz constants can impair fast sampling efficiency more than full-timestep sampling, since numerical solvers rely on the similarity of function values and derivatives over consecutive steps. When using larger discretization steps with fast sampling algorithms, the functions need better smoothness and smaller Lipschitz constants. Thus, employing E-TSDM should enhance the generative abilities of fast sampling methods.",A,0
182,Copyright Violations and Large Language Models,"Key considerations include respect for intellectual property rights, adherence to legal regulations, transparency and accountability in model capabilities and limitations, ethical data usage and permissions. Thanks to the anonymous reviewers for their helpful feedback. This work is supported by the Novo Nordisk Foundation.","Important factors to think about are honoring ownership rights of ideas, following the law, being open and responsible about what models can and can't do, using data morally and with approval. Many thanks to the nameless experts for their useful critiques. This project is funded by the Novo Nordisk Foundation.","Crucial points to remember are showing regard for who owns ideas, obeying rules and regulations, being clear and answerable regarding model strengths and weaknesses, using information ethically and with permission. Much appreciation to the unidentified evaluators for their constructive comments. This research is sponsored by the Novo Nordisk Foundation. ","Vital considerations are respecting intellectual property privileges, complying with legal guidelines, transparency and accountability about what models are capable of and where they fall short, moral data practices and consent. Sincere thanks to the anonymous reviewers for their helpful feedback. The Novo Nordisk Foundation provides funding for this work.",A,0
501,Model Tells you what to Discard - Adaptive KV Cache Compression for LLMs,"We use standard generation tasks to evaluate LLaMa and our fine-tuned LLaMa models. For LLaMa, we choose four different tasks, including Human Eval (Chen et al., 2021), GSM8k (Cobbe et al., 2021), NQ (Kwiatkowski et al., 2019) and TQA (Kembhavi et al., 2017) to evaluate model abilities on different domains (code, math, question answering and reading comprehension). Note that the four tasks all formulate each testing sample in a generative format, where answers are extracted after model generation finishes. This is crucial for a fair evaluation of model generation quality. For instruction finetuned LLaMa, we evaluate it on instruction tuning benchmark AlpacaEval (Li et al., 2023), which consists of 805 question prompts from diverse domains.","We utilize common generative tasks to assess LLaMa and our adapted LLaMa models. For LLaMa, we select four distinct tasks, namely Human Eval (Chen et al., 2021), GSM8k (Cobbe et al., 2021), NQ (Kwiatkowski et al., 2019) and TQA (Kembhavi et al., 2017) to evaluate model capabilities across various domains (code, mathematics, question answering and reading comprehension). Note that the four tasks all formulate each test sample in a generative manner, where answers are extracted after model generation is complete. This is vital for an impartial evaluation of model generation quality. For the instruction fine-tuned LLaMa, we evaluate it on the instruction tuning benchmark AlpacaEval (Li et al., 2023), which consists of 805 question prompts from diverse domains.","We make use of standard generative assignments to appraise LLaMa and our adapted LLaMa models. For LLaMa, we select four unique assignments, specifically Human Eval (Chen et al., 2021), GSM8k (Cobbe et al., 2021), NQ (Kwiatkowski et al., 2019) and TQA (Kembhavi et al., 2017) to assess model capabilities across various areas (code, math, question answering and reading comprehension). Note that the four assignments all formulate each test sample in a generative way, where answers are extracted after model generation is finished. This is critical for an impartial appraisal of model generation quality. For the instruction fine-tuned LLaMa, we appraise it on the instruction tuning benchmark AlpacaEval (Li et al., 2023), which consists of 805 question prompts from diverse areas.","We utilize standard generative exercises to evaluate LLaMa and our tuned LLaMa models. For LLaMa, we choose four distinct exercises, specifically Human Eval (Chen et al., 2021), GSM8k (Cobbe et al., 2021), NQ (Kwiatkowski et al., 2019) and TQA (Kembhavi et al., 2017) to assess model abilities across various domains (code, mathematics, question answering and reading comprehension). Note that the four exercises all formulate each test sample in a generative manner, where answers are extracted after model generation is complete. This is vital for an unbiased evaluation of model generation quality. For the instruction fine-tuned LLaMa, we evaluate it on the instruction tuning benchmark AlpacaEval (Li et al., 2023), which consists of 805 question prompts from diverse domains.",A,0
687,Standardizing Distress Analysis,"Visual BERT COCO enhances the model’s ability to analyze visual content and perform various vision-related tasks.  A.4.6 BiRNN-HateXplain and BERT-HateXplain We fine-tune the models10 made available by Mathew et al. (2021) on our DCaM dataset by changing the output layers as described earlier to suit our task’s objective.  A.4.7 SpanBERT SpanBERT (Joshi et al., 2020) follows a different pre-training objective compared to traditional BERT system (e.g.  predicting masked contiguous spans instead of tokens) and performs better on question-answering tasks.  Following the work in (Ghosh et al., 2022c) where SpanBERT is used to solve a mix of classification and cause extraction tasks, we fine-tune the SpanBERT base model on our DCaM dataset to meet our objective. ","Visual BERT COCO enhances the model's capacity to analyze visual content and execute various vision-related tasks. A.4.6 BiRNN-HateXplain and BERT-HateXplain We adapt the models made available by Mathew et al. (2021) to our DCaM dataset by modifying the output layers as described earlier to fit our task's goal. A.4.7 SpanBERT SpanBERT (Joshi et al., 2020) employs a different pre-training objective compared to traditional BERT systems (e.g. predicting masked contiguous spans instead of tokens) and performs better on question-answering tasks. Following the work in (Ghosh et al., 2022c) where SpanBERT is utilized to solve a mix of classification and cause extraction tasks, we adapt the SpanBERT base model on our DCaM dataset to fulfill our goal.","Visual BERT COCO boosts the model's skill to parse visual content and execute various vision-oriented tasks. A.4.6 BiRNN-HateXplain and BERT-HateXplain We customize the models provided by Mathew et al. (2021) to our DCaM dataset by altering the output layers as described before to match our task's purpose. A.4.7 SpanBERT SpanBERT (Joshi et al., 2020) employs a different pre-training purpose compared to traditional BERT systems (e.g. predicting masked contiguous spans instead of tokens) and performs better on question-answering tasks. Following the work in (Ghosh et al., 2022c) where SpanBERT is leveraged to solve a mix of classification and cause extraction tasks, we customize the SpanBERT base model on our DCaM dataset to achieve our purpose.  ","Visual BERT COCO enhances the model's aptitude to parse visual content and execute various vision-oriented tasks. A.4.6 BiRNN-HateXplain and BERT-HateXplain We adapt the models provided by Mathew et al. (2021) to our DCaM dataset by modifying the output layers as described previously to match our task's objective. A.4.7 SpanBERT SpanBERT (Joshi et al., 2020) uses a different pre-training objective compared to traditional BERT systems (e.g. predicting masked contiguous spans instead of tokens) and performs better on question-answering tasks. Following the work in (Ghosh et al., 2022c) where SpanBERT is leveraged to solve a mix of classification and cause extraction tasks, we tailor the SpanBERT base model on our DCaM dataset to fulfill our objective.",A,0
223,Cultural Concept Adaptation on Multimodal Reasoning,"Key results for the task of visually grounded reasoning across languages and cultures reveal that our methods consistently and significantly outperform baseline measures. Additionally, our technique can be tailored to enhance specific languages or cultural topics by adjusting the sampling distribution, thus reducing model bias.","The main findings for the job of using visual information to logically reason across languages and cultures show that our approaches reliably and notably exceed baseline metrics. Furthermore, our method can be adapted to boost certain languages or cultural subjects by changing the sampling distribution, thereby decreasing model prejudice.","The most important outcomes for the objective of thinking logically using visual data across languages and cultures indicate that our techniques consistently and substantially surpass baseline scores. Also, our procedure can be customized to improve particular languages or cultural themes by modifying the sampling distribution, thereby lowering model bias. ","The primary results for the task of rational thinking utilizing visual material across languages and cultures demonstrate that our processes steadily and significantly outdo baseline evaluations. Additionally, our system can be tailored to enhance specific languages or cultural topics by adjusting the sampling distribution, thereby reducing model partiality.",A,0
617,SOUL,"However, SC is not equivalent to the broader field of sentiment analysis as it does not require a deep understanding of the underlying sentiments and opinions expressed in the text. To determine the overall sentiment orientation, a model can simply rely on superficial textual features, such as the presence of specific words or phrases indicating positivity or negativity (Wulczyn et al., 2017;Wang and Culotta, 2020, 2021; Moon et al., 2021; Choi et al., 2022). Therefore, even if a model demonstrates satisfactory performance in sentiment classification, it may not fully capture the subtle nuances of sentiment in languages, such as mixed sentiments towards different aspects, motivation of the expressed opinions, and possible outcomes of such sentiments, etc. ","However, SC is not the same as the larger area of sentiment analysis since it does not need a deep grasp of the underlying feelings and viewpoints expressed in the text. To decide the overall sentiment direction, a model can just depend on superficial textual features, like the existence of specific words or phrases indicating positivity or negativity (Wulczyn et al., 2017;Wang and Culotta, 2020, 2021; Moon et al., 2021; Choi et al., 2022). Thus, even if a model shows satisfactory performance in sentiment classification, it may not fully capture the subtle nuances of sentiment in languages, such as mixed feelings towards different aspects, motivation of the expressed opinions, and possible outcomes of such sentiments, etc.","However, SC is not identical to the more expansive field of sentiment analysis because it does not require a profound understanding of the underlying emotions and perspectives conveyed in the text. To determine the overall sentiment orientation, a model can simply utilize superficial textual cues, such as the occurrence of particular words or phrases denoting positivity or negativity (Wulczyn et al., 2017;Wang and Culotta, 2020, 2021; Moon et al., 2021; Choi et al., 2022). Therefore, even if a model exhibits adequate performance in sentiment classification, it may fail to fully grasp the subtle nuances of sentiment in languages, like mixed emotions towards various facets, impetus behind the expressed viewpoints, and potential results of such sentiments, etc.  ","However, SC is not tantamount to the more comprehensive domain of sentiment analysis since it does not necessitate an in-depth comprehension of the underlying feelings and outlooks articulated in the text. To ascertain the overall sentiment inclination, a model can simply leverage superficial textual indicators, such as the presence of specific verbiage or phrasing connoting positivity or negativity (Wulczyn et al., 2017;Wang and Culotta, 2020, 2021; Moon et al., 2021; Choi et al., 2022). Consequently, even if a model evinces satisfactory aptitude in sentiment classification, it may yet fail to fully capture the subtle nuances of sentiment in languages, such as mixed emotions toward discrete aspects, impetus underlying the expressed perspectives, and potential upshots of such sentiments, etc.",A,0
453,LLM-enhanced Self-training for Cross-domain Constituency Parsing,"We divide the LLM-enhanced self-training constituency parsing into six detailed steps on each iteration: 1) LLM Generating: We first leverage the Large Language Model to produce a raw corpus bR for the target domain, based on GRs extracted from the currently available treebank and a few sample sentences (R) from the target domain. 2) Parser Training: Next, we train a constituency parser using the source treebank S and the selected pseudo trees bD for the target domain. During the initial step, the pseudo treebank isempty (bD = {}), and the parser is trained solely on the source domain data. 3) Domain Parsing: We apply the trained parser to parse the generated raw corpus bR , resulting in a set of candidate parse trees D. ","We split the LLM-boosted self-supervised syntax parsing into six precise phases per cycle: 1) LLM Creation: We first use the Massive Language Model to construct a raw text bR for the intended field, drawing on GRs obtained from the existing parse tree repository and a few example utterances (R) from the intended field. 2) Parser Education: Subsequently, we develop a syntax parser utilizing the source parse tree store S and the chosen pseudo trees bD for the intended field. At the initial phase, the pseudo treebank is empty (bD = {}), and the parser is coached exclusively on the source domain information. 3) Domain Parsing: We run the developed parser to analyze the produced raw text bR, yielding a collection of candidate parse trees D.","We divide the LLM-enhanced self-learning parse tree generation into six step-by-step processes on each round: 1) LLM Authoring: We first leverage the Large Language Model to author a raw manuscript bR for the target area, drawing from GRs extracted from the current parse tree catalog and a few sample sentences (R) from the target area. 2) Parser Education: Next, we educate a parse tree generator using the source parse tree catalog S and the selected pseudo trees bD for the target area. Initially, the pseudo treebank is vacant (bD = {}), and the parser is educated solely on the source domain data. 3) Domain Parsing: We apply the educated parser to analyze the authored raw manuscript bR, producing a set of candidate parse trees D.","We separate the LLM-boosted self-teaching parse tree construction into six precise stages per repetition: 1) LLM Writing: We first utilize the Large Language Model to write a raw corpus bR for the intended domain, deriving from GRs obtained from the current parse tree repository and a few sample sentences (R) from the intended domain. 2) Parser Training: Subsequently, we train a parse tree builder using the source parse tree repository S and the selected pseudo trees bD for the intended domain. Initially, the pseudo treebank is empty (bD = {}), and the parser is trained exclusively on the source domain data. 3) Domain Parsing: We run the trained parser to parse the written raw corpus bR, generating a set of candidate parse trees D.",A,0
309,"Explain, Edit, Generate","Table 5 shows examples of the evidence edited by RACE. We can observe that rationale- and entity based editing enables the edited evidence to still retain multi-hop correlation with each other and present a completely different fact from the original evidence. Hence, the claim generator can generate logical, fluent, and linguistically diverse counterfactual claims based on the edited evidence.","The table displays instances of the proof adjusted by RACE. We can see that justification and entity grounded editing lets the adapted proof still keep multi-step connection with one another and show a totally different reality from the first proof. Thus, the claim generator can make logical, fluent, and linguistically varied hypothetical claims founded on the adapted proof.","The table exhibits samples of the justification modified by RACE. We can notice that reason and entity oriented editing enables the adapted justification to still hold multi-step association with each other and depict a completely divergent fact from the original justification. Hence, the claim generator can construct rational, smooth, and linguistically diverse counterfactual claims grounded on the adapted justification. ","The table presents examples of the evidence changed by RACE. We can discern that rationale and entity focused editing allows the altered evidence to still maintain multi-hop correlation with one another and convey a totally different actuality from the original evidence. Therefore, the claim generator can formulate logical, fluid, and linguistically diverse hypothetical claims based on the altered evidence.",A,0
506,Neural Fine-Tuning Search for Few-Shot Learning,"Few-shot recognition (Lake et al., 2011; Miller et al., 2000; Wang et al., 2020b) aims to learn novel concepts from few examples, often by rapid adaptation of a model trained on a disjoint set of labels. Many solutions adopt a meta-learning perspective (Finn et al., 2017; Lee et al., 2019; Ravi & Larochelle, 2017; Snell et al., 2017), or train a powerful feature extractor on the source classes (Tian et al., 2020; Wang et al., 2019) – both of which assume that the training and testing classes are drawn from the same underlying distribution e.g., written characters (Lake et al., 2015), or ImageNet categories (Vinyals et al., 2016). Later work considers a more realistic and challenging setting of few-shot adaptation not only across visual categories, but also across diverse visual domains (Triantafillou et al., 2020; Ullah et al., 2022).","Few-shot learning (Lake et al., 2011; Miller et al., 2000; Wang et al., 2020b) aims to acquire new concepts from a small number of samples, frequently by fast tuning of a model trained on a separate set of labels. Many solutions take a meta-learning approach (Finn et al., 2017; Lee et al., 2019; Ravi & Larochelle, 2017; Snell et al., 2017), or train a powerful feature extractor on the source classes (Tian et al., 2020; Wang et al., 2019) – both assuming that the training and testing classes are drawn from the same fundamental distribution e.g., handwritten characters (Lake et al., 2015), or ImageNet categories (Vinyals et al., 2016). Later work considers a more realistic and challenging setting of few-shot adaptation not just across visual categories, but also across diverse visual domains (Triantafillou et al., 2020; Ullah et al., 2022).","Few-example recognition (Lake et al., 2011; Miller et al., 2000; Wang et al., 2020b) aims to learn new concepts from a small number of samples, often by fast tuning of a model trained on a separate set of labels. Many solutions take a meta-learning approach (Finn et al., 2017; Lee et al., 2019; Ravi & Larochelle, 2017; Snell et al., 2017), or train a powerful feature extractor on the source classes (Tian et al., 2020; Wang et al., 2019) – both presuming that the training and testing classes are drawn from the same basic distribution e.g., handwritten characters (Lake et al., 2015), or ImageNet categories (Vinyals et al., 2016). Later work considers a more realistic and challenging setting of few-example adaptation not just across visual categories, but also across diverse visual domains (Triantafillou et al., 2020; Ullah et al., 2022).","Few-instance learning (Lake et al., 2011; Miller et al., 2000; Wang et al., 2020b) aims to learn novel concepts from a small number of examples, often by rapid fine-tuning of a model trained on a separate set of labels. Many solutions take a meta-learning approach (Finn et al., 2017; Lee et al., 2019; Ravi & Larochelle, 2017; Snell et al., 2017), or train a powerful feature extractor on the source classes (Tian et al., 2020; Wang et al., 2019) – both assuming that the training and testing classes are drawn from the same fundamental distribution e.g., handwritten characters (Lake et al., 2015), or ImageNet categories (Vinyals et al., 2016). Later work considers a more realistic and challenging setting of few-instance adaptation not just across visual categories, but also across diverse visual domains (Triantafillou et al., 2020; Ullah et al., 2022).",A,0
429,Eliminating Lipschitz Singularities in Diffusion Models,"In this section, we present compelling evidence that our E-TSDM outperforms existing approaches on a variety of datasets. To achieve this, we first detail the experimental setup used in our studies in Section 5.1. Subsequently, in Section 5.2, we compare the synthesis performance of E-TSDM against that of the baseline on various datasets. Remarkably, our approach sets a new state-of-the-art benchmark for diffusion models on FFHQ 256×256 (Karras et al., 2019). In Section 5.3, we conduct multiple ablation studies and quantitative analysis from two perspectives. Firstly, we demonstrate the generalizability of E-TSDM by implementing it on continuous-time diffusion models and varying the noise schedules.","In this part, we put forward compelling proof that our E-TSDM is superior to existing methods on many datasets. To do this, we first explain the experimental configuration used in our studies in Section 5.1. After that, in Section 5.2, we contrast the synthesis capabilities of E-TSDM against the baseline on different datasets. Strikingly, our approach establishes a new state-of-the-art benchmark for diffusion models on FFHQ 256×256 (Karras et al., 2019). In Section 5.3, we conduct multiple ablation experiments and quantitative analysis from two angles. First, we demonstrate the adaptability of E-TSDM by implementing it on continuous-time diffusion models and altering the noise schedules.","Here, we present convincing evidence that our E-TSDM surpasses current approaches on various datasets. To show this, we first describe the experimental setup utilized in our studies in Section 5.1. Next, in Section 5.2, we compare the synthesis performance of E-TSDM to that of the baseline on multiple datasets. Remarkably, our approach sets a new standard for diffusion models on FFHQ 256×256 (Karras et al., 2019). In Section 5.3, we perform multiple ablation studies and quantitative analysis from two perspectives. First, we exhibit the flexibility of E-TSDM by applying it to continuous-time diffusion models and changing the noise schedules.  ","In this portion, we provide compelling proof that our E-TSDM is superior to existing methods on many datasets. To accomplish this, we first outline the experimental configuration employed in our studies in Section 5.1. Following that, in Section 5.2, we contrast the synthesis capabilities of E-TSDM with the baseline across various datasets. Strikingly, our approach establishes a new state-of-the-art benchmark for diffusion models on FFHQ 256×256 (Karras et al., 2019). In Section 5.3, we undertake multiple ablation experiments and quantitative analysis from two standpoints. First, we demonstrate the adaptability of E-TSDM by implementing it on continuous-time diffusion models and modifying the noise schedules.",A,0
478,Model Tells you what to Discard - Adaptive KV Cache Compression for LLMs,"The generative inference of LLMs usually involves using the KV Cache mechanism to enhance the generation speed. KV cache stores previously computed Key/Value vectors in attention calculation and reuses those values for the current token generation. As such, it avoids recalculations of previous tokens at each token generation step at the cost of extra memory consumption. Despite being a prominent technique, the memory consumption of KV cache increases rapidly as the model size and generation length increase, drastically increasing the pressure of on-device memory.","The deductive reasoning capabilities of large language models typically make use of the key-value cache system to improve the speed of text generation. The key-value cache stores previously computed key/value vector pairs that were used in attention calculations, and reuses those vectors when generating the current token. This prevents having to recalculate attention for previous tokens at each new token generation step, at the expense of requiring additional memory. Although widely used, the memory needs of the key-value cache grow quickly as model size and length of generated text increase, dramatically escalating the demand for on-device memory.","The ability of large language models to make inferences usually involves leveraging a key-value cache to accelerate text generation. This cache retains previously computed key/value vectors from attention mechanisms and reapplies them during current token creation, avoiding recomputation of prior tokens at each generation interval, but requiring extra memory. Despite being a prevalent approach, key-value cache memory use grows rapidly with larger models and longer generation, greatly expanding pressure on available on-device memory.","The capacity of large language models to deduce information typically utilizes a key-value cache system to improve text generation speed. The key-value cache stores key/value vector pairs previously calculated during attention for reuse when generating the current token, preventing recomputation of previous tokens at each generation step, at the cost of increased memory needs. While a common technique, key-value cache memory demands grow quickly with larger models and longer generation lengths, substantially escalating on-device memory requirements.",A,0
176,Copyright Violations and Large Language Models,"For books, the number of editions and reviews on GoodReads are selected as popularity indicators. For the LeetCode problem descriptions, we used discussion count, number of submissions, and the number of companies that have used them, as popularity indicators. Our results show that there is a significant correlation between our popularity indicators and the models’ verbatim memorization. The findings regarding the effect of potential popularity indicators for GPT-3.5 are presented in Figure 3. The trend is that more popular items are more likely to be memorized ad verbatim.","Regarding published works, the quantity of editions and critiques on GoodReads are chosen as markers of popularity. For the LeetCode issue explanations, we utilized discourse tally, amount of entries, and the figure of corporations that have utilized them, as popularity markers. Our discoveries demonstrate that there is a significant relationship between our popularity markers and the models' verbatim retention. The discoveries concerning the impact of potential popularity markers for GPT-3.5 are exhibited in Figure 3. The pattern is that more mainstream things are more prone to be remembered precisely word for word.","For books, the number of printings and evaluations on GoodReads are picked as indicators of fame. For the LeetCode problem clarifications, we employed conversation check, quantity of submissions, and the amount of organizations that have employed them, as fame pointers. Our results exhibit that there is a huge relationship between our fame pointers and the models' verbatim maintenance. The discoveries with respect to the impact of potential fame markers for GPT-3.5 are shown in Figure 3. The trend is that more prevalent items are more inclined to be remembered verbatim. ","Regarding published content, the amount of printings and surveys on GoodReads are chosen as measures of renown. For the LeetCode issue clarifications, we used talk tally, number of entries, and the quantity of companies that have used them, as renown markers. Our discoveries demonstrate that there is a significant association between our renown markers and the models' verbatim retention. The discoveries concerning the impact of potential renown markers for GPT-3.5 are shown in Figure 3. The pattern is that more common items are more prone to be remembered word for word.",A,0
538,Non-autoregressive Text Editing with Copy-aware Latent Alignments,"Our desiderata in this work is to design a nonautoregressive model for text editing that enjoys the merits of both efficiency and effectiveness, meanwhile generalizing well to other languages. This poses two considerations: 1) flexible, nonmanually defined edit space; 2) a minimal set of tailored operations (Dong et al., 2019) to maintain the generalization. Taking inspirations from recent progresses in non-autoregressive text generation (Libovicky and Helcl, 2018; Saharia et al., 2020; Huang et al., 2022b), in this work, we propose a novel method for text editing that meets the aforementioned expectations by making a direct yet effective extension to connectionist temporal classification (CTC) (Graves et al., 2006). ","The goal of this work is to create a non-autoregressive model for text editing that has the benefits of both efficiency and effectiveness, while also generalizing well to other languages. This presents two considerations: 1) a flexible, non-manually defined edit space; 2) a minimal set of tailored operations (Dong et al., 2019) to maintain generalization. Drawing inspiration from recent progress in non-autoregressive text generation (Libovicky and Helcl, 2018; Saharia et al., 2020; Huang et al., 2022b), in this work, we propose a new method for text editing that meets the aforementioned expectations by making a straightforward but effective extension to connectionist temporal classification (CTC) (Graves et al., 2006).","Our aim in this work is to build a non-autoregressive model for editing text that has the advantages of both speed and accuracy, and can also generalize well to other tongues. This presents two factors to consider: 1) an adaptable, non-manually specified edit space; 2) a small set of customized operations (Dong et al., 2019) to keep generalization. Taking ideas from recent advances in non-autoregressive text creation (Libovicky and Helcl, 2018; Saharia et al., 2020; Huang et al., 2022b), here we propose a novel method for editing text that fulfills the above expectations by making a direct yet effective addition to connectionist temporal classification (CTC) (Graves et al., 2006).  ","The objective in this work is to construct a non-autoregressive model for modifying text that enjoys the benefits of efficiency and effectiveness, while also extending well to other languages. This introduces two considerations: 1) a flexible, non-manually defined space for edits; 2) a minimal set of tailored operations (Dong et al., 2019) to retain generalization. Drawing inspiration from recent improvements in non-autoregressive text generation (Libovicky and Helcl, 2018; Saharia et al., 2020; Huang et al., 2022b), here we propose a new approach for editing text that satisfies the aforementioned expectations by making a straightforward yet potent extension to connectionist temporal classification (CTC) (Graves et al., 2006).",A,0
718,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"Unsupervised GEC systems aim to over come this limitation. However, the current performance of unsupervised GEC systems (Alikaniotis and Raheja, 2019; Yasunaga et al., 2021) is much lower than supervised systems. Moreover, they still require manually defined or extracted confusion sets to generate synthetic data and assess sentence grammaticality. As a result, this greatly hinders the applicability of unsupervised GEC systems. The SOTA unsupervised GEC system, LM-critic (Yasunaga et al., 2021), uses the Break-It-Fix-It (BIFI) framework (Yasunaga and Liang, 2021) to extract realistic parallel data from unlabeled data. Specifically, the BIFI framework utilizes a fixer and a critic. ","Unsupervised grammar error correction systems try to get around the limitation of requiring labeled data. However, the performance of current unsupervised systems (Alikaniotis and Raheja, 2019; Yasunaga et al., 2021) is much worse than supervised systems. Also, they still need manually created or extracted sets of confused words to generate synthetic data and evaluate sentence correctness. This greatly limits the usefulness of unsupervised grammar error correction systems. The state-of-the-art unsupervised system, LM-critic (Yasunaga et al., 2021), uses the Break-It-Fix-It framework (Yasunaga and Liang, 2021) to extract realistic parallel data from unlabeled data. Specifically, the Break-It-Fix-It framework uses a corrector and a critic.","Unsupervised grammar correction systems attempt to overcome the need for labeled training data. But the accuracy of current unsupervised systems (Alikaniotis and Raheja, 2019; Yasunaga et al., 2021) is far below supervised systems. They also still require manually defined or extracted sets of commonly confused words to create synthetic training data and judge sentence grammaticality. This severely restricts the applicability of unsupervised grammar correction systems. The best unsupervised system, LM-critic (Yasunaga et al., 2021), utilizes the Break-It-Fix-It framework (Yasunaga and Liang, 2021) to extract realistic parallel data from unlabeled data. In particular, the Break-It-Fix-It framework uses a fixer and a critic.","Unsupervised grammar error correction systems try to avoid needing labeled training data. However, present unsupervised systems (Alikaniotis and Raheja, 2019; Yasunaga et al., 2021) have much lower performance than supervised systems. They also still need manually created or extracted sets of commonly confused words to generate synthetic training data and evaluate sentence correctness. This greatly limits the usefulness of unsupervised grammar error correction systems. The state-of-the-art unsupervised system, LM-critic (Yasunaga et al., 2021), uses the Break-It-Fix-It framework (Yasunaga and Liang, 2021) to extract realistic parallel data from unlabeled data. Specifically, the Break-It-Fix-It framework utilizes a corrector and an evaluator.",A,0
163,Copyright Violations and Large Language Models,"The trade-off between memorization and generalization (Elangovan et al., 2021) operates along a continuum from storing verbatim to storing highly abstract (compressed) knowledge. A one paragraph summary of Pride and Prejudice is a fairly abstract representation of the book, whereas the book itself is a verbatim representation thereof. Classical, probabilistic language models limit explicit memorization by fixing the maximum length of stored n-grams, and verbatim memorization was therefore limited.","There is a balance between remembering things word-for-word and forming general concepts (Elangovan et al., 2021). This ranges from keeping the exact details to keeping just the main ideas in a summarized form. A one paragraph summary of Pride and Prejudice has the key points but not all the specifics, while the full book has everything verbatim. Old language models restricted explicit memorization by capping the length of n-grams they would store, so they couldn't remember things word-for-word.","There is a tradeoff between storing information verbatim versus in a generalized, abstracted form (Elangovan et al., 2021). This spans from keeping every word to just the core concepts. A one paragraph précis of Pride and Prejudice has the essence but not the specifics, whereas the book itself has the full verbatim text. Conventional probabilistic language models constrained explicit memorization by limiting the longest n-grams stored, so verbatim storage was restricted.  ","The extent to which information is stored precisely versus in a summarized, high-level way (Elangovan et al., 2021) ranges on a scale from keeping the full details to just the key points. A one paragraph synopsis of Pride and Prejudice retains the core ideas without the verbatim text, while the full book preserves every word. Old statistical language models capped the length of n-grams that could be memorized, thus limiting the ability to store things word-for-word.",A,0
248,DisCo Distilled Student Models Co-training for Semi-supervised Text Mining,"Many text mining models are constructed by fine-tuning a large deep pre-trained language model (PLM) in downstream tasks. However, a significant challenge nowadays is maintaining performance when we use a lightweight model with limited labelled samples. We present DisCo, a semi-supervised learning (SSL) framework for fine-tuning a cohort of small student models generated from a large PLM using knowledge distillation. Our key insight is to share complementary knowledge among distilled student cohorts to promote their SSL effectiveness.","Numerous text analysis systems are built by adjusting a massive deep pre-educated language prototype (PLM) for specific downstream jobs. Though, a main obstacle today is keeping performance when utilizing a lightweight model with few labeled examples. We introduce DisCo, a semi-supervised learning (SSL) structure for tuning a group of small student models spawned from a large PLM using knowledge distillation. Our key understanding is to mutually share complementary knowledge between distilled student cohorts to encourage their SSL effectiveness.","Many text mining algorithms are produced by fine-tuning an enormous deep pre-trained natural language model (PLM) for particular downstream tasks. However, a huge challenge now is retaining accuracy when employing a compact model with scarce tagged samples. We put forward DisCo, a semi-supervised learning (SSL) framework for calibrating a set of small student models derived from a large PLM utilizing knowledge distillation. Our vital insight is to mutually share complementary information among distilled student groups to boost their SSL performance.  ","Numerous text analysis systems are constructed by adjusting a massive deep pre-trained language model (PLM) for specific downstream applications. Though, a principal obstacle today is maintaining effectiveness when operating a lightweight model with few labeled examples. We present DisCo, a semi-supervised learning (SSL) structure for tuning a cohort of small student models generated from a large PLM using knowledge distillation. Our key understanding is to collaboratively share complementary knowledge among distilled student cohorts to encourage their SSL effectiveness.",A,0
570,RESEE,"Three types of token embeddings are considered in the encoder module to sink in the knowledge from different modalities.  To prove the effectiveness of RESEE, we further compare our dialogue model with several strong baselines, including four task-oriented pre-trained models and two similar multimodal dialogue systems.  RESEE outperforms most baselines on both automatic and human evaluations.  We also conduct comprehensive ablation experiments to demonstrate (1) the model performance gains brought by different visual knowledge, (2) the model performance with increased visual knowledge volumes, and (3) the relation between the proposed visual knowledge and the conventional document knowledge. ","The encoder module utilizes three kinds of token representations to incorporate knowledge from various modalities. To demonstrate the efficacy of RESEE, we also benchmark our dialogue model against several robust baseline systems, including four mission-oriented pretrained models and two analogous multimodal dialogue frameworks. RESEE surpasses most baselines on both automated and human assessments. We further perform extensive ablation experiments that exhibit (1) the model performance improvements resulting from different visual knowledge, (2) the model performance with larger volumes of visual knowledge, and (3) the connection between the proposed visual knowledge and conventional document knowledge.","The encoder makes use of three types of token embeddings to assimilate understanding from multiple modalities. To prove the effectiveness of RESEE, we also compare our conversation model with several strong reference systems, including four goal-focused pre-trained models and two similar multimedia conversation frameworks. RESEE outdoes most reference systems on both machine and human evaluations. We also conduct comprehensive reduction experiments to demonstrate (1) the model performance increases provided by different visual information, (2) the model performance with greater volumes of visual information, and (3) the relationship between the proposed visual information and traditional document information.  ","The encoder utilizes three varieties of token representations to incorporate insights from different modalities. To validate the efficacy of RESEE, we also benchmark our dialogue model against several robust baselines, including four objective-oriented pre-trained models and two analogous multimodal dialogue systems. RESEE exceeds most baselines on both automated and human assessments. We also perform extensive ablation experiments that exhibit (1) the model performance boosts resulting from diverse visual knowledge, (2) the model performance with expanded volumes of visual knowledge, and (3) the connection between the proposed visual knowledge and traditional document knowledge.",A,0
698,"Unnatural Error Correction, GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text","For instance, GPT-4 can reconstruct the original sentences to near-perfect recovery rate in the extreme scenario, as in Figure 1. The most related works are the studies investigating the effects of word or sub-word level perturbations (Sinha et al., 2021a,b; Pham et al., 2021; Abdou et al., 2022) and the studies evaluating the robustness of LLMs (Wang et al., 2023; Zhu et al., 2023).  To the best of our knowledge, no existing studies have investigated LLMs’ ability to handle character-level permutations, particularly those of an extremely high level that drastically change tokenization.  Our study aims to fill this gap. ","As an example, GPT-4 is able to recreate the original sentences with a very high success rate even in the most extreme case, as shown in Figure 1. The most relevant previous work includes research examining the effects of disturbances at the word or sub-word level (Sinha et al., 2021a,b; Pham et al., 2021; Abdou et al., 2022) and studies assessing the resilience of LLMs (Wang et al., 2023; Zhu et al., 2023). As far as we know, no existing work has looked at LLMs' capacity to handle character-level rearrangements, especially highly extreme ones that dramatically alter tokenization. Our study seeks to address this gap in the literature.","To illustrate, GPT-4 can reconstruct the original sentences with near-flawless accuracy even under the most extreme conditions, as Figure 1 shows. The most pertinent prior studies are those probing the impacts of perturbations at the word or subword level (Sinha et al., 2021a,b; Pham et al., 2021; Abdou et al., 2022) and those gauging the robustness of LLMs (Wang et al., 2023; Zhu et al., 2023). To our knowledge, no previous work has investigated LLMs' aptitude for handling character-level shuffles, particularly drastic ones that profoundly change tokenization. Our study aims to fill this unaddressed area.","As an example, GPT-4 is capable of reproducing the original sentences with near-perfect success even in the most extreme scenario, as depicted in Figure 1. The most relevant previous works are studies analyzing the effects of disturbances at the word or subword level (Sinha et al., 2021a,b; Pham et al., 2021; Abdou et al., 2022) and studies evaluating the sturdiness of LLMs (Wang et al., 2023; Zhu et al., 2023). As far as we know, no existing research has examined LLMs' skill at handling character-level jumbles, especially extreme ones that radically alter tokenization. Our study seeks to address this gap in the literature.",A,0
74,ALDi Quantifying the Arabic Level of Dialectness of Text,"Annotators also labeled full sentences according to their level of dialectness. Although the inter-annotator agreement was relatively good (less so for the sentence level), only a small corpus was annotated (19k words). Moreover, the corpus has sentences that are mostly in MSA with limited code-switching. A later work piloted a simplified version of the scheme on another corpus of 30k words (Elfardy and Diab, 2012). Both corpora are not publicly released.","In addition, annotators classified entire sentences based on their degree of dialect usage. While there was decent consistency between the annotators (less so for sentences), only a small set of texts was annotated (19k words). Furthermore, the texts contained sentences that were largely in Modern Standard Arabic with some code-switching. A following study tested a simplified form of the system on another set of 30k words (Elfardy and Diab, 2012). Neither of the text collections has been made publicly available.","Moreover, annotators categorized full sentences by their level of dialectal language. Although agreement between the annotators was quite good (less for sentences), just a small corpus was annotated (19k words). Also, the corpus mostly has sentences in Modern Standard Arabic with limited language mixing. A later effort tested a simplified version of the scheme on a different corpus of 30k words (Elfardy and Diab, 2012). Neither corpus has been released publicly.  ","In addition, annotators classified whole sentences based on their degree of containing dialect. While there was relatively good consistency between the annotators (less consistency for sentences), only a small set of texts was annotated (19k words). Also, the texts had sentences that were mostly in Modern Standard Arabic with some language switching. A subsequent study tried out a simplified form of the system on another set of 30k words (Elfardy and Diab, 2012). Neither text collection has been made publicly available.",A,0
457,LLM-enhanced Self-training for Cross-domain Constituency Parsing,"This approach ensures that the most relevant instances are selected, enhancing the model’s gradual adaptation to the target domain. The distance computation can be performed at either the token level or the grammar rule level by adjusting the set to represent token distribution or grammar rule distribution, respectively. The grammar rules we use include both terminal and non-terminal rules. Our instance selection process involves three levels of criteria: token, confidence, and grammar rule. We also combine the two bestperforming criteria, namely confidence-based selection and grammar-rule-based selection, resulting in a more effective criterion for identifying high-quality instances for adaptation to the target domain. ","This method guarantees that the most applicable examples are chosen, improving the model's step-by-step adjustment to the new domain. The distance calculation can be done either at the token stage or the grammar rule stage by changing the set to represent token distribution or grammar rule distribution, respectively. The grammar rules we utilize include both terminal and non-terminal rules. Our example selection process has three levels of criteria: token, confidence, and grammar rule. We also unite the two best-performing criteria, specifically confidence-based selection and grammar-rule-based selection, resulting in a more successful criterion for pinpointing high-quality examples for adapting to the target domain.","This technique ensures that the most relevant cases are picked, enhancing the model's gradual acclimation to the target area. The distance computation can be executed at either the token position or the grammar rule position by tuning the set to represent token dispersion or grammar rule dispersion, respectively. The grammar rules we employ include both terminal and non-terminal rules. Our case selection workflow involves three tiers of criteria: token, confidence, and grammar rule. We also combine the two top-performing criteria, namely confidence-based picking and grammar-rule-based picking, resulting in a more effective criterion for identifying high-quality cases for acclimating to the target area.  ","This approach ascertains that the most applicable instances are chosen, bettering the model's incremental alignment to the target scope. The distance tally can be actualized at either the token grade or the grammar rule grade by calibrating the set to mimic token diffusion or grammar rule diffusion, respectively. The grammar edicts we harness encompass both terminal and non-terminal edicts. Our instance culling workflow implicates three strata of criteria: token, confidence, and grammar edict. We also coalesce the two apex-performing criteria, explicitly confidence-based culling and grammar-edict-based culling, effectuating a more efficacious criterion for pinpointing apex-quality instances for aligning to the target scope.",A,0
526,Neural Fine-Tuning Search for Few-Shot Learning,"Neural Architecture Search (NAS) is a large topic (Elsken et al., 2019) which we do not attempt to review in detail here. Mainstream NAS aims to discover new architectures that achieve high performance when training on a single dataset from scratch in a manyshot regime. To this end, research aims to develop faster search algorithms (Abdelfattah et al., 2021; Guo et al., 2020; Liu et al., 2019; Xiang et al., 2023), and better search spaces (Ci et al., 2021; Fang et al., 2020; Radosavovic et al., 2019; Zhou et al., 2021). We build upon the popular SPOS (Guo et al., 2020) family of search strategies that encapsulate the entire search space inside a supernet that is trained by sampling paths randomly, and a search algorithm then determines the optimal path.","Neural Architecture Search (NAS) is a broad subject (Elsken et al., 2019) that we will not try to extensively review here. Mainstream NAS seeks to find new architectures that perform well when trained from scratch on a single dataset with ample data. Thus, research strives to create faster search algorithms (Abdelfattah et al., 2021; Guo et al., 2020; Liu et al., 2019; Xiang et al., 2023), and superior search spaces (Ci et al., 2021; Fang et al., 2020; Radosavovic et al., 2019; Zhou et al., 2021). We utilize the popular SPOS (Guo et al., 2020) family of search approaches that represent the whole search space within a supernet trained by randomly sampling paths, and a search algorithm then chooses the optimal path.","Neural architecture search (NAS) covers a wide range of work (Elsken et al., 2019) that we will not attempt to comprehensively summarize here. Mainstream NAS aims to develop novel architectures that achieve high accuracy when trained from the beginning on a single dataset with abundant examples. Thus, research works to build faster search algorithms (Abdelfattah et al., 2021; Guo et al., 2020; Liu et al., 2019; Xiang et al., 2023), and more effective search spaces (Ci et al., 2021; Fang et al., 2020; Radosavovic et al., 2019; Zhou et al., 2021). We leverage the popular SPOS (Guo et al., 2020) family of search methods that encapsulate the full search space within a supernet trained by randomly sampling paths, and a search algorithm then selects the optimal path.","Neural architecture search (NAS) encompasses a large body of work (Elsken et al., 2019) that we will not try to comprehensively review here. Mainstream NAS aims to develop new architectures that achieve high performance when trained from scratch on a single dataset with plenty of examples. Thus, research aims to construct faster search algorithms (Abdelfattah et al., 2021; Guo et al., 2020; Liu et al., 2019; Xiang et al., 2023), and better search spaces (Ci et al., 2021; Fang et al., 2020; Radosavovic et al., 2019; Zhou et al., 2021). We build on the popular SPOS (Guo et al., 2020) family of search approaches that represent the full search space within a supernet trained by randomly sampling paths, and a search algorithm then selects the best path.",A,0
104,Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering,"We use XLM-RoBERTa-base (Conneau et al., 2020) as the backbone to train a source model on SQuAD (Rajpurkar et al., 2016) and evaluate it on NaturalQA (Kwiatkowski et al., 2019), which is a cross-domain setting. We compare the results of direct inference and Tent. We experimented with various optimizers and learning rates for Tent, as illustrated in Figure 9. We find that no matter what kind of optimizer and what learning rate we set, the performance of the model will decrease.","We utilize XLM-RoBERTa-base (Conneau et al., 2020) as the foundation to educate a source model on SQuAD (Rajpurkar et al., 2016) and assess it on NaturalQA (Kwiatkowski et al., 2019), which is a cross-domain configuration. We contrast the outcomes of direct deduction and Tent. We tried different enhancers and learning rates for Tent, as delineated in Figure 9. We find that regardless of what kind of enhancer and what learning rate we set, the exhibition of the model will decline.","We employ XLM-RoBERTa-base (Conneau et al., 2020) as the core component to train an original model on SQuAD (Rajpurkar et al., 2016) and evaluate its performance on NaturalQA (Kwiatkowski et al., 2019), which represents a cross-domain scenario. We make comparisons between the results obtained from direct inference and Tent. We conducted experiments with various optimizers and learning rates for Tent, as shown in Figure 9. We determine that irrespective of the type of optimizer and learning rate we choose, the capability of the model will decrease.  ","We make use of XLM-RoBERTa-base (Conneau et al., 2020) as the backbone to instruct an initial model on SQuAD (Rajpurkar et al., 2016) and appraise its effectiveness on NaturalQA (Kwiatkowski et al., 2019), which constitutes a cross-domain case. We juxtapose the outputs of direct deduction and Tent. We explored varying optimizers and learning velocities for Tent, as depicted in Figure 9. We conclude that no matter the variety of optimizer and learning rate we select, the proficiency of the model will diminish.",A,0
114,Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering,"Although OIL can alleviate model collapse with imitation learning from the mean teacher, there will still be significant performance degradation, at most from 46.17% to 40.98% on EM. Even the latest baseline SAR cannot completely avoid model collapse. However, Anti-CF has no performance degradation on any dataset, avoiding the model collapse that other TTA methods may encounter. We also plot Anti-CF’s start position distribution (Figure 1(c)) and entropy of NatrualQA (Figure 2). We note that the entropy on Anti-CF decreases slowly compared to Tent, and the predictions avoid completely lean towards the majority of the labels. This corroborates that Anti-CF can effectively prevent the model collapse caused by TTA.","While OIL can reduce model collapse using imitation learning from the mean teacher, there will still be significant performance drops, at most from 46.17% to 40.98% on EM. Even the newest baseline SAR cannot fully prevent model collapse. However, Anti-CF has no performance drops on any dataset, avoiding the model collapse that other TTA methods might encounter. We also graph Anti-CF's start position distribution (Figure 1(c)) and entropy of NatrualQA (Figure 2). We see that the entropy on Anti-CF slowly decreases compared to Tent, and the predictions avoid completely favoring the majority of the labels. This supports that Anti-CF can effectively prevent the model collapse caused by TTA.","Although OIL can mitigate model degradation with imitation learning from the mean teacher, there will still be major performance declines, at most from 46.17% to 40.98% on EM. Even the most recent baseline SAR cannot completely prevent model degradation. However, Anti-CF has no performance declines on any dataset, avoiding the model degradation that other TTA methods may face. We also plot Anti-CF's start position distribution (Figure 1(c)) and entropy of NatrualQA (Figure 2). We observe that the entropy on Anti-CF slowly reduces compared to Tent, and the predictions avoid completely leaning towards the majority of the labels. This corroborates that Anti-CF can effectively prevent the model degradation caused by TTA.","While OIL can alleviate model deterioration with imitation learning from the mean teacher, there will still be significant performance drops, at most from 46.17% to 40.98% on EM. Even the most advanced baseline SAR cannot fully avoid model deterioration. However, Anti-CF has no performance drops on any dataset, avoiding the model deterioration that other TTA methods may encounter. We also plot Anti-CF's start position distribution (Figure 1(c)) and entropy of NatrualQA (Figure 2). We note that the entropy on Anti-CF decreases slowly compared to Tent, and the predictions avoid completely favoring the majority of the labels. This supports that Anti-CF can effectively prevent the model deterioration caused by TTA.",A,0
291,"Explain, Edit, Generate","This setting poses some unique challenges, such as requiring to identify the causal features to be edited, ensuring sound logical relations in evidence editing and claim generation, and avoiding unverifiable claims. Meanwhile, ensuring the semantic diversity and the minimal perturbation of the counterfactuals can also be challenging. To this end, we propose a general pipeline, RACE, to tackle these challenges. As shown in Figure 1, our RACE consists of four stages: (I) Explainer: rationale extraction (§3.1), (II) Editor: evidence editing (§3.2), (III) Generator: claim generation (§3.3), (IV) Filtering (§3.4). Note that our method handles SUP and REF instances differently, as the large difference in generation space between these two types of instances.","This environment presents some unique difficulties, such as needing to pinpoint the causal aspects to be modified, making sure the logical connections in evidence editing and claim creation are sound, and avoiding unconfirmable claims. Meanwhile, ensuring the semantic diversity and minimal disturbance of the counterfactuals can also be tricky. For this purpose, we put forward a general workflow, RACE, to tackle these challenges. As depicted in Figure 1, our RACE is comprised of four phases: (I) Explainer: rationale extraction (§3.1), (II) Editor: evidence editing (§3.2), (III) Generator: claim generation (§3.3), (IV) Filtering (§3.4). Note that our approach handles SUP and REF examples differently, due to the large variance in generation space between these two types of instances.","This situation introduces some unique obstacles, such as having to identify the causal attributes to modify, making certain the logical links in evidence alteration and claim formulation are valid, and avoiding unverifiable assertions. Meanwhile, guaranteeing the semantic diversity and minimal disturbance of the counterfactuals can also be difficult. To accomplish this, we present a general pipeline, RACE, to address these challenges. As shown in Figure 1, our RACE is made up of four steps: (I) Explainer: rationale extraction (§3.1), (II) Editor: evidence editing (§3.2), (III) Generator: claim generation (§3.3), (IV) Filtering (§3.4). Note that our approach processes SUP and REF examples differently, because of the large difference in generation space between these two types of examples.  ","This context introduces some unique hurdles, such as having to pinpoint the causal features to edit, ensuring rational logical connections in evidence modification and claim formulation, and avoiding unconfirmable assertions. Meanwhile, guaranteeing the semantic diversity and minimal alteration of the counterfactuals can also be tricky. For this purpose, we present a general workflow, RACE, to tackle these challenges. As depicted in Figure 1, our RACE consists of four phases: (I) Explainer: rationale extraction (§3.1), (II) Editor: evidence editing (§3.2), (III) Generator: claim generation (§3.3), (IV) Filtering (§3.4). Note that our method handles SUP and REF instances differently, due to the large variance in generation space between these two types of examples.",A,0
92,ALDi Quantifying the Arabic Level of Dialectness of Text,"As implied by our previous experiment, the Token DI model acts as a sentence level DI model, tagging all the tokens as dialectal if only one token shows a distinctive dialectal feature. This behavior might be an artifact of the model’s fine-tuning dataset, where annotators were asked to use the surrounding context to determine an ambiguous token’s language (EGY or MSA). Conversely, the Sentence ALDi model provides a more nuanced distinction between the different features. The negation form (F4, F5) used in Egyptian Arabic seems to cause the model to categorically consider the sentence as highly dialectal.","Our prior test hinted that the Token DI system judges every word as having dialectal features if even one token displays a distinctive dialectal trait. This may be due to the fine-tuning data used to train the model, where human labelers had to use context to decide the language of unclear tokens (EGY or MSA). In contrast, the Sentence ALDi system makes more subtle differentiations between the features. The Egyptian Arabic way of negating (F4, F5) appears to make the system view the whole sentence as very dialectal.","Our earlier experiment suggested the Token DI program tags all words as dialectal if a single word shows a dialectal characteristic. This could stem from the model's fine-tuning information, where people labeled ambiguous words' language (EGY or MSA) using surrounding context. On the flip side, the Sentence ALDi program distinguishes the traits more precisely. The Egyptian Arabic negation form (F4, F5) seems to cause the program to see the sentence as very dialectal overall.","Our prior analysis hinted the Token DI algorithm labels every term dialectal if any one term displays a distinctive dialectal attribute. This might originate from the algorithm's fine-tuning data, where human reviewers had to leverage context to decide unclear terms' language (EGY or MSA). In contrast, the Sentence ALDi algorithm differentiates the attributes more delicately. The Egyptian Arabic negation method (F4, F5) appears to cause the algorithm to view the whole sentence as highly dialectal.",A,0
529,Neural Fine-Tuning Search for Few-Shot Learning,"Besides the gradient-based few-shot adaptation methods mentioned in Section 4, an alternative line of work (Requeima et al., 2019; Bateni et al., 2020) uses feed-forward networks to modulate the feature extraction process. However, these dynamic feature extractors are less able to generalise to completely novel domains than gradient-based methods (Finn & Levine, 2018), as the adaptation module itself suffers from an out of distribution problem.","In addition to the gradient-dependent few-shot adaptation techniques discussed in Section 4, another approach (Requeima et al., 2019; Bateni et al., 2020) utilizes feedforward networks to regulate the feature extraction process. However, these dynamic feature extractors are less capable of generalizing to entirely new domains compared to gradient-based approaches (Finn & Levine, 2018), since the adaptation module itself faces an out of distribution issue.","Aside from the gradient-reliant few-shot adaptation procedures stated in Section 4, there is another line of research (Requeima et al., 2019; Bateni et al., 2020) that employs feedforward networks to control the feature extraction workflow. Though, those dynamic feature extractors are less able to extend to fully novel areas versus gradient-founded tactics (Finn & Levine, 2018), given that the adaptation component itself contends with an out of distribution predicament. ","In supplement to the gradient-hinged few-shot adaptation techniques outlined in Section 4, an alternate approach (Requeima et al., 2019; Bateni et al., 2020) harnesses feedforward networks to modulate the feature extraction progression. However, those dynamic feature extractors are less capable of expanding to entirely fresh domains relative to gradient-rooted methods (Finn & Levine, 2018), since the adaptation unit itself grapples with an out of distribution issue.",A,0
89,ALDi Quantifying the Arabic Level of Dialectness of Text,"Each translation was then validated by 3 judges. For our analysis, we discard samples having a non-perfect validation confidence score, and ones that still have a distinctive dialectal lexical term in their MSA translations. The distribution of the ALDi scores in Figure 3 reveals that MSA Lexicon does not discriminate strongly between MSA and DA, while Token DI mostly assigns scores of 0 or 1 (acting like Sentence DI), despite the possibility to do otherwise.","Every translation was then confirmed by 3 evaluators. For our review, we ignore examples having a non-flawless validation certainty metric, and ones that still contain a distinctive dialectal lexical expression in their MSA translations. The distribution of the ALDi totals in Figure 3 shows that MSA Lexicon does not strongly differentiate between MSA and DA, while Token DI mostly designates values of 0 or 1 (behaving like Sentence DI), despite the chance to do otherwise.","Each translation was then checked by 3 judges. For our examination, we do not include samples having a non-perfect validation confidence number, and ones that still contain a distinctive dialectal word in their MSA translations. The spread of the ALDi marks in Figure 3 demonstrates that MSA Lexicon does not strongly separate MSA and DA, while Token DI largely assigns scores of 0 or 1 (acting similarly to Sentence DI), despite the ability to do differently.  ","Every translation was then evaluated by 3 appraisers. For our analysis, we exclude examples having a non-ideal validation certainty rating, and ones that still hold a distinctive dialectal term in their MSA translations. The allocation of the ALDi totals in Figure 3 reveals that MSA Lexicon does not strongly differentiate between MSA and DA, while Token DI mostly designates values of 0 or 1 (behaving akin to Sentence DI), despite the capacity to do otherwise.",A,0
446,LLM-enhanced Self-training for Cross-domain Constituency Parsing,"Constituency parsing, a fundamental task in natural language processing (NLP), has achieved remarkable progress on in-domain benchmarks (Liu and Zhang, 2017; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019; Cui et al., 2022), indicating the growing competence of parsers in capturing the underlying syntactic structures. However, opendomain constituency parsing is notably challenging (Fried et al., 2019; Yang et al., 2022). In diverse, open-domain scenarios, constituency parsing faces complexities beyond the well-defined task. Addressing these challenges is crucial for its broader real-world NLP applications. ","Constituency parsing, an essential job in natural language processing (NLP), has made impressive improvements on benchmarks within a specific domain (Liu and Zhang, 2017; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019; Cui et al., 2022), showing the increasing ability of parsers to grasp the fundamental syntactic structures. However, constituency parsing in an open domain is notably difficult (Fried et al., 2019; Yang et al., 2022). In varied, open-domain situations, constituency parsing encounters intricacies beyond the well-defined task. Tackling these challenges is vital for its wider real-world NLP uses.","Constituency parsing, a key duty in natural language processing (NLP), has achieved remarkable advancements on tests within a certain field (Liu and Zhang, 2017; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019; Cui et al., 2022), indicating the growing skill of parsers in seizing the underlying syntactic structures. However, constituency parsing in an open field is especially tricky (Fried et al., 2019; Yang et al., 2022). In diverse, open-field cases, constituency parsing faces intricacies beyond the well-defined duty. Addressing these challenges is crucial for its broader real-life NLP functions.  ","Constituency parsing, a fundamental responsibility in natural language processing (NLP), has made impressive improvements on evaluations within a specific area (Liu and Zhang, 2017; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019; Cui et al., 2022), displaying the increasing competence of parsers in grasping the basic syntactic structures. However, constituency parsing in an open territory is particularly difficult (Fried et al., 2019; Yang et al., 2022). In varied, open-territory situations, constituency parsing encounters complexities beyond the well-defined responsibility. Tackling these challenges is vital for its wider real-world NLP roles.",A,0
683,Standardizing Distress Analysis,"We run our experiments for 200 epochs and report the averaged scores after 5 runs of the experiments to account for the non-determinism of Tensorflow GPU operations.  A.4 Baselines We discuss the details of the considered baselines below.  Similar to the DICE approach, to adapt the baselines to our multi-task scenario, we add a linear layer on top of the hidden-states output in the output layer of the CE task to calculate span start and end logits.  The output layer for the CE task employs sigmoid activation, in which the threshold value is set at 0.4. ","We execute our tests for 200 cycles and document the mean results after 5 executions of the tests to make up for the randomness of Tensorflow GPU actions. A.4 Reference Points We examine the specifics of the referenced starting points below. Identical to the DICE system, to tailor the reference points to our multi-objective situation, we append a linear layer on top of the hidden-states production in the output layer of the CE assignment to figure span start and end logits. The output layer for the CE task uses sigmoid activation, where the threshold value is fixed at 0.4.","We carry out our experiments for 200 epochs and report the averaged scores after conducting the experiments 5 times to account for the non-determinism of Tensorflow GPU operations. A.4 Baselines We go over the details of the considered baselines below. Similar to the DICE method, to adapt the baselines to our multi-task scenario, we add a linear layer on top the hidden-states output in the output layer of the CE task to calculate span start and end logits. The output layer for the CE task utilizes sigmoid activation, in which the threshold value is set to 0.4.","We run our tests for 200 cycles and document the mean marks after 5 executions of the tests to compensate for the randomness of Tensorflow GPU actions. A.4 Reference Points We discuss the specifics of the referenced starting points below. Like the DICE approach, to fit the reference points to our multi-objective situation, we append a linear layer on top of the hidden-states production in the output layer of the CE assignment to figure span start and end logits. The output layer for the CE task employs sigmoid activation, where the threshold value is fixed at 0.4.",A,0
731,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"In this work, we choose RoBERTa as the MLM in our implementation. As described in Section 3.2.1, only candidates with a low edit distance from wj are appropriate replacements. Therefore, we eliminate candidate tokens that have an edit distance exceeding a certain threshold. Finally, we sample wr from the remaining candidates using a pre-defined distribution solely based on the edit distance. To circumvent the problem of consistently sampling the same high-frequency tokens for insertion and deletion errors, we design a smoothing function to smooth the frequency of tokens in the vocabulary. This process is detailed in Algorithm 1. ","For this project, we select RoBERTa to be the MLM in our system. As explained in Section 3.2.1, only candidates that have a small edit distance from wj make suitable substitutions. Thus, we remove any potential tokens whose edit distance is higher than a particular limit. After that, we randomly pick wr from the remaining options using a pre-determined distribution that only considers the edit distance. To avoid the issue of consistently sampling the same common tokens for insertions and deletions, we create a smoothing function that makes the frequencies of tokens in the vocabulary more uniform. The process is described in Algorithm 1.","In our work, we utilize RoBERTa as the MLM. As stated in Section 3.2.1, candidates with low edit distances from wj are the only appropriate replacements. Hence, we take out candidates whose edit distance is over a threshold. Subsequently, we draw wr from the remaining candidates based solely on a predefined distribution of the edit distance. To get around the problem of always sampling the same high-frequency tokens for insertions and deletions, we construct a smoothing function that evens out the frequencies of tokens in the vocabulary. This procedure is laid out in Algorithm 1.  ","For this study, we employ RoBERTa as the MLM. As elucidated in Section 3.2.1, only candidates with small edit distances from wj are fitting substitutions. Accordingly, we exclude candidate tokens exceeding a particular edit distance threshold. We then sample wr from the remaining candidates using a pre-determined distribution based solely on edit distance. To circumvent the issue of persistently sampling identical high-frequency tokens for insertions and deletions, we devise a smoothing function that makes the frequencies of vocabulary tokens more uniform. This process is delineated in Algorithm 1.",A,0
753,Unsupervised Grammatical Error Correction Rivaling Supervised Methods,"For English GEC, we use 32 NVIDIA A100 GPUs. For Chinese GEC, we use 8 NVIDIA A100 GPUs. The experiments took 14 days for English and 2 days in total for Chinese. We use the default training configuration under different toolkits unless otherwise stated. The detailed training configurations for English and Chinese are shown in Table 8 and Table 9, respectively. The best checkpoint is selected based on the performance on the validation set. Specifically, when building the fixer, we follow Yasunaga and Liang (2021) to randomly sample 5,000 sentences from the obtained training sentence pairs as the validation data for both English and Chinese. ","We utilize 32 NVIDIA A100 GPUs for English grammatical error correction. For Chinese grammatical error correction, we employ 8 NVIDIA A100 GPUs. It took 14 days total for the English experiments and 2 days total for the Chinese experiments. We apply the default training settings under the various toolkits unless we say otherwise. Table 8 and Table 9 show the precise training arrangements for English and Chinese respectively. We choose the best checkpoint based on the validation set performance. Specifically, when constructing the fixer, we follow Yasunaga and Liang (2021) in randomly sampling 5,000 sentences from the acquired training sentence pairs to use as the validation data for both English and Chinese.","For English grammatical error correction, our experiments use 32 NVIDIA A100 GPUs. For Chinese grammatical error correction, we utilize 8 NVIDIA A100 GPUs. The total time taken was 14 days for English and 2 days for Chinese. We utilize the standard training configuration with each toolkit unless we state otherwise. Table 8 and Table 9 display the detailed training settings for English and Chinese correspondingly. We select the optimal checkpoint according to the validation set results. In particular, when developing the fixer, we follow the approach of Yasunaga and Liang (2021) by randomly sampling 5,000 sentences from the obtained training sentence pairs to employ as the validation data for both English and Chinese.  ","We make use of 32 NVIDIA A100 GPUs for English grammatical error correction experiments. For Chinese grammatical error correction experiments, we utilize 8 NVIDIA A100 GPUs. The English experiments took a total of 14 days, while the Chinese experiments took 2 days total. We apply the default training configuration for each toolkit unless specified otherwise. Table 8 and Table 9 show the precise training parameters for English and Chinese respectively. We choose the best checkpoint based on validation set performance. Specifically, when creating the fixer, we follow the method of Yasunaga and Liang (2021) by randomly selecting 5,000 sentences from the acquired training sentence pairs to serve as validation data for both English and Chinese.",A,0
479,Model Tells you what to Discard - Adaptive KV Cache Compression for LLMs,"When memory usage exceeds GPU capacity, the generative inference of LLMs typically resort to offloading (Aminabadi et al., 2022; Sheng et al., 2023). While these methods help mitigate the pressure on the scarce GPU memory from using KV cache, offloading KV cache to CPU/NVMe can still add non-trivial overhead to generative inference performance due to the limited PCIe bandwidth between the GPU and CPU on many devices. Therefore, it becomes a crucial task to reduce the memory footprint of KV cache without costly retraining or fine-tuning.","As GPU memory fills up, generative models like large language models often need to offload data to the CPU or disk (Aminabadi et al., 2022; Sheng et al., 2023). Though offloading helps with the limited GPU memory, it can slow things down because of bottlenecks moving data between the GPU and other parts of the system. So it's important to shrink the GPU memory footprint without expensive retraining or fine-tuning.","When GPU memory is exceeded, generative models such as large language models typically need to move data elsewhere like the CPU or disk (Aminabadi et al., 2022; Sheng et al., 2023). While this mitigates GPU memory scarcity from using cache, offloading cache can still hamper performance due to constrained bandwidth between the GPU and CPU. Thus, decreasing cache memory usage without costly retraining is crucial.  ","As generative models like large language models surpass GPU capacity, they often must offload data to the CPU or disk (Aminabadi et al., 2022; Sheng et al., 2023). Though this eases pressure on limited GPU memory, performance can suffer from moving cache to slower storage. With bandwidth between GPU and CPU constrained on many devices, reducing cache footprint without retraining is key.",A,0
11,A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We base our experiments on WinoMT (Stanovsky et al., 2019), a well-known benchmark for evaluating gender bias in MT. The collection is based on templates. Each instance mentions two professions and a pronoun coreferent to one of them (see Figure 1 for an example). When translating from English, a notional gender language, to Spanish or German, two grammatical gender languages, the pronoun dictates the coreferent inflection because of syntactic agreement.","Our experiments use WinoMT (Stanovsky et al., 2019) as a starting point, which is a widely recognized benchmark for analyzing gender bias in machine translation. The dataset utilizes templates, where each sample has two occupations and a pronoun referring to one of them (see Figure 1 for an illustration). Translating from English, which does not have grammatical gender, into Spanish or German, which do have grammatical gender, means the pronoun determines the inflected form of the coreferent due to syntactic agreement.","We conduct our experiments utilizing WinoMT (Stanovsky et al., 2019) as a basis, which is a well-known standard for evaluating gender prejudice in machine translation. The collection utilizes templates, with each case mentioning two professions and a pronoun referring to one of them (refer to Figure 1 for a sample). When translating from English, a language without grammatical gender, into Spanish or German, languages with grammatical gender, the pronoun necessitates the coreferent inflection owing to syntactic concord. ","Our experiments leverage WinoMT (Stanovsky et al., 2019) as a foundation, which is an established benchmark for assessing gender bias in machine translation. The dataset employs templates, where each example contains two occupations and a pronoun referring to one of them (see Figure 1 for a sample). Translating from English, which lacks grammatical gender, to Spanish or German, which have grammatical gender, means the pronoun determines the inflected form of the coreferent because of syntactic agreement.",A,0
340,Fifty Shades of Bias,"Standard inter-annotator agreement measures are inadequate for evaluating the quality of comparative annotations. Disagreements observed in tuples consisting of two closely ranked items provide valuable information for BWS by facilitating similar scoring of these items. Therefore, following best practices, we compute average split-half reliability (SHR) values to asses the reproducibility of the annotations and the final ranking. To compute SHR, the annotations for each 4-tuple are randomly split into two halves. Using these two splits, two sets of rankings are determined. We then calculate the correlation values between these two sets.","Typical methods for assessing consistency between annotators are not suitable for judging the quality of comparative annotations. Differences found in pairs of items ranked very closely together give useful insights in BWS by enabling comparable scoring of these items. So, adhering to best practices, we figure average split-half dependability (SHD) scores to evaluate the replicability of the annotations and final ranking. To get SHD, the annotations for each 4-tuple are arbitrarily divided into two halves. Applying these two splits, two ranking sets are generated. We then compute the correlation values between these two sets.","Standard techniques for measuring agreement among annotators are inadequate for assessing the quality of relative annotations. Variations detected in tuples of two closely ranked entities provide valuable information for BWS by allowing similar scoring of these entities. Thus, per best practices, we determine mean split-half consistency (SHC) values to judge the repeatability of the annotations and ultimate ranking. To get SHC, the annotations for each 4-tuple are randomly split into two halves. Employing these two splits, two ranking sets are produced. We then calculate the correlation values between these two sets.","Conventional inter-rater agreement metrics are unsuitable for evaluating the quality of comparative marks. Discrepancies found in pairs of closely ranked items give useful insights in BWS by permitting comparable scoring of these items. Hence, following best practices, we establish average split-half reliability (SHR) figures to appraise the reproducibility of the annotations and final ranking. To obtain SHR, the annotations for each 4-tuple are randomly divided into two halves. Using these two splits, two ranking sets are generated. We then compute the correlation values between these two sets.",A,0
43,Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"For each slot type, the slot label matrix is obtained by averaging over the representations of the slot label tokens. Unlike the conventional discrete and static label embeddings (Liu et al., 2020b; Siddique et al., 2021; Ma et al., 2022) that capture the semantics of each textual label separately, we attempt to build the label-utterance correlation, and the adaptive interaction between the slot labels and utterance tokens encourages the model to learn the context-aware soft label embeddings dynamically, which will be exploited as the supervision information for the metric learning.","The slot label matrix for each slot type is created by taking the mean of the representations of the slot label words. In contrast to the standard discrete and fixed label embeddings (Liu et al., 2020b; Siddique et al., 2021; Ma et al., 2022) that encode the meaning of each textual label individually, we try to build the label-utterance connection. The flexible interaction between the slot labels and utterance words prompts the model to learn the context-sensitive soft label embeddings dynamically, which will be used as the supervision data for the metric learning.","For every slot type, the slot label matrix is generated by calculating the average of the vector representations of the slot label terms. Unlike the traditional discrete and static label embeddings (Liu et al., 2020b; Siddique et al., 2021; Ma et al., 2022) that capture the semantics of each text label separately, we aim to construct the correlation between labels and utterances. The adaptive interaction between slot labels and utterance tokens causes the model to learn context-aware soft label embeddings dynamically, which will be leveraged as supervision information for metric learning.","The slot label matrix for each slot type is obtained by taking the mean of the embeddings of the individual slot label words. In contrast to conventional discrete and fixed label embeddings (Liu et al., 2020b; Siddique et al., 2021; Ma et al., 2022) that encode the meaning of each text label in isolation, we seek to build the relationship between labels and utterances. The flexible interaction between slot labels and utterance tokens prompts the model to dynamically learn context-sensitive soft label embeddings, which will serve as supervision signal for metric learning.",A,0
983,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture. Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed. Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for a more detailed introduction. ","The Transformer was first demonstrated to be useful for automatic translation between languages, but it has since been utilized in many different natural language processing applications (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Because it is becoming so common, all the models we examine here are built on the Transformer design. Other than the specifics noted below and the variations we investigate in Section 3.2, we do not diverge substantially from the original Transformer architecture. Rather than giving a full definition of this model, we point interested readers to the original paper (Vaswani et al., 2017) or subsequent tutorials3,4 for a more in-depth introduction.","The Transformer was initially shown to be effective at machine translation, but subsequently it has been employed in many different NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Owing to its expanding ubiquity, all the models we analyze are founded on the Transformer structure. Aside from the particulars mentioned below and the variants we explore in Section 3.2, we do not deviate considerably from this architecture as first proposed. Instead of providing an exhaustive definition of this model, we refer curious readers to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for a more thorough introduction.","The Transformer was first demonstrated to be useful for automated translation between languages, and has since been applied in many different natural language processing tasks (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Because it is becoming very common, all the models we study here are constructed using the Transformer design. Except for the details noted below and the variations we investigate in Section 3.2, we do not diverge significantly from the original Transformer architecture. Rather than providing a comprehensive definition of this model, we point interested readers to the original paper (Vaswani et al., 2017) or subsequent tutorials3,4 for a more in-depth introduction.",A,1
1047,Generative Adversarial Nets,"Until recently, most work on deep generative models focused on models that provided a parametric specification of a probability distribution function. The model can then be trained by maximizing the log likelihood. In this family of model, perhaps the most succesful is the deep Boltzmann machine [25]. Such models generally have intractable likelihood functions and therefore require numerous approximations to the likelihood gradient. These difficulties motivated the development of “generative machines”–models that do not explicitly represent the likelihood, yet are able to generate samples from the desired distribution.","In the past, most research on deep generative models concentrated on models that gave a parametric definition of a probability distribution function. These models could then be trained by maximizing the log likelihood. In this type of model, perhaps the most successful is the deep Boltzmann machine [25]. Such models usually have intractable likelihood functions and thus need many approximations to the likelihood gradient. These problems led to the creation of ""generative machines"" - models that do not explicitly show the likelihood, but can still generate samples from the desired distribution.","Until recently, a lot of work on deep generative models was focused on models that provided a parametric specification of a probability distribution function. The model could then be trained by maximizing the log likelihood. In this family of models, maybe the most succesful was the deep Boltzmann machine [25]. Such models tend to have likelihood functions that are intractable and therefore need many approximations to the likelihood gradient. These difficulties motivated the development of “generative machines”– models that do not explicitly represent the likelihood, yet are able to generate samples from the desired distribution.","In the past, most research on deep generative models concentrated on models that provided a parametric definition of a probability distribution function. The model could then be trained by maximizing the log likelihood. In this type of model, perhaps the most successful was the deep Boltzmann machine [25]. Such models generally have likelihood functions that cannot be solved and therefore require many approximations to the likelihood gradient. These problems led to the creation of ""generative machines"" - models that do not explicitly show the likelihood, but can still generate samples from the desired distribution.",A,1
826,Bag of Tricks for Efficient Text Classification,"To test scalability of our approach, further evaluation is carried on the YFCC100M dataset (Thomee et al., 2016) which consists of almost 100M images with captions, titles and tags. We focus on predicting the tags according to the title and caption (we do not use the images). We remove the words and tags occurring less than 100 times and split the data into a train, validation and test set. The train set contains 91,188,648 examples (1.5B tokens). The validation has 930,497 examples and the test set 543,424. The vocabulary size is 297,141 and there are 312,116 unique tags. We will release a script that recreates this dataset so that our numbers could be reproduced. We report precision at 1.","In order to evaluate the scalability of our method, we performed more assessments using the YFCC100M dataset (Thomee et al., 2016). This dataset contains close to 100 million images along with captions, titles, and tags. We focused on predicting the tags using the title and caption only (without using the images). We removed words and tags that occurred less than 100 times and divided the data into training, validation, and test sets. The training set had 91,188,648 examples (1.5 billion tokens). The validation set had 930,497 examples and the test set had 543,424. There were 297,141 unique vocabulary words and 312,116 unique tags. We will make available a script that recreates this dataset so that our results can be reproduced. We report precision at 1.","To evaluate how well our approach scales, we conducted further experiments using the YFCC100M dataset (Thomee et al., 2016). This dataset has nearly 100 million images, each with captions, titles, and tags. We focused on predicting the tags from the title and caption only, without using the images. We filtered out words and tags occurring less than 100 times, and split the data into training, validation, and test sets. The training set had 91,188,648 examples (1.5 billion tokens). The validation set had 930,497 examples and the test set had 543,424. There was a vocabulary of 297,141 words and 312,116 unique tags. We will release a script to recreate this dataset so our results can be reproduced. We report precision at 1.","In order to test how well our approach handles large amounts of data, we did additional evaluation using the YFCC100M dataset (Thomee et al., 2016). This dataset contains close to 100 million images, each with a caption, title, and tags. We focused on predicting the tags from just the title and caption, without using the images. We removed words and tags occurring less than 100 times, and split the data into training, validation, and test sets. The training set contained 91,188,648 examples (1.5 billion tokens). The validation set had 930,497 examples and the test set had 543,424. There were 297,141 unique words in the vocabulary and 312,116 unique tags. We will provide a script to recreate this dataset so our results can be replicated. We report precision at 1.",A,1
1236,Language Models are Unsupervised Multitask Learners,"Trinh & Le (2018) used Common Crawl in their work on commonsense reasoning but noted a large amount of documents “whose content are mostly unintelligible”. We observed similar data issues in our initial experiments with Common Crawl. Trinh & Le (2018)’s best results were achieved using a small subsample of Common Crawl which included only documents most similar to their target dataset, the Winograd Schema Challenge. While this is a pragmatic approach to improve performance on a specific task, we want to avoid making assumptions about the tasks to be performed ahead of time. Instead, we created a new web scrape which emphasizes document quality.","Trinh and Le utilized Common Crawl for their research on reasoning and logic but pointed out that many of the documents were mostly incomprehensible. We saw the same problems with meaningless data when we first tried using Common Crawl. Trinh and Le got their best results by only using a small part of Common Crawl that was most similar to the Winograd Schema Challenge dataset they were working with. While that is a practical way to do better on one particular task, we want to avoid assuming what the tasks will be beforehand. So instead, we made a new web scrape that focuses on document quality.","Trinh and Le made use of Common Crawl in their work on common sense reasoning however they highlighted that there was a lot of content that was largely unintelligible. We encountered comparable data quality problems when we initially tested Common Crawl. Trinh and Le achieved their top performance by only using a subsample of Common Crawl containing documents very similar to their target dataset, the Winograd Schema Challenge. Even though that is a sensible tactic for improving results on one specific task, we want to avoid presuming what the tasks will be in advance. Rather, we created a new web scrape prioritizing document quality.  ","Trinh and Le utilized Common Crawl for their research into commonsense reasoning but pointed out many documents had content that was mostly incomprehensible. We saw similar data quality issues when we first experimented with Common Crawl. Trinh and Le obtained their best results by using just a small part of Common Crawl containing documents highly similar to their target dataset, the Winograd Schema Challenge. While that pragmatic approach improves performance on one particular task, we want to avoid predetermining what the tasks will be. Instead, we made a new web scrape emphasizing document quality.",A,1
1382,Transformer-XL,"One feasible but crude approximation is to split the entire corpus into shorter segments of man-ageable sizes, and only train the model within each segment, ignoring all contextual information from previous segments. This is the idea adopted by Al-Rfou et al. (2018). We call it the vanilla model and visualize it in Fig. 1a. Under this training paradigm, information never flows across segments in either the forward or backward pass. There are two critical limitations of using a fixed length context. First, the largest possible dependency length is upper bounded by the segment length, which is a few hundred on character-level language modeling (Al-Rfou et al., 2018).","A possible but unsophisticated way to handle this is to break up the full body of text into smaller, more manageable chunks, and train the model on each chunk separately, not taking into account any context from previous chunks. This is the approach used by Al-Rfou et al. (2018). We refer to this as the basic model and show it in Fig. 1a. With this training method, information never crosses between chunks in either direction. There are two major drawbacks to having a fixed context length. First, the longest attainable dependency is limited by the chunk size, which is a few hundred for character-level language modeling (Al-Rfou et al., 2018).","One rough but workable solution is to split the whole corpus into shorter, more manageable segments and only train the model within those segments, disregarding any contextual data from prior segments. This is the concept used by Al-Rfou et al. (2018). We call this the standard model and illustrate it in Fig. 1a. With this training approach, information never travels between segments in either direction. There are two main limitations to having a fixed context length. First, the maximum possible dependency length is constrained by the segment length, which is a few hundred for character-level language modeling (Al-Rfou et al., 2018).","A basic but viable approximation is to divide the full text corpus into shorter, more manageable sized chunks, and only train the model on each chunk, ignoring any context from previous chunks. This is the idea used by Al-Rfou et al. (2018). We refer to this as the simple model and depict it in Fig. 1a. Under this training method, information never passes between chunks in either the forward or backward direction. There are two major drawbacks to having a fixed context size. First, the longest possible dependency is limited by the chunk size, which is a few hundred for character-level language modeling (Al-Rfou et al., 2018).",A,1
1192,Language Models are Few-Shot Learners,"This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task.","This most recent framework has resulted in considerable improvements on many difficult NLP tasks including reading comprehension, question answering, textual entailment, and more. It has continued to progress thanks to new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a significant restriction of this method is that even though the architecture is task-general, task-particular datasets and task-particular fine-tuning are still required: to attain robust performance on a wanted task usually necessitates fine-tuning on thousands to hundreds of thousands of examples exclusive to that task.","This latest paradigm has led to major advancements on numerous challenging natural language processing tasks such as reading comprehension, question answering, textual entailment, and so on. It has kept improving due to novel architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a considerable limitation of this approach is that although the architecture is not specific to any one task, task-focused datasets and tuning are still needed: achieving strong results on a desired task typically requires tuning on thousands to hundreds of thousands of examples particular to that task.  ","This most recent framework has produced substantial improvements on many tough NLP tasks including reading comprehension, question answering, textual entailment, and more. It has continued advancing thanks to new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a big constraint of this method is that while the architecture is general and not tailored to any specific task, task-centered datasets and tuning are still necessary: attaining strong performance on a target task usually requires tuning on thousands to hundreds of thousands of examples specific to that task.",A,1
1458,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"We have released our two best-performing models 1 to facilitate further research. The rest of the paper is organised as follows. In Sect. 2, we describe our ConvNet configurations. The details of the image classification training and evaluation are then presented in Sect. 3, and the configurations are compared on the ILSVRC classification task in Sect. 4. Sect. 5 concludes the paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B. Finally, Appendix C contains the list of major paper revisions.","We have made public our top two models to enable more research. The remainder of the paper is structured like this. In Section 2, we explain our ConvNet setups. Section 3 then provides the specifics of the image classification training and testing, and Section 4 compares the configurations on the ILSVRC classification task. Section 5 wraps up the paper. For thoroughness, we also depict and evaluate our ILSVRC-2014 object localization system in Appendix A, and examine the generalization of very deep features to other data sets in Appendix B. Appendix C lists the major paper revisions.","We have released our two best-performing models to promote more research. The rest of the article is organized as follows. Section 2 describes our ConvNet architectures. Section 3 then presents the details of image classification training and assessment, and Section 4 compares the configurations on the ILSVRC classification task. Section 5 concludes the article. For completeness, we also portray and appraise our ILSVRC-2014 object localization system in Appendix A, and investigate the extension of very deep features to other data sets in Appendix B. Appendix C contains the list of major article revisions.  ","We have made public our top two models to facilitate additional research. The remainder of the manuscript is structured as follows. Section 2 delineates our ConvNet configurations. The specifics of image classification training and evaluation are then provided in Section 3, and Section 4 compares the configurations on the ILSVRC classification task. Section 5 concludes the manuscript. For thoroughness, we also depict and evaluate our ILSVRC-2014 object localization system in Appendix A, and examine the generalization of very deep features to other datasets in Appendix B. Appendix C enumerates the major manuscript revisions.",A,1
905,Deep Residual Learning for Image Recognition,"Although both forms should be able to asymptotically approximate the desired functions (as hypothesized), the ease of learning might be different. This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left). As we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counterpart. The degradation problem suggests that the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers.","While the two structures ought to be capable of closely estimating the target functions over time (as theorized), the simplicity of acquiring the skill could differ. This rethinking is driven by the unanticipated happenings regarding the deterioration issue (Fig. 1, left). As talked about earlier, if the extra tiers can be made to act as unchanged mappings, a more profound model should train with error less than or equal to its more shallow version. The degradation problem hints that the solvers may struggle to model identity mappings using multiple nonlinear tiers.","Although both forms should eventually be able to closely predict the desired functions (as proposed), the ease of picking up the skill might vary. This re-conception stems from the unexpected phenomena around the degradation problem (Fig. 1, left). As discussed previously, if the added layers can be constructed to act as identity functions, a deeper model ought to learn with error no more than its shallower equivalent. The degradation issue suggests that the solvers may have difficulty approximating identity functions using multiple nonlinear layers.","While both architectures should be capable of approximating the target functions over time (as theorized), the difficulty of acquiring the skill could differ. This rethinking is prompted by the counterintuitive happenings regarding the degradation issue (Fig. 1, left). As mentioned earlier, if the extra layers can be made to behave as identity functions, a deeper model should train with error less than or equal to its shallower version. The degradation problem indicates that the solvers may struggle to approximate identity functions using multiple nonlinear layers.",A,1
1411,U-Net_Convolutional Networks for Biomedical Image Segmentation,"The value of data augmentation for learning invariance has been shown in Dosovitskiy et al. [2] in the scope of unsupervised feature learning. Another challenge in many cell segmentation tasks is the separation of touching objects of the same class; see Figure 3. To this end, we propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function. The resulting network is applicable to various biomedical segmentation problems. In this paper, we show results on the segmentation of neuronal structures in EM stacks (an ongoing competition started at ISBI 2012), where we out-performed the network of Ciresan et al. [1].","The usefulness of augmenting data to learn invariance was demonstrated by Dosovitskiy et al. [2] for unsupervised feature learning. Separating adjoining objects of the same type is another difficulty in many cell segmentation tasks; refer to Figure 3. We suggest utilizing a weighted loss function, where the background labels that separate touching cells are heavily weighted. The resulting network can be used for various biomedical segmentation challenges. Here, we present results segmenting neuronal structures in EM image stacks (a continuing competition begun at ISBI 2012), surpassing the performance of Ciresan et al.'s [1] network.","Dosovitskiy et al. [2] showed that increasing data helps learn invariance, for unsupervised feature learning. Distinguishing merged objects of one class is hard in some cell segmentation; see Figure 3. We recommend a weighted loss function, strongly weighting background tags between adjacent cells. This network applies to multiple biomedical segmentation tasks. We show results segmenting neuron structure in EM stacks (a competition since ISBI 2012), beating Ciresan et al.'s [1] network. ","The usefulness of expanding data to learn invariance was exhibited by Dosovitskiy et al. [2] for unsupervised characteristic learning. Differentiating joined entities of one type is tricky in certain cell division; consult Figure 3. We put forward a weighted loss function, intensely weighting background markers separating adjoining cells. This system is applicable to various biomedical partitioning challenges. We demonstrate results partitioning neurological anatomy in EM piles (a contest since ISBI 2012), surpassing Ciresan et al.'s [1] system.",A,1
896,Deep Residual Learning for Image Recognition,"Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. In this paper, we address the degradation problem by introducing a deep residual learning framework. Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual mapping.","We will think about a neural network with fewer layers compared to a similar network with additional layers stacked on top of it. We can construct a solution for the deeper network where the extra layers do nothing and the original layers are the same as the shallower network. The fact that we can build this solution means the deeper network should not have higher training error compared to the shallower one. In this paper, we tackle the degradation issue by presenting a deep residual learning framework. Rather than expecting a few stacked layers to directly approximate a desired mapping, we explicitly have these layers model the residual mapping.","Let's examine a neural network architecture with limited layers versus a comparable architecture with more layers added on top. There is a way to construct a solution for the deeper model: the supplementary layers are identity mappings, and the rest of the layers are identical to the learned shallower model. The existence of this constructed solution signifies that a deeper model should yield no greater training error than its shallower counterpart. In this paper, we address the degradation problem through introducing a deep residual learning framework. Instead of anticipating that a few stacked layers will directly fit a preferred underlying mapping, we explicitly require these layers to fit a residual mapping.","We will analyze a neural network with a small number of layers and a similar network with additional layers stacked on it. We can create a solution for the deeper network where the extra layers are pass-through and the original layers match the shallower network. Being able to construct this solution means the deeper network should not have higher training error compared to the shallower one. In this paper, we tackle the degradation issue by presenting a deep residual learning framework. Rather than expecting a few stacked layers to directly model a desired mapping, we explicitly require these layers to model the residual mapping.",A,1
853,BERT,"For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no rightside context. In order to make a good faith attempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQuAD, but the results are still far worse than those of the pretrained bidirectional models. The BiLSTM hurts performance on the GLUE tasks.","It is obvious that a left-to-right model will do badly at predicting tokens for SQuAD, because the token-level hidden states don't have any context from the right side. To try in good faith to improve the left-to-right system, we added a randomly initialized bidirectional LSTM on top. This does noticeably improve results on SQuAD, but the results are still much worse than those of the pre-trained bidirectional models. The bidirectional LSTM harms performance on the GLUE tasks.","For SQuAD, it is clear that a model that reads only left-to-right will be poor at predicting tokens, since the token-level hidden states lack rightside context. In an honest attempt to strengthen the left-to-right system, we supplemented it with an untrained bidirectional LSTM. This significantly boosts results on SQuAD, but the results are still far inferior to those of pretrained bidirectional models. The bidirectional LSTM degrades performance on the GLUE benchmarks.  ","It is evident that a left-to-right model will perform inadequately at token prediction for SQuAD, because the token-level hidden states have no context from the right side. In a good faith effort to improve the left-to-right system, we added an untrained bidirectional LSTM on top. This does meaningfully enhance results on SQuAD, but the results are still much worse than those of the pre-trained bidirectional models. The bidirectional LSTM impairs performance on the GLUE tasks.",A,1
1293,RoBERTa_A Robustly Optimized BERT Pretraining Approach,"In the original BERT pretraining procedure, the model observes two concatenated document segments, which are either sampled contiguously from the same document (with p = 0.5) or from distinct documents. In addition to the masked language modeling objective, the model is trained to predict whether the observed document segments come from the same or distinct documents via an auxiliary Next Sentence Prediction (NSP) loss.","The first version of BERT was pretrained by showing it two sections of text joined together. Half the time these were from the same document, and half the time they were from different documents. As well as predicting the masked words, BERT had to predict whether the two sections were from the same document or not. This was an extra task called Next Sentence Prediction that BERT was trained on.","In the original pretraining process for BERT, it would see two parts of text joined together. These parts were either taken from the same document (50% of the time) or from different documents. BERT had two training objectives: masked language modeling to predict masked words, and next sentence prediction where it predicted if the two text parts were from the same document or not. The latter was an auxiliary loss function. ","When BERT was first pretrained, it was shown pairs of text segments that were concatenated together. These segments were sampled either from the same document (50% probability) or from different documents. BERT was trained on two tasks: masked language model prediction to predict masked words, and next sentence prediction where it predicted whether the two segments came from the same document or not. The latter was an extra training loss.",A,1
1106,Going deeper with convolutions,"Such a two stage approach leverages the accuracy of bounding box segmentation with low-level cues, as well as the highly powerful classification power of state-of-the-art CNNs. We adopted a similar pipeline in our detection submissions, but have explored enhancements in both stages, such as multi-box [5] prediction for higher object bounding box recall, and ensemble approaches for better categorization of bounding box proposals.","This two part method takes advantage of the precision of bounding box division using low-level hints, and also the extremely effective categorization capability of cutting-edge CNNs. We utilized a comparable workflow in our detection submissions, but have investigated enhancements in both phases, like multi-box [5] forecasting for superior object bounding box recall, and ensemble tactics for improved classification of bounding box proposals.","This approach in two steps leverages the accuracy of bounding box segmentation using basic visual cues, as well as the highly powerful ability of modern CNNs to categorize objects. We used a similar pipeline in our detection submissions, but we explored improvements in both steps, including multi-box [5] prediction to get higher object bounding box recall, and ensemble methods to better classify the proposed bounding boxes. ","This two stage method takes advantage of precise bounding box delineation using low-level visual features, and the extremely powerful classification abilities of state-of-the-art CNNs. We used a similar workflow for our detection submissions, but investigated enhancements at both stages, such as multi-box [5] forecasting to improve object bounding box recall, and ensemble techniques to better categorize the proposed bounding boxes.",A,1
1086,GloVe_Global Vectors for Word Representation,"The performance of the model depends weakly on the cutoff, which we fix to xmax = 100 for all our experiments. We found that α = 3/4 gives a modest improvement over a linear version with α = 1. Although we offer only empirical motivation for choosing the value 3/4, it is interesting that a similar fractional power scaling was found to give the best performance in (Mikolov et al., 2013a).","The effectiveness of the model is only slightly influenced by the maximum value, which we set to 100 for all of our tests. We determined that using α = 3/4 provides a small boost over using α = 1, which would be a linear model. While we only have experimental results to justify selecting 3/4, it is notable that a similar nonlinear scaling with an exponent was optimal in (Mikolov et al., 2013a).","The model's success depends minimally on the cutoff point, which we establish as 100 for our experiments. We found that setting α = 3/4 gives a modest improvement compared to a linear version with α = 1. Although our rationale for picking 3/4 is empirical, it is interesting that a comparable nonlinear power scaling worked best in (Mikolov et al., 2013a).  ","How well the model works is not very sensitive to the maximum cutoff value, which we use 100 for all tests. Using α = 3/4 provides a small enhancement over a linear model with α = 1. While our motivation for 3/4 is experimental, it is notable that a similar fractional exponent scaling performed optimally in (Mikolov et al., 2013a).",A,1
1449,Universal Language Model Fine-tuning for Text Classification,"In order to assess the impact of each contribution, we perform a series of analyses and ablations. We run experiments on three corpora, IMDb, TREC6, and AG that are representative of different tasks, genres, and sizes. For all experiments, we split off 10% of the training set and report error rates on this validation set with unidirectional LMs. We fine-tune the classifier for 50 epochs and train all methods but ULMFiT with early stopping.","To evaluate the effect of each part, we conduct multiple examinations and removals. We do tests on three groups of data, IMDb, TREC6, and AG which are typical of various tasks, styles, and sizes. For all tests, we separate 10% of the training set and document mistake percentages on this confirmation set with one-directional LMs. We adjust the classifier for 50 epochs and prepare all techniques except ULMFiT with early stopping.","In order to gauge the influence of each contribution, we perform a series of analyses and eliminations. We execute trials on three collections, IMDb, TREC6, and AG which represent different objectives, types, and magnitudes. For all trials, we detach 10% of the training set and document error rates on this validation set with one-way LMs. We fine-tune the classifier for 50 epochs and train all approaches excluding ULMFiT with early halting.","To assess the impact of each part, we do multiple reviews and removals. We conduct experiments on three groups, IMDb, TREC6, and AG that typify various tasks, styles, and sizes. For all experiments, we separate 10% of the training set and report mistake percentages on this confirmation set with single-direction LMs. We adjust the classifier for 50 epochs and prepare all methods except ULMFiT with early ending.",A,1
1002,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Note that an issue arises if our model outputs text on a text classification task that does not correspond to any of the possible labels (for example if the model outputs “hamburger” when the only possible labels for a task were “entailment”, “neutral”, or “contradiction”). In this case, we always count the model’s output as wrong, though we never observed this behavior in any of our trained models. Note that the choice of text prefix used for a given task is essentially a hyperparameter; we found that changing the exact wording of the prefix had limited impact and so did not perform extensive experiments into different prefix choices. ","Be aware that a problem comes up if our model generates text for a text classification assignment that does not match any of the available labels (for instance, if the model outputs ""hamburger"" but the only possible labels for the task were ""entailment"", ""neutral"", or ""contradiction""). In this situation, we always consider the model's output incorrect, although we never saw this occur with any of our trained models. Recognize that the selection of text prefix utilized for a particular task is basically a hyperparameter; we determined that modifying the precise wording of the prefix had minimal effect and therefore did not conduct extensive experiments with different prefix options.","Note that a complication emerges if our model produces text on a text categorization job that does not align with any of the potential tags (for example, if the model generates ""hamburger"" but the only feasible tags for the task were ""entailment"", ""neutral"", or ""contradiction""). In such a case, we invariably deem the model's output fallacious, despite never witnessing this behavior in any of our conditioned models. Acknowledge that the choice of text prefix employed for a given task is fundamentally a hyperparameter; we ascertained that altering the exact verbiage of the prefix had negligible impact and thus did not perform comprehensive experiments on different prefix selections.  ","Understand that an issue materializes if our model formulates text on a text sorting assignment that does not match any of the viable labels (for instance, if the model composes ""hamburger"" however the sole viable labels for the task were ""entailment"", ""neutral"", or ""contradiction""). In said case, we always adjudicate the model's output erroneous, despite never espying this conduct in any of our cultivated models. Appreciate that the picking of text prefix harnessed for a particular task is intrinsically a hyperparameter; we discerned that modifying the precise diction of the prefix had trivial collision and ergo did not enact extensive experiments on discrete prefix choices.",A,1
1051,Generative Adversarial Nets,"Using a previously trained model as the noise distribution allows training a sequence of models of increasing quality. This can be seen as an informal competition mechanism similar in spirit to the formal competition used in the adversarial networks game. The key limitation of NCE is that its “discriminator” is defined by the ratio of the probability densities of the noise distribution and the model distribution, and thus requires the ability to evaluate and backpropagate through both densities. Some previous work has used the general concept of having two neural networks compete.","Leveraging a pre-trained model as the noise distribution enables training a sequence of progressively better models. This can be viewed as an informal competition mechanism analogous in essence to the formal competition utilized in adversarial networks. A primary constraint of NCE is its ""discriminator"" being characterized by the proportion of the probability densities of the noise distribution and model distribution, hence necessitating the capacity to compute and backpropagate through both densities. Prior work has applied the general notion of having two neural networks contend.","Using a previously learned model as the noise distribution facilitates instructing a succession of increasingly superior models. This could be interpreted as an informal rivalry mechanism similar in spirit to the formal rivalry employed in adversarial networks. The major limitation of NCE is its ""discriminator"" being defined by the ratio of the probability densities of the noise distribution and model distribution, thus needing the ability to evaluate and backpropagate through both densities. Some past work has utilized the general concept of having two neural networks compete against each other.  ","Leveraging a formerly trained model as the noise distribution enables teaching a sequence of progressively better models. This can be viewed as an informal competition mechanism analogous in essence to the formal competition used in adversarial networks. A key constraint of NCE is its ""discriminator"" being characterized by the proportion of the probability densities of the noise distribution and model distribution, therefore necessitating the ability to compute and propagate gradients through both densities. Prior work has applied the general notion of having two neural networks engage in rivalry.",A,1
1079,GloVe_Global Vectors for Word Representation,"The statistics of word occurrences in a corpus is the primary source of information available to all unsupervised methods for learning word representations, and although many such methods now exist, the question still remains as to how meaning is generated from these statistics, and how the resulting word vectors might represent that meaning. In this section, we shed some light on this question. We use our insights to construct a new model for word representation which we call GloVe, for Global Vectors, because the global corpus statistics are captured directly by the model.","The frequency of words in a large collection of text is the main data available to any unsupervised technique for learning vector representations of words. Many such methods exist now, but it's still unclear exactly how meaning arises from these frequencies and how the resulting word vectors encode that meaning. Here we provide some insight into this question. We use these ideas to build a new model called GloVe, which stands for Global Vectors, because it directly incorporates global corpus statistics.","The count of how often each word appears in a body of text is the primary information accessible to all unguided ways of learning word embeddings, and while there are now many such approaches, how meaning emerges from these counts and how the resulting word vectors represent meaning remains unclear. In this section, we shed light on this issue. We utilize our understanding to construct a novel model for word embeddings called GloVe, which stands for Global Vectors, since it directly captures global corpus statistics.  ","The tally of word occurrences within a large text dataset is the main data available to any unsupervised learning method for generating vector representations of words. Many such techniques now exist, but how meaning arises from these tallies and how the resulting word vectors encode meaning is still an open question. Here we provide insight into this question. We leverage these insights to build a new model called GloVe, short for Global Vectors, because it incorporates global corpus statistics directly.",A,1
1212,Language Models are Few-Shot Learners,"For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K examples from that task’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.","To assess few-shot learning, we judge each case in the assessment set by arbitrarily selecting K instances from that task's preparation set as requirements, separated by 1 or 2 line breaks contingent upon the task. For LAMBADA and Storycloze there is no supervised preparation set accessible so we draw requiring examples from the improvement set and assess on the test set. For Winograd (the first, not SuperGLUE form) there is just a single dataset, so we directly draw requiring examples from it.","To evaluate few-shot learning, we appraise each sample in the evaluation collection by randomly choosing K samples from that task's training collection as conditions, demarcated by 1 or 2 line breaks based on the task. For LAMBADA and Storycloze there is no supervised training collection available so we extract conditioning samples from the development collection and evaluate on the test collection. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we directly draw conditioning samples from it.","To test few-shot learning, we judge every case in the assessment set by randomly selecting K cases from that task's training set as prerequisites, separated by 1 or 2 line breaks depending on the task. For LAMBADA and Storycloze there is no supervised training set present so we take conditioning cases from the development set and assess on the test set. For Winograd (the initial, not SuperGLUE form) there is just one dataset, so we directly take conditioning cases from it.",A,1
1369,Sequence to Sequence Learning with Neural Networks,"We suspect that they could achieve similar improvements by simply training their networks on reversed source sentences. End-to-end training is also the focus of Hermann et al. [12], whose model represents the inputs and outputs by feedforward networks, and map them to similar points in space. However, their approach cannot generate translations directly: to get a translation, they need to do a look up for closest vector in the pre-computed database of sentences, or to rescore a sentence.","We think they might get comparable enhancements just by teaching their neural networks using backward source sentences. Training the full system together is also the emphasis of Hermann et al. [12]. Their system embodies the inputs and outputs with feedforward networks, and relates them to analogous spots in space. However, their tactic can't directly form translations: to obtain a translation, they must search for the closest vector in the pre-computed database of sentences, or rescore a sentence.","We conjecture they could gain similar refinements by simply instructing their neural networks on reversed source sentences. End-to-end learning is also the focus of Hermann et al. [12]. Their model represents the inputs and outputs with feedforward networks, and associates them with comparable points in space. Though, their approach is unable to generate translations directly: to acquire a translation, they must look up the closest vector in the pre-calculated database of sentences, or rerank a sentence.  ","We hypothesize they might attain analogous enhancements by just schooling their neural networks on backward source sentences. Whole system training is also the concentration of Hermann et al. [12]. Their framework characterizes the inputs and outputs with feedforward networks, and relates them to similar spots in space. However, their tactic cannot straightforwardly form translations: to attain a translation, they need to search for the closest vector in the pre-figured database of sentences, or rescore a sentence.",A,1
1144,ImageNet A Large_Scale Hierarchical Image Database,"ImageNet is an ambitious project. Thus far, we have constructed 12 subtrees containing 3.2 million images. Our goal is to complete the construction of around 50 million images in the next two years. We describe here the method we use to construct ImageNet, shedding light on how properties of Sec. 2 can be ensured in this process","ImageNet is an ambitious undertaking. So far, we have built 12 subgroups containing 3.2 million pictures. Our aim is to finish building around 50 million images in the next 24 months. We explain here the technique we utilize to build ImageNet, illuminating how attributes of Sec. 2 can be guaranteed in this procedure.","ImageNet is a bold endeavor. Up to this point, we have assembled 12 subcategories containing 3.2 million photos. Our objective is to complete the assembly of around 50 million pictures in the following two years. We portray here the strategy we use to assemble ImageNet, clarifying how qualities of Sec. 2 can be ensured in this interaction. ","ImageNet is an ambitious project. Until now, we have constructed 12 subsets containing 3.2 million visuals. Our goal is to finish constructing around 50 million visuals in the next 24 months. We elucidate here the approach we employ to construct ImageNet, illuminating how properties of Sec. 2 can be ensured in this process.",A,1
1371,Transformer-XL,"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, TransformerXL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation.","Transformers can learn long-term connections but their context is restricted to a fixed length when modeling language. We put forward a new neural network called Transformer-XL that can learn beyond a fixed context length without disordering time coherence. It uses segment-level recurrence and a new positional encoding method. Our approach allows capturing longer dependencies and also fixes the context fragmentation issue. Consequently, TransformerXL learns 80% longer dependencies than RNNs and 450% longer than vanilla Transformers, performs better on short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during testing.","Transformers have potential for learning extended dependencies but are constrained by a fixed-length context in language modeling. We present a novel neural network architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal order. It utilizes a segment-level recurrence system and an original positional encoding scheme. Our technique not only allows capturing longer-term dependency, but also solves the context fragmentation problem. As a result, TransformerXL learns dependency that is 80% longer than RNNs and 450% longer than standard Transformers, achieves superior performance on both short and long sequences, and is up to 1,800+ times quicker than standard Transformers during evaluation.","Transformers can learn longer-term connections but are limited by a fixed-length context in language modeling. We introduce a new neural network called Transformer-XL that can learn beyond a fixed context length without disturbing temporal order. It uses segment-level recurrence and a novel positional encoding approach. Our method enables capturing extended dependencies and also resolves context fragmentation. Consequently, TransformerXL learns 80% longer dependencies than RNNs and 450% longer than base Transformers, has better performance on short and long sequences, and is up to 1,800+ times faster than base Transformers during testing.",A,1
1471,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and 10−2 variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot & Bengio (2010). To obtain the fixed-size 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al., 2012).","We kept the learning rate unchanged for the pre-trained layers, allowing them to be modified during training. For random initialization (when relevant), we took the weights from a normal distribution with zero mean and 10−2 variance. The biases started at zero. After submitting the paper, we realized we could initialize the weights without pre-training using the random initialization method of Glorot & Bengio (2010). To get the fixed 224×224 ConvNet input images, we randomly cropped the resized training images (one crop per image per SGD iteration). To further increase the training set size, the crops were randomly flipped horizontally and had random RGB color shifts (Krizhevsky et al., 2012).","The learning rate was not lowered for the pre-initialized layers, so they could adapt during learning. With random initialization (where used), the weights were sampled from a normal distribution with zero mean and 10−2 variance. Biases were set to zero initially. Notably, after submitting the paper we found weights could be initialized without pre-training using the random initialization of Glorot & Bengio (2010). To get the fixed 224×224 ConvNet input images, random crops were taken from scaled training images (one per image per SGD iteration). To further expand the training set, crops underwent random horizontal flips and RGB color shifts (Krizhevsky et al., 2012).  ","We did not reduce the learning rate for the pre-trained layers, permitting them to change during training. For random starting values (where relevant), we drew the weights from a normal distribution with zero mean and 10−2 variance. The biases began at zero. After submitting the paper, we realized weights could be initialized without pre-training using the random initialization of Glorot & Bengio (2010). To obtain the fixed 224×224 ConvNet input images, random crops were taken from resized training images (one per image per SGD iteration). To further grow the training set, crops were randomly flipped horizontally and had random RGB color changes (Krizhevsky et al., 2012).",A,1
1266,Neural Machine Translation by Jointly Learning To Align and Translate,"The proposed approach provides an intuitive way to inspect the (soft-)alignment between the words in a generated translation and those in a source sentence. This is done by visualizing the annotation weights αij from Eq. (6), as in Fig. 3. Each row of a matrix in each plot indicates the weights associated with the annotations. From this we see which positions in the source sentence were considered more important when generating the target word. We can see from the alignments in Fig. 3 that the alignment of words between English and French is largely monotonic.","The suggested method offers an instinctive manner to examine the (soft-)correlation between the terms in a produced translation and those in an original sentence. This is accomplished by picturing the annotation weights αij from Eq. (6), as in Fig. 3. Each row of a matrix in each plot signifies the weights linked with the annotations. From this we discern which locations in the source sentence were deemed more vital when creating the target word. We can perceive from the alignments in Fig. 3 that the alignment of words between English and French is mostly sequential.","The proposed technique provides an intuitive approach to inspect the (soft-)association between the words in a generated translation and those in a source sentence. This is achieved by visualizing the annotation coefficients αij from Equation (6), as shown in Figure 3. Each row of a matrix in each graph indicates the coefficients related to the annotations. From this we can see which positions in the source sentence were considered more important when producing the target word. We can discern from the alignments in Figure 3 that the alignment of words between English and French is largely monotonic.  ","The suggested approach gives an instinctive way to examine the (soft-)linkage between the terms in a produced translation and those in an original sentence. This is done by picturing the annotation weights αij from Formula (6), as in Figure 3. Each row of a matrix in each plot shows the weights connected with the annotations. From this we can discern which locations in the source sentence were viewed as more critical when generating the target word. We can see from the alignments in Figure 3 that the alignment of words between English and French is mostly sequential.",A,1
1237,Language Models are Unsupervised Multitask Learners,"To do this we only scraped web pages which have been curated/filtered by humans. Manually filtering a full web scrape would be exceptionally expensive so as a starting point, we scraped all outbound links from Reddit, a social media platform, which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting, educational, or just funny. The resulting dataset, WebText, contains the text subset of these 45 million links. To extract the text from HTML responses we use a combination of the Dragnet (Peters & Lecocq, 2013) and Newspaper1 content extractors.","For this task, we exclusively gathered data from webpages that were selected/filtered by people. Manually sorting through all the data from a full web scrape would be incredibly costly, so we began by compiling all the outbound links posted on Reddit, a social networking site, that had a karma score of at least 3. This can be viewed as a heuristic sign that other users found the link fascinating, informative, or humorous. The resulting dataset, WebText, comprises the text portions of these 45 million links. To extract the text from the HTML responses we utilize a combination of the Dragnet (Peters & Lecocq, 2013) and Newspaper1 content extraction tools.","To accomplish this we only accumulated content from web pages that were chosen/refined by humans. Sorting through everything from a comprehensive web scrape by hand would be prohibitively expensive, so our starting point was scraping all external links posted on Reddit, a social media platform, that had earned at least 3 karma points. This can be considered a heuristic indicator that other users found the link compelling, educational, or amusing. The resulting data set, WebText, includes the text portions of those 45 million links. To extract the text from the HTML responses we used a combination of the Dragnet (Peters & Lecocq, 2013) and Newspaper1 content extraction utilities.  ","For this endeavor we exclusively gathered data from webpages that were selected/filtered by people. Manually sifting through everything from a full web scrape would be extremely costly, so our initial step was aggregating all outbound links published on Reddit, a social media website, that had accrued at least 3 karma points. This can be viewed as a heuristic sign that other users found the link interesting, informative, or funny. The resulting dataset, WebText, consists of the text segments of those 45 million links. To extract the text from the HTML responses we utilized a combination of the Dragnet (Peters & Lecocq, 2013) and Newspaper1 content extraction tools.",A,1
1281,RoBERTa_A Robustly Optimized BERT Pretraining Approach,"Self-training methods such as ELMo (Peters et al. , 2018), GPT (Radford et al. , 2018), BERT (Devlin et al. , 2019), XLM (Lample and Conneau , 2019), and XLNet (Yang et al. , 2019) have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.","Recently developed unsupervised learning techniques like ELMo, GPT, BERT, XLM, and XLNet have substantially improved performance. However, pinpointing the most critical components of these methods can be tricky. Training takes a lot of computing power, restricting optimization opportunities. Training data is usually private and varies in size, making it hard to quantify modeling improvements.","New unsupervised learning systems such as ELMo, GPT, BERT, XLM, and XLNet have led to major gains. But determining the most important parts of these systems is difficult. Training demands extensive computing resources, limiting tuning opportunities. Training data tends to be proprietary and inconsistent in size, obstructing measurement of modeling enhancements.  ","Novel self-supervised learning models including ELMo, GPT, BERT, XLM, and XLNet have achieved impressive progress. Though, isolating the most vital aspects of these approaches is challenging. Training is computationally demanding, constraining hyperparameter tuning. Training sets are typically private and inconsistent in amount, hindering quantification of modeling developments.",A,1
1410,U-Net_Convolutional Networks for Biomedical Image Segmentation,"This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory. As for our tasks there is very little training data available, we use excessive data augmentation by applying elastic deformations to the available training images. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. This is particularly important in biomedical segmentation, since deformation used to be the most common variation in tissue and realistic deformations can be simulated efficiently.","This method of splitting up images into smaller tiles is crucial for running the neural network on big images, because otherwise the image resolution would be constrained by the graphics card's memory capacity. Since there are very few labeled training images for our applications, we augment the data extensively by warping the existing training images elastically. This enables the network to become invariant to those distortions, without needing examples of them in the annotated image set. This invariance is especially useful in biomedical segmentation, as deformation tends to be the most prevalent variation in tissue, and realistic deformations can be simulated easily.","This approach of dividing images into tiles is vital for applying the neural network to large images, as the resolution would otherwise be limited by the memory available on the GPU. Given the scarcity of labeled training data for our tasks, we perform aggressive data augmentation by elastically transforming the available training images. That allows the network to learn to be unaffected by those transformations, without requiring instances of them in the annotated training set. That robustness is particularly valuable in biomedical segmentation, since deformation was the most frequent variation in tissue, and realistic deformations can be simulated efficiently.","This strategy of splitting images into tiles is essential for running the neural network on big images, since the resolution would otherwise be constrained by the memory on the graphics processing unit. Because there is very little annotated training data for our applications, we use extensive data augmentation by elastically warping the existing training images. That enables the network to become invariant to those warps, without examples of them needing to be in the labeled image collection. That invariance is especially important in biomedical segmentation, as deformation tended to be the most common variation in tissue, and realistic deformations can be simulated easily.",A,1
1119,Going deeper with convolutions,"We would like to keep our representation sparse at most places (as required by the conditions of [2]) and compress the signals only whenever they have to be aggregated en masse. That is, 1×1 convolutions are used to compute reductions before the expensive 3×3 and 5×5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation which makes them dual-purpose. The final result is depicted in Figure 2(b). In general, an Inception network is a network consisting of modules of the above type stacked upon each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid.","We want our representation to be sparse in most places (as needed by the requirements of [2]) and compress the signals only when they must be combined together. That means 1×1 convolutions are utilized to calculate reductions before the costly 3×3 and 5×5 convolutions. In addition to being used for reductions, they also employ rectified linear activation which makes them serve two purposes. The end result is shown in Figure 2(b). Broadly, an Inception network consists of modules of the above type stacked on top of each other, with occasional max-pooling layers with stride 2 to reduce the grid resolution by half.","Our goal is to have a sparse representation in most areas (per the constraints of [2]) and consolidate the signals exclusively when they need to be aggregated in large numbers. Specifically, 1×1 convolutions are leveraged to compute reductions prior to the expensive 3×3 and 5×5 convolutions. On top of being used for reductions, they also incorporate rectified linear activation, making them dual-use. The final output is illustrated in Figure 2(b). At a high level, an Inception network comprises modules of the aforementioned type piled on each other, with sporadic max-pooling layers with stride 2 to halve the grid resolution.  ","We want our representation to be sparse for the most part (as stipulated by [2]) and compress the signals only when they must be combined in bulk. That is, 1×1 convolutions are utilized to calculate reductions before the costly 3×3 and 5×5 convolutions. Apart from being used for reductions, they also employ rectified linear activation, serving two purposes. The end result is depicted in Figure 2(b). In a nutshell, an Inception network consists of modules of the above kind stacked on top of one another, with occasional max-pooling layers with stride 2 to reduce the grid resolution by half.",A,1
1233,Language Models are Unsupervised Multitask Learners,"Preliminary experiments confirmed that sufficiently large language models are able to perform multitask learning in this toy-ish setup but learning is much slower than in explicitly supervised approaches. While it is a large step from the well-posed setup described above to the messiness of “language in the wild”, Weston (2016) argues, in the context of dialog, for the need to develop systems capable of learning from natural language directly and demonstrated a proof of concept – learning a QA task without a reward signal by using forward prediction of a teacher’s outputs. While dialog is an attractive approach, we worry it is overly restrictive.","Initial tests showed that large enough language models can carry out multitask learning in this simplistic configuration, but acquiring knowledge occurs much more slowly than with explicit supervision. Although there is a considerable gap between the well-defined scenario stated above and the complexity of natural language, Weston (2016) contends, regarding dialog, that systems able to learn directly from natural language without rewards are necessary, and provided a demonstration - acquiring a QA task without a reward signal by predicting a teacher's outputs. However, while dialog is appealing, we are concerned it is excessively limiting.","Early experiments proved sufficiently large language models can do multi-task learning in this basic setup, however learning is far slower versus explicitly supervised techniques. While there is a huge difference between the clear-cut case described previously and the chaos of ""real world language"", Weston (2016) argues, in dialog's context, systems that can learn straight from natural language sans rewards are required, and showed a proof of concept - learning a QA task sans a reward signal by forecasting a teacher's outputs. However, even though dialog is attractive, we worry it's overly constraining.  ","Initial trials showed adequately large language models are capable of multi-task learning in this simplified arrangement, but acquiring knowledge is much slower compared to explicit supervision. Although there is a massive gap between the well-defined scenario stated before and the turmoil of ""language in the wild"", Weston (2016) contends, regarding dialog, systems able to learn directly from natural language without rewards are needed, and exhibited a proof of concept - learning a QA task without a reward signal by predicting a teacher's outputs. However, even though dialog is appealing, we are concerned it is excessively restrictive.",A,1
1355,Sequence to Sequence Learning with Neural Networks,"We applied our method to the WMT’14 English to French MT task in two ways. We used it to directly translate the input sentence without using a reference SMT system and we it to rescore the n-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample translations, and visualize the resulting sentence representation. We used the WMT’14 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean “selected” subset from [29].","We tested our approach on the WMT'14 English to French machine translation challenge in two ways. We utilized it to translate the input sentence directly without an SMT reference system and we used it to rescore the n-best lists from an SMT baseline. We present the precision of these translation techniques, show example translations, and visualize the resulting sentence representation. We utilized the WMT'14 English to French data. We trained our models on a subset of 12M sentences containing 348M French words and 304M English words, which is a clean ""selected"" subset from [29].","We implemented our technique on the 2014 Workshop on Machine Translation English to French machine translation task in two manners. We employed it to translate the input sentence straightaway without an SMT reference system and we utilized it to rescore the n-best lists of an SMT baseline. We document the accuracy of these translation approaches, provide sample translations, and depict the resulting sentence representation. We leveraged the 2014 WMT English to French data. We trained our models on a subset of 12 million sentences comprising 348 million French words and 304 million English words, which is a clean ""selected"" subset from [29].  ","We applied our approach to the 2014 Workshop on Machine Translation English to French machine translation challenge in two ways. We used it to translate the input sentence directly without a reference statistical machine translation system and we used it to rescore the n-best lists from a statistical machine translation baseline. We present the precision of these translation methods, furnish example translations, and visualize the resulting sentence representation. We employed the 2014 Workshop on Machine Translation English to French data. We trained our models on a subset of 12 million sentences containing 348 million French words and 304 million English words, which is a clean ""selected"" subset from [29].",A,1
1211,Language Models are Few-Shot Learners,"As found in [KMH+20, MKAT18], larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table 2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU’s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B.","Research has shown that bigger models are often able to handle larger batch sizes, however they need smaller learning rates [KMH+20, MKAT18]. We evaluate the gradient noise during training which guides our selection of batch size [MKAT18]. Table 2.1 displays the parameters we utilized. To train the larger models without exhausting memory, we implement a combination of model parallelism within each matrix multiplication and across the neural network layers. All models were trained using V100 GPUs on part of a high-bandwidth cluster given by Microsoft. Information about the training procedure and hyperparameter configurations are available in Appendix B.","As demonstrated in prior work [KMH+20, MKAT18], models with more parameters can typically use larger batch sizes, but need smaller learning rates. We measure gradient noise during training to inform our batch size selection [MKAT18]. The settings we used are shown in Table 2.1. To prevent running out of memory when training the bigger models, we use both model parallelism within each matrix multiply and across the layers. The models were all trained on V100 GPUs provided by Microsoft as part of a high-bandwidth cluster. The training process and hyperparameter choices are detailed in Appendix B.  ","Existing research [KMH+20, MKAT18] has found that larger models can handle bigger batch sizes, but need smaller learning rates. We track gradient noise during training to guide batch size [MKAT18]. Our parameter settings are in Table 2.1. To train the larger models without memory issues, we use model parallelism in each matrix multiply and across layers. The models were trained on Microsoft's high-bandwidth cluster using V100 GPUs. Appendix B describes the training and hyperparameters.",A,1
1363,Sequence to Sequence Learning with Neural Networks,"A C++ implementation of deep LSTM with the configuration from the previous section on a single GPU processes a speed of approximately 1,700 words per second. This was too slow for our purposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was executed on a different GPU and communicated its activations to the next GPU / layer as soon as they were computed. Our models have 4 layers of LSTMs, each of which resides on a separate GPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible for multiplying by a 1000 × 20000 matrix.","A C++ version of a deep LSTM neural network with the setup from the prior section running on a single graphics card can analyze around 1,700 words each second. This speed was inadequate for our needs, so we made the model run in parallel using a machine with 8 graphics cards. Every layer of the LSTM was executed on a different graphics card and sent its outputs to the next graphics card/layer right after calculating them. Our models have 4 LSTM layers, each on its own graphics card. The other 4 graphics cards were utilized to parallelize the softmax, with each one responsible for multiplying by a 1000 × 20000 matrix.","An implementation in C++ of a deep LSTM neural net with the architecture from before on one GPU can process approximately 1,700 words per second. This was too slow for what we wanted, so we made our model parallel using an 8-GPU computer. Every LSTM layer was run on a separate GPU and communicated its outputs to the next GPU/layer immediately after computing them. Our models have 4 LSTM layers, each on its own GPU. The other 4 GPUs were used to parallelize the softmax, with each one multiplying by a 1000 × 20000 matrix.","A C++ version of a deep long short-term memory neural network with the setup described previously running on a single graphics processing unit can analyze around 1,700 words per second. This speed was not fast enough for our purposes, so we parallelized our model using a machine with 8 graphics processing units. Each long short-term memory layer was executed on a different graphics processing unit and transmitted its outputs to the next graphics processing unit/layer right after calculating them. Our models have 4 long short-term memory layers, each on its own graphics processing unit. The other 4 graphics processing units were utilized to parallelize the softmax function, with each one responsible for multiplying by a 1000 × 20000 matrix.",A,1
864,Deep contextualized word representations,"More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer. Combining the internal states in this manner allows for very rich word representations. Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on supervised word sense disambiguation tasks) while lowerlevel states model aspects of syntax (e.g., they can be used to do part-of-speech tagging). ","In other words, we determine a linear mixture of the vectors piled on top of each input word for each final job, which substantially enhances performance over only utilizing the highest LSTM layer. Uniting the internal conditions in this way permits very abundant word representations. Employing intrinsic assessments, we demonstrate that the more advanced LSTM conditions grab context-dependent features of word meaning (for instance, they can be utilized without alteration to execute well on supervised word sense disambiguation assignments) while lower-level conditions model facets of syntax (for example, they can be leveraged to do part-of-speech tagging).","More specifically, we calculate a linear blend of the vectors stacked over every input word for each concluding task, which markedly improves results over just harnessing the topmost LSTM tier. Integrating the internal statuses in this fashion enables very rich word depictions. Using inherent evaluations, we establish that the higher LSTM statuses catch context-dependent aspects of word significance (e.g., they can be utilized as-is to perform well on supervised word sense disambiguation jobs) while lower-level statuses model aspects of syntax (e.g., they can be leveraged to do part-of-speech tagging).","In other words, we determine a linear combination of the vectors piled on top of each input word for each final task, which substantially improves performance over only leveraging the highest LSTM layer. Combining the internal states in this way allows for very abundant word representations. Using intrinsic assessments, we demonstrate that the more advanced LSTM states capture context-dependent facets of word meaning (for example, they can be utilized without modification to perform well on supervised word sense disambiguation tasks) while lower-level states model aspects of syntax (for instance, they can be used to do part-of-speech tagging).",A,1
1267,Neural Machine Translation by Jointly Learning To Align and Translate,"We see strong weights along the diagonal of each matrix. However, we also observe a number of non-trivial, non-monotonic alignments. Adjectives and nouns are typically ordered differently between French and English, and we see an example in Fig. 3 (a). From this figure, we see that the model correctly translates a phrase [European Economic Area] into [zone economique europ ´ een]. The RNNsearch was able to correctly align [zone] with [Area], jumping ´ over the two words ([European] and [Economic]), and then looked one word back at a time to complete the whole phrase [zone economique europ ´ eenne]. ´","There are robust weights along the main diagonal of each matrix. However, there are also several non-trivial, non-monotonic alignments present. The order of adjectives and nouns is often different between French and English, as shown in Fig. 3 (a). This figure demonstrates that the model accurately translates the phrase [European Economic Area] into [zone economique europ ́een]. The RNNsearch correctly aligned [zone] with [Area], skipping over the two words ([European] and [Economic]), and then looked back word-by-word to complete the whole phrase [zone economique europ ́eenne].","We notice strong values on the diagonal of every matrix. But there are also a number of noteworthy, non-monotonic matches too. Descriptors and nouns are typically sequenced differently in French and English, and we observe an illustration in Fig. 3 (a). From this illustration, we note that the model properly interprets a phrase [European Economic Area] into [zone economique europ ́een]. The RNNsearch managed to correctly match [zone] with [Area], jumping past the two words ([European] and [Economic]), and then examined one word at a time to finish the entire phrase [zone economique europ ́eenne].","There are robust magnitudes along the main diagonal of each matrix. However, there are also several significant, non-monotonic correlations present as well. Adjectives and nouns are often ordered differently between French and English, as evidenced in Fig. 3 (a). This figure shows that the model accurately translates the phrase [European Economic Area] into [zone economique europ ́een]. The RNNsearch correctly matched [zone] with [Area], bypassing the two words ([European] and [Economic]), and then inspected one word back sequentially to complete the whole phrase [zone economique europ ́eenne].",A,1
818,Bag of Tricks for Efficient Text Classification,"We use the softmax function f to compute the probability distribution over the predefined classes. For a set of N documents, this leads to minimizing the negative loglikelihood over the classes: − 1 N X N n=1 yn log(f(BAxn)), where xn is the normalized bag of features of the nth document, yn the label, A and B the weight matrices. This model is trained asynchronously on multiple CPUs using stochastic gradient descent and a linearly decaying learning rate. When the number of classes is large, computing the linear classifier is computationally expensive. More precisely, the computational complexity is O(kh) where k is the number of classes and h the dimension of the text representation.","We utilize the softmax function f to determine the probability distribution across the pre-specified classes. For a group of N documents, this results in minimizing the negative log-likelihood over the classes: − 1 N ΣN n=1 yn log(f(BAxn)), where xn is the normalized bag of features of the nth document, yn the label, A and B the weight matrices. This model is trained asynchronously on multiple CPUs applying stochastic gradient descent and a linearly decreasing learning rate. When there are many classes, computing the linear classifier requires extensive computation. Specifically, the computational complexity is O(kh) where k is the number of classes and h is the dimension of the text representation.","We make use of the softmax function f to calculate the probability distribution over the pre-defined classes. For a collection of N documents, this leads to reducing the negative log-likelihood over the classes: − 1 N ΣN n=1 yn log(f(BAxn)), where xn is the normalized bag of features of the nth document, yn the label, A and B the weight matrices. This model is trained out of sync on several CPUs using stochastic gradient descent and a linearly decreasing learning rate. When there are numerous classes, calculating the linear classifier demands substantial computation. More exactly, the computational complexity is O(kh) where k is the quantity of classes and h is the dimension of the text representation.  ","We employ the softmax function f to determine the probability distribution across the pre-specified classes. For a set of N documents, this results in decreasing the negative log-likelihood over the classes: − 1 N ΣN n=1 yn log(f(BAxn)), where xn is the normalized bag of features of the nth document, yn the label, A and B the weight matrices. This model is trained asynchronously across multiple CPUs applying stochastic gradient descent and a linearly reducing learning rate. When there are many classes, computing the linear classifier necessitates significant computation. Specifically, the computational complexity is O(kh) where k is the number of classes and h is the dimension of the text representation.",A,1
1110,Going deeper with convolutions,"Although the strict mathematical proof requires very strong conditions, the fact that this statement resonates with the well known Hebbian principle – neurons that fire together, wire together – suggests that the underlying idea is applicable even under less strict conditions, in practice. On the downside, todays computing infrastructures are very inefficient when it comes to numerical calculation on non-uniform sparse data structures. Even if the number of arithmetic operations is reduced by 100×, the overhead of lookups and cache misses is so dominant that switching to sparse matrices would not pay off. The gap is widened even further by the use of steadily improving, highly tuned, numerical libraries that allow for extremely fast dense matrix multiplication, exploiting the minute details of the underlying CPU or GPU hardware [16, 9].","While a rigorous mathematical proof demands very stringent conditions, the fact that this statement aligns with the well-known Hebbian principle - neurons that fire together, wire together - implies the underlying concept could apply even with less strict conditions, in practice. However, current computing infrastructure is very inefficient for numerical calculation on irregular sparse data structures. Even if the number of arithmetic operations is reduced by 100 times, the overhead of lookups and cache misses is so predominant that switching to sparse matrices would not be advantageous. This gap is further widened by the use of constantly improving, highly optimized numerical libraries that enable extremely fast dense matrix multiplication, leveraging the minute specifics of the underlying CPU or GPU hardware [16, 9].","Although a strict mathematical proof necessitates very strong prerequisites, the resonance of this statement with the familiar Hebbian principle - neurons that fire together, connect together - hints that the core idea could hold true even with less stringent prerequisites, in reality. Nevertheless, present-day computing frameworks are very ineffective for numerical computation on uneven sparse data organizations. Even if the quantity of arithmetic operations is decreased by 100 times, the overhead of lookups and cache misses is so primary that transitioning to sparse matrices would not be beneficial. This gap is broadened even more by the use of steadily enhancing, highly tuned, numerical libraries that facilitate extremely rapid dense matrix multiplication, exploiting the minute particulars of the underlying CPU or GPU hardware [16, 9].","While a rigid mathematical proof calls for very forceful stipulations, the fact that this statement chimes with the recognized Hebbian tenet - neurons that fire together, intertwine together - proposes that the underlying notion could pertain even under less forceful stipulations, in practice. However, current computing infrastructures are very deficient when it comes to numerical calculation on irregular sparse data configurations. Even if the number of arithmetic operations is condensed by 100 times, the overhead of lookups and cache misses is so foremost that shifting to sparse matrices would not be advantageous. This divide is stretched even further by the use of steadily refining, highly tuned, numerical libraries that enable extremely swift dense matrix multiplication, harnessing the minute specifics of the underlying CPU or GPU hardware [16, 9].",A,1
1081,GloVe_Global Vectors for Word Representation,"For words k like water or fashion, that are either related to both ice and steam, or to neither, the ratio should be close to one. Table 1 shows these probabilities and their ratios for a large corpus, and the numbers confirm these expectations. Compared to the raw probabilities, the ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words. The above argument suggests that the appropriate starting point for word vector learning should be with ratios of co-occurrence probabilities rather than the probabilities themselves.","Words such as water or fashion, which are related to both ice and steam or neither, should have a ratio close to one. Table 1 displays these probabilities and ratios for a large text collection, and the figures support these predictions. In contrast to the unmodified probabilities, the ratio is superior at differentiating applicable words (solid and gas) from non-applicable words (water and fashion) and it is also better at telling apart the two applicable words. The preceding reasoning indicates that ratios of co-occurrence probabilities, rather than the raw probabilities, should be the basis for learning word vectors.","Certain words like water or fashion, that are linked to ice and steam or not related to either, will probably have a ratio near one. Table 1 provides these likelihoods and their ratios for a big set of texts, and the values align with these expectations. Compared with the unmodified probabilities, the ratio is more capable of singling out relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better at distinguishing between the two relevant words. The above thinking hints that ratios of co-occurrence probabilities rather than the plain probabilities themselves should be the starting point for learning word vectors.  ","Specific words such as water or fashion, which are associated with both ice and steam or with neither, are expected to have a ratio close to one. Table 1 exhibits these probabilities and ratios for a large collection of texts, and the figures bear out these predictions. In contrast with the raw probabilities, the ratio is more adept at isolating pertinent words (solid and gas) from impertinent words (water and fashion) and it is also better at differentiating between the two pertinent words. The preceding analysis intimates that ratios of co-occurrence probabilities rather than the unmodified probabilities should form the basis for acquiring word vectors.",A,1
1535,"You Only Look Once_Unified, Real-Time Object Detection","The grid design enforces spatial diversity in the bounding box predictions. Often it is clear which grid cell an object falls in to and the network only predicts one box for each object. However, some large objects or objects near the border of multiple cells can be well localized by multiple cells. Non-maximal suppression can be used to fix these multiple detections. While not critical to performance as it is for R-CNN or DPM, non-maximal suppression adds 2- 3% in mAP.","The grid layout creates variety in the predicted bounding box locations. It's frequently evident which grid section an object is in and the network just predicts one box per object. Though, some large objects or objects close to the edge of multiple cells can be accurately located by numerous cells. Non-maximum suppression can fix these multiple detections. While not as critical as for R-CNN or DPM, non-max suppression provides 2-3% in mAP.","The grid pattern generates diversity in the forecasted bounding box spots. Often it's clear which grid square an object falls into and the network only foretells one box for each object. However, some big objects or objects near the border of various squares can be precisely pinpointed by multiple squares. Non-maximal suppression can amend these multiple detections. Although not as vital as for R-CNN or DPM, non-maximum suppression contributes 2-3% in mAP.  ","The grid arrangement produces variation in the expected bounding box places. It's frequently obvious which grid piece an object is inside and the network just predicts one box per object. Though, some large objects or objects close to the edge of various pieces can be accurately located by multiple pieces. Non-maximum suppression can fix these multiple detections. While not as crucial as for R-CNN or DPM, non-maximum suppression provides 2-3% in mAP.",A,1
1467,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION," Small-size convolution filters have been previously used by Ciresan et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. Goodfellow et al. (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy et al., 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNet (22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions). ","Previously, Ciresan and colleagues (2011) utilized small-sized convolution filters, however their networks were far less deep compared to ours. Moreover, they did not assess performance on the large-scale ILSVRC dataset. Goodfellow's group (2014) applied deep convolutional neural networks (11 weight layers) to street number recognition, demonstrating improved performance with greater depth. Separate from our work, GoogLeNet (Szegedy et al., 2014) was a top performer in the ILSVRC-2014 classification task. Similar to our approach, GoogLeNet relies on an extremely deep convolutional network (22 weight layers) and small convolution filters (not only 3x3 but also 1x1 and 5x5 convolutions).  ","In prior work, Ciresan and co-authors (2011) used small convolution filters, but their networks were much shallower than ours and they did not test on the large ILSVRC dataset. Goodfellow and colleagues (2014) applied deep convolutional neural networks (11 weight layers) to street number recognition and showed increased depth led to better performance. Developed independently from our work, GoogLeNet (Szegedy et al., 2014), a top performer in the ILSVRC-2014 classification task, was alike in using very deep convolutional networks (22 weight layers) and small convolution filters (along with 3x3, they utilized 1x1 and 5x5 convolutions).","Previously, Ciresan and team (2011) employed small convolution filters, however their networks were far less deep versus ours, and they did not evaluate using the substantial ILSVRC dataset. Goodfellow's group (2014) leveraged deep convolutional neural networks (11 weight layers) for street number recognition, demonstrating enhanced performance with greater depth. Separately from our efforts, GoogLeNet (Szegedy et al., 2014), a top finisher in the ILSVRC-2014 classification task, was similar in leveraging very deep convolutional networks (22 weight layers) and small convolution filters (not just 3x3 but also 1x1 and 5x5 convolutions).",A,1
1014,Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Recent advances in object detection are driven by the success of region proposal methods (e.g., [22]) and region-based convolutional neural networks (R-CNNs) [6]. Although region-based CNNs were computationally expensive as originally developed in [6], their cost has been drastically reduced thanks to sharing convolutions across proposals [7, 5]. The latest incarnation, Fast R-CNN [5], achieves near real-time rates using very deep networks [19], when ignoring the time spent on region proposals. Now, proposals are the computational bottleneck in state-of-the-art detection systems.","The latest progress in recognizing objects in images is enabled by the effectiveness of techniques that suggest image regions (for example, [22]) and neural networks that examine image regions (R-CNNs) [6]. While R-CNNs were originally very slow [6], their speed has been greatly improved by reusing computations across regions [7, 5]. The most recent version, Fast R-CNN [5], can operate nearly in real time using very deep neural networks [19], if you don't count the time taken to generate region proposals. So now, creating region proposals is the computational limitation in cutting-edge object detection systems.","Recent advancements in detecting objects in images have been driven by the triumph of methods that propose image regions (like [22]) and convolutional neural networks that analyze proposed regions (region-based CNNs or R-CNNs) [6]. Although R-CNNs were very computationally expensive originally [6], their cost has plummeted thanks to sharing neural network computations between proposed regions [7, 5]. The newest version, Fast R-CNN [5], can run almost in real time using very deep neural networks [19], excluding the time for generating region proposals. Therefore, generating region proposals is now the computational bottleneck in state-of-the-art object detection systems.  ","The latest improvements in recognizing objects in images are enabled by the success of techniques that suggest areas of the image to analyze (e.g. [22]) and convolutional neural networks that focus on proposed image regions (region-based CNNs or R-CNNs) [6]. While R-CNNs were initially extremely computationally expensive [6], their cost has been drastically cut by reusing neural network computations across proposed regions [7, 5]. The most modern version, Fast R-CNN [5], can operate nearly in real time using very deep neural networks [19], not counting the time for proposing regions. As a result, proposing regions is now the computational limitation in cutting-edge object detection systems.",A,1
1129,ImageNet A Large_Scale Hierarchical Image Database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500- 1000 clean and full resolution images.","The rapid growth of pictorial information online could help develop more advanced systems for cataloging, finding, arranging and using visual data. However, the best ways to control and structure this information are still unclear. We present a new database called ""ImageNet"" here, a large taxonomy of photos constructed using WordNet as a framework. ImageNet seeks to provide most of the 80,000 WordNet categories with 500 to 1000 high quality, full size photos on average.","The explosion of images on the web enables more refined tools and algorithms for indexing, searching, organizing and interacting with visual content and multimedia. But how to actually leverage and systematize this data remains an open question. We introduce a novel database named ""ImageNet"" which categorizes images on a large scale using the WordNet hierarchy. ImageNet aims to attach 500 to 1000 pristine, high resolution images to a majority of the 80,000 WordNet synsets.","The rapid expansion of pictorial data online can facilitate more advanced models and techniques for cataloging, finding, arranging and using visual and multimedia content. However, the best methods for harnessing and structuring this data are still unclear. We present here a new database called ""ImageNet"" which taxonomizes images extensively using the WordNet categories as a framework. ImageNet seeks to link most of WordNet's 80,000 synsets with an average of 500 to 1000 high quality, full resolution photographs.",A,1
1018,Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Our RPNs are thus a kind of fully-convolutional network (FCN) [14] and they can be trained end-to-end specifically for the task for generating detection proposals. To unify RPNs with Fast R-CNN [5] object detection networks, we propose a simple training scheme that alternates between fine-tuning for the region proposal task and then fine-tuning for object detection, while keeping the proposals fixed. This scheme converges quickly and produces a unified network with conv features that are shared between both tasks.","Our RPN models are a type of fully convolutional network (FCN) [14] and they can be trained from end to end particularly for generating object proposal regions. To combine RPNs with Fast R-CNN [5] object detection models, we suggest a simple training method that switches between fine-tuning for proposing regions and then fine-tuning for object detection, while keeping the proposals unchanged. This method converges rapidly and results in a unified network with convolutional features that are shared between both tasks.","Our RPN architectures are a form of fully convolutional networks (FCNs) [14] and they are trainable in an end-to-end manner designed specifically for producing object proposal boxes. To integrate RPNs with Fast R-CNN [5] object detection architectures, we put forth a straightforward training procedure that toggles between optimizing for the proposal generation objective and then optimizing for object detection, with the proposals fixed. This procedure quickly converges and yields a unified network with conv filters that are mutual across both objectives.","Our RPN designs are a variety of fully convolutional neural networks (FCNNs) [14] and they are able to be trained from start to finish particularly for generating object proposal boxes. To combine RPNs with Fast R-CNN [5] object detection designs, we present a simple training methodology that alternates between tuning for the proposal generation task and then tuning for object detection, while keeping the proposals stationary. This methodology rapidly converges and produces a unified network with convolutional filters that are shared between both tasks.",A,1
1156,ImageNet A Large_Scale Hierarchical Image Database,"For each synset, we randomly sample 90% of the images to form the positive training image set, leaving the rest of the 10% as testing images. We form a common negative image set by aggregating 10 images randomly sampled from each synset. When training an image classifier for a particular synset, we use the positive set from this synset as well as the common negative image set excluding the images drawn from this synset, and its child and parent synsets. We evaluate the classification results by AUC (the area under ROC curve). Fig. 9 shows the results of AUC for synsets at different levels of the hierarchy, compared with an independent classifier that does not exploit the tree structure of ImageNet.","For every group of synonyms, we arbitrarily choose 90% of the photos to create the positive training image collection, leaving the other 10% as testing photos. We form a shared negative image collection by bringing together 10 randomly selected photos from each group of synonyms. When developing an image classifier for a specific group of synonyms, we utilize the positive collection from this group as well as the shared negative image collection excluding the photos drawn from this group, and its subordinate and parent groups. We assess the classification outcomes by AUC (the area under ROC curve). Fig. 9 displays the outcomes of AUC for groups of synonyms at various levels of the hierarchy, compared with an independent classifier that does not take advantage of the tree design of ImageNet.","For all synonym sets, we randomly pick 90% of the images to make up the affirmative training image set, keeping the remaining 10% as testing images. We assemble a common negative image set by aggregating 10 arbitrarily chosen images from each synonym set. When constructing an image classifier for a particular synonym set, we employ the affirmative set from this synonym set and the common negative image set leaving out the images taken from this synonym set, and its child and parent synonym sets. We evaluate the classification results by AUC (the area under ROC curve). Fig. 9 exhibits the outcomes of AUC for synonym sets at different tiers of the hierarchy, compared with an independent classifier that does not utilize the tree architecture of ImageNet.  ","For every collection of synonymous words, we haphazardly take 90% of the pictures to form the positive training image collection, retaining the other 10% as testing pictures. We assemble a shared negative image collection by bringing together 10 randomly picked pictures from each collection of synonymous words. When developing an image classifier for a specific collection of synonymous words, we use the positive collection from this collection of synonymous words and the shared negative image collection excluding the pictures taken from this collection of synonymous words, and its subordinate and parent collections of synonymous words. We appraise the classification results by AUC (the area under ROC curve). Fig. 9 displays the outcomes of AUC for collections of synonymous words at various levels of the hierarchy, compared with an independent classifier that does not exploit the tree design of ImageNet.",A,1
1543,"You Only Look Once_Unified, Real-Time Object Detection","Sermanet et al. train a convolutional neural network to perform localization and adapt that localizer to perform detection [32]. OverFeat efficiently performs sliding window detection but it is still a disjoint system. OverFeat optimizes for localization, not detection performance. Like DPM, the localizer only sees local information when making a prediction. OverFeat cannot reason about global context and thus requires significant post-processing to produce coherent detections.","Sermanet and colleagues educated a convolutional neural network to do localization and tailored that localizer to execute detection [32]. OverFeat competently executes sliding window detection but it is still a separate system. OverFeat enhances for localization, not detection capability. Similar to DPM, the localizer only perceives local details when forming a prediction. OverFeat cannot deduce about comprehensive situation and hence necessitates considerable post-processing to generate coherent detections.","Sermanet and co-workers trained a convolutional neural network to carry out localization and adapted that localizer to conduct detection [32]. OverFeat efficiently implements sliding window detection however it is still a disconnected system. OverFeat focuses on localization, not detection performance. Analogous to DPM, the localizer only perceives local facts when making a forecast. OverFeat cannot reason regarding global context thus requires significant after-processing to produce coherent detections. ","Sermanet and colleagues taught a convolutional neural network to execute localization and tailored that localizer to perform detection [32]. OverFeat competently implements sliding window detection however it is still a separate system. OverFeat enhances for localization, not detection capability. Similar to DPM, the localizer only sees local details when forming a prediction. OverFeat cannot deduce about comprehensive situation thus necessitates considerable after-processing to generate coherent detections.",A,1
1183,ImageNet Classification with Deep Convolutional Neural Networks,"So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.","Thus, whenever new information is given, the neural network generates a distinct design, though all these plans utilize the same coefficients. This approach decreases intricate co-dependencies of nerve cells, since a nerve cell is unable to depend on the existence of specific other nerve cells. Consequently, it is compelled to learn more durable characteristics that work together with numerous arbitrary subgroups of the other nerve cells. During testing, we employ all the nerve cells but increase their outputs by 0.5, which reasonably estimates taking the geometric average of the predictive distributions formed by the exponentially-numerous dropout networks.","So, every time there is new data, the neural network comes up with a different structure, but all these structures use the same weights. This method lowers complex co-adaptations of neurons, because a neuron can't rely on particular other neurons being present. It is thus forced to learn more robust features that are useful with many different random subsets of the other neurons. When testing, we use all the neurons but multiply their outputs by 0.5, which approximates taking the geometric mean of the predictive distributions made by the exponentially many dropout networks. ","Therefore, whenever there is new input, the neural network generates a new architecture, though all these designs share the same weights. This technique decreases complicated co-dependencies of neurons, since a neuron is unable to depend on specific other neurons existing. Consequently, it must learn more durable features that work with many random subgroups of the other neurons. During testing, we utilize all the neurons but increase their outputs by 0.5, which reasonably estimates taking the geometric average of the predictive distributions formed by the exponentially numerous dropout networks.",A,1
865,Deep contextualized word representations,"Simultaneously exposing all of these signals is highly beneficial, allowing the learned models select the types of semi-supervision that are most useful for each end task. Extensive experiments demonstrate that ELMo representations work extremely well in practice. We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis. The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions. For tasks where direct comparisons are possible, ELMo outperforms CoVe (McCann et al., 2017), which computes contextualized representations using a neural machine translation encoder. ","Presenting all of these signals at the same time is very advantageous, as it allows the learned models to choose the types of semi-supervision that are most beneficial for each final task. Comprehensive experiments show that ELMo representations are extremely effective in practice. We first demonstrate that they can be seamlessly incorporated into present models for six varied and difficult language understanding tasks, including textual entailment, question answering and sentiment analysis. Just adding the ELMo representations significantly improves the state of the art in every case, with relative error reductions of up to 20%. For tasks where direct comparisons are feasible, ELMo surpasses CoVe (McCann et al., 2017), which generates contextualized representations utilizing a neural machine translation encoder.","Exposing all of these signals together is highly useful, permitting the learned models to select the semi-supervision types that are most valuable for each target task. Wide-ranging experiments prove that ELMo representations work incredibly well in reality. We first exhibit that they can be easily integrated into existing models for six diverse and challenging language understanding problems, like textual entailment, question answering and sentiment analysis. Merely incorporating the ELMo representations alone considerably improves the state of the art in every case, including up to 20% relative error decreases. For tasks where direct comparisons are possible, ELMo beats CoVe (McCann et al., 2017), which produces contextualized representations employing a neural machine translation encoder.","Presenting all of these signals at once is very beneficial, enabling the learned models to choose the semi-supervision forms that are most helpful for each final task. Extensive experiments demonstrate that ELMo representations are extremely effective in practice. We first show that they can be seamlessly added to current models for six varied and tough language understanding tasks, including textual entailment, question answering and sentiment analysis. Simply integrating the ELMo representations by itself significantly enhances the state of the art in every case, with relative error reductions of up to 20%. For tasks where direct comparisons are viable, ELMo is superior to CoVe (McCann et al., 2017), which generates contextualized representations using a neural machine translation encoder.",A,1
854,BERT,"We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.","We understand that it could also be feasible to develop separate left-to-right and right-to-left models and depict each token as the combination of the two models, as ELMo does. However: (a) this is two times more costly than a single bidirectional model; (b) this is counterintuitive for tasks like question answering, since the right-to-left model would be unable to base the answer on the question; (c) this is strictly less capable than a deep bidirectional model, since it cannot utilize both left and right context at every layer.","We acknowledge that it would also be possible to create distinct left-to-right and right-to-left systems and represent each token as the merging of the two systems, as ELMo does. However: (a) this is twice as expensive as a single two-way model; (b) this is unintuitive for tasks like QA, since the right-to-left system would be unable to condition the answer on the question; (c) this is strictly less powerful than an in-depth two-way model, since it cannot use both left and right context at each layer.","We realize that it would also be viable to develop separate left-to-right and right-to-left models and depict each token as the fusion of the two models, as ELMo does. However: (a) this is two times more costly than a single two-way model; (b) this is illogical for tasks like questioning answering, since the right-to-left model would be unable to base the answer on the question; (c) this is strictly less capable than an extensive two-way model, since it cannot leverage both left and right context at every layer.",A,1
1516,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Note that all these three unique patterns involve the relative positions rather than absolute ones, and hence are likely enabled by the “relative attention” mechanism in XLNet. We conjecture these unique patterns contribute to the performance advantage of XLNet. On the other hand, the proposed permutation LM objective mostly contributes to a better data efficiency, whose effects may not be obvious from qualitative visualization.","Observe that all three of these distinctive designs depend on comparative rather than absolute placements, so they are probably facilitated by the ""relative attention"" component in XLNet. We hypothesize these unusual designs help explain the superior performance of XLNet. However, the suggested permutation language model goal appears to mainly assist with better data efficiency, which may not be evident from qualitative inspection.","Notice that all of these three novel blueprints involve positional relationships rather than absolute positions, so they are plausibly empowered by the ""relative attention"" apparatus in XLNet. We theorize these novel blueprints lend a hand to the enhanced capabilities of XLNet. Though, the advised permutation language archetype ambition seems to chiefly lend a hand with superior data thrift, whose goods may not be noticeable from qualitative scanning. ","Recognize that all three of these unique outlines hinge on correlated rather than absolute placements, hence they are potentially enabled by the ""relative attention"" mechanism within XLNet. We posit these distinctive outlines contribute to the heightened effectiveness of XLNet. However, the suggested permutation language model objective apparently mostly assists with superior data economy, whose benefits may not be discernible from qualitative examination.",A,1
1062,Generative Adversarial Nets,"This new framework comes with advantages and disadvantages relative to previous modeling frameworks. The disadvantages are primarily that there is no explicit representation of pg(x), and that D must be synchronized well with G during training (in particular, G must not be trained too much without updating D, in order to avoid “the Helvetica scenario” in which G collapses too many values of z to the same value of x to have enough diversity to model pdata), much as the negative chains of a Boltzmann machine must be kept up to date between learning steps.","This recently introduced structure has both positives and negatives compared to earlier modeling systems. The negatives are mostly that pg(x) is not directly depicted, and D needs to be well coordinated with G during learning (specifically, G can't be trained excessively without refreshing D, to avoid ""the Helvetica case"" where G maps too many z values to the same x value to have sufficient diversity to represent pdata), similar to how the negative chains of a Boltzmann machine must be kept current between learning phases.","This newly presented framework has some advantages and drawbacks relative to prior modeling approaches. The drawbacks are primarily the lack of explicit representation for pg(x), and the need to synchronize D closely with G during training (in particular, avoiding overtraining G without updating D, which can lead to ""the Helvetica scenario"" where G maps too many z values to the same x value to adequately capture diversity for modeling pdata), analogous to keeping the negative chains updated in a Boltzmann machine. ","This newly introduced architecture has some pros and cons compared to previous modeling frameworks. The cons are mostly the absence of direct depiction of pg(x), and the requirement to coordinate D tightly with G during learning (specifically, avoiding overtraining G without refreshing D, which can result in ""the Helvetica case"" where G compresses too many z values into the same x value to have enough diversity to represent pdata), similar to maintaining up-to-date negative chains in a Boltzmann machine.",A,1
1312,Sentence Embeddings using Siamese BERT-Networks,"In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.","This paper introduces Sentence-BERT (SBERT), a modified version of the pre-trained BERT model that uses siamese and triplet neural networks to create sentence embeddings that capture semantic meaning and can be compared using cosine similarity. This decreases the time to find the most similar pair from 65 hours with BERT/RoBERTa to around 5 seconds with SBERT, while keeping BERT's accuracy. We assess SBERT and SRoBERTa on standard semantic textual similarity tasks and transfer learning tasks, where they surpass other state-of-the-art sentence embedding techniques.","In this paper, we present Sentence-BERT (SBERT), a modification of the pre-trained BERT network using siamese and triplet network architectures to generate semantically meaningful sentence embeddings that can be compared via cosine similarity. This reduces the time to identify the most similar pair from 65 hours with BERT/RoBERTa to about 5 seconds with SBERT, maintaining BERT's accuracy. We evaluate SBERT and SRoBERTa on common semantic textual similarity tasks and transfer learning tasks, outperforming other cutting-edge sentence embedding methods.","This paper introduces Sentence-BERT (SBERT), a variant of the pretrained BERT model using siamese and triplet networks to create sentence embeddings that capture semantic meaning and can be compared via cosine similarity. This lowers the time to find the most similar pair from 65 hours with BERT/RoBERTa to around 5 seconds with SBERT, retaining BERT's accuracy. We test SBERT and SRoBERTa on standard semantic textual similarity and transfer learning tasks, outperforming other state-of-the-art sentence embedding techniques.",A,1
1527,"You Only Look Once_Unified, Real-Time Object Detection","Our network architecture is inspired by the GoogLeNet model for image classification [34]. Our network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use 1 × 1 reduction layers followed by 3 × 3 convolutional layers, similar to Lin et al [22]. The full network is shown in Figure 3. We also train a fast version of YOLO designed to push the boundaries of fast object detection. Fast YOLO uses a neural network with fewer convolutional layers (9 instead of 24) and fewer filters in those layers. Other than the size of the network, all training and testing parameters are the same between YOLO and Fast YOLO.","The design of our neural network framework takes inspiration from the GoogLeNet architecture for categorizing images [34]. Our framework contains 24 layers for detecting features plus 2 fully connected layers. Rather than using the inception modules from GoogLeNet, we utilize 1 × 1 reduction layers followed by 3 × 3 convolutional layers, similar to the approach by Lin et al [22]. Figure 3 outlines the full structure of our network. Additionally, we train a rapid version of YOLO intended to push the limits of quick object detection. This Fast YOLO uses a neural network with fewer convolutional layers (9 rather than 24) and fewer filters per layer. Aside from the network size, all training and testing parameters are identical between YOLO and Fast YOLO.","Our neural network design derives inspiration from the GoogLeNet model for image classification tasks [34]. It consists of 24 convolutional layers and 2 fully connected layers. Instead of the inception modules in GoogLeNet, we use 1 × 1 reduction layers followed by 3 × 3 convolutional layers, akin to the work by Lin et al [22]. Figure 3 shows the complete network architecture. We also trained a faster variant of YOLO meant to extend the boundaries of rapid object detection. This Fast YOLO utilizes a neural network with fewer convolutional layers (9 instead of 24) and fewer filters in those layers. Other than the network size, all training and testing settings are the same across YOLO and Fast YOLO.  ","We based the architecture of our neural network on the GoogLeNet model for image classification [34]. It contains 24 convolutional layers and 2 fully connected layers. Rather than using the inception modules of GoogLeNet, we employ 1 × 1 reduction layers followed by 3 × 3 convolutional layers, similar to Lin et al [22]. The full network structure is diagrammed in Figure 3. Additionally, we trained a speedy version of YOLO designed to push the limits of fast object detection. This Fast YOLO uses a neural network with fewer convolutional layers (9 instead of 24) and fewer filters per layer. Aside from the network dimensions, all training and testing parameters are identical for YOLO and Fast YOLO.",A,1
1497,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Since our objective function fits in the AR framework, we incorporate the state-of-the-art AR language model, Transformer-XL [9], into our pretraining framework, and name our method after it. We integrate two important techniques in Transformer-XL, namely the relative positional encoding scheme and the segment recurrence mechanism. We apply relative positional encodings based on the original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden states from previous segments.","Because our goal function is compatible with the AR structure, we include the cutting-edge AR language model, Transformer-XL [9], into our pretraining system, and title our approach after it. We combine two key methods from Transformer-XL: the relative positional encoding plan and the segment repetition mechanism. We implement relative positional encodings founded on the original sequence as described before, which is simple. Now we explain how to integrate the recurrence mechanism into the proposed permutation configuration and empower the model to reuse hidden states from preceding segments.","Since our objective function is suitable for the AR framework, we incorporate the state-of-the-art AR language model, Transformer-XL [9], into our pretraining framework, and name our approach after it. We unite two vital techniques in Transformer-XL: the relative positional encoding scheme and the segment recurrence mechanism. We put into practice relative positional encodings based on the original sequence as previously discussed, which is straightforward. We now discuss how to integrate the recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden states from previous segments.","Because our goal function works with the AR architecture, we include the cutting-edge AR language model, Transformer-XL [9], into our pretraining system, and title our method after it. We combine two key techniques from Transformer-XL: the relative positional encoding plan and the segment repetition mechanism. We implement relative positional encodings based on the original sequence as previously described, which is simple. We now explain how to integrate the recurrence mechanism into the proposed permutation configuration and empower the model to reuse hidden states from prior segments.",A,1
1148,ImageNet A Large_Scale Hierarchical Image Database,"We encourage users to select images regardless of occlusions, number of objects and clutter in the scene to ensure diversity. While users are instructed to make accurate judgment, we need to set up a quality control system to ensure this accuracy. There are two issues to consider. First, human users make mistakes and not all users follow the instructions. Second, users do not always agree with each other, especially for more subtle or confusing synsets, typically at the deeper levels of the tree. Fig. 7(left) shows an example of how users’ judgments differ for “Burmese cat”.","We prompt users to pick images no matter if there are obstructions, quantity of items or disorder in the scene to guarantee variety. Although users are told to make precise choices, we must create a quality assurance system to ensure correctness. There are two concerns to think about. Initially, people make errors and not every user follows the guidelines. Also, users do not constantly concur with one another, particularly for more subtle or perplexing concepts, usually at the further levels of the hierarchy. Fig. 7(left) demonstrates how users' assessments differ for ""Burmese cat"".","We encourage users to choose images independent of impediments, number of articles and disarray in the shot to ensure diversity. While we advise users to make accurate evaluations, we must implement a quality control framework to guarantee precision. There are two considerations. Firstly, humans commit mistakes and not every person abides by the instructions. Secondly, individuals do not always agree with each other, especially for more subtle or confusing ideas, typically deeper in the structure. Fig. 7(left) shows how users' judgments vary for ""Burmese cat"".  ","We prompt users to select photos irrespective of barriers, quantity of objects and disorder in the scene to ensure variety. Although we direct users to make precise appraisals, we must establish a quality assurance process to guarantee accuracy. There are two factors. Initially, people err and not every person follows the guidelines. Additionally, individuals do not always concur with each other, particularly for more subtle or perplexing concepts, generally further down the hierarchy. Fig. 7(left) illustrates how users' evaluations differ for ""Burmese cat"".",A,1
999,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"In our experiments, the model is fed the question and its context and asked to generate the answer token-by-token. For WMT English to German, we use the same training data as (Vaswani et al., 2017) (i.e. News Commentary v13, Common Crawl, Europarl v7) and newstest2013 as a validation set (Bojar et al., 2014). For English to French, we use the standard training data from 2015 and newstest2014 as a validation set (Bojar et al., 2015). For English to Romanian, which is a standard lower-resource machine translation benchmark, we use the train and validation sets from WMT 2016 (Bojar et al., 2016). ","In our tests, we provide the model with the question and context and have it generate the answer word-by-word. For translating English to German, we utilize the same training information as Vaswani et al. (2017), which includes News Commentary v13, Common Crawl, and Europarl v7, using newstest2013 as the validation set (Bojar et al., 2014). For English to French, we employ the standard 2015 training data with newstest2014 as the validation set (Bojar et al., 2015). For English to Romanian, a common low-resource machine translation benchmark, we use the training and validation sets from WMT 2016 (Bojar et al., 2016).","During our experiments, we feed the model the question and surrounding context then have it produce the answer one token at a time. For English-German translation, we employ the same training data as Vaswani et al. (2017), specifically News Commentary v13, Common Crawl, and Europarl v7, using newstest2013 for validation (Bojar et al., 2014). For English-French, we utilize the standard 2015 training information with newstest2014 as validation (Bojar et al., 2015). For English-Romanian, a typical low-resource machine translation test, we use the training and validation sets from WMT 2016 (Bojar et al., 2016).  ","In our tests, we input the question and context into the model and have it generate the answer word-by-word. For English to German translation, we use the same training data as Vaswani et al. (2017), which includes News Commentary v13, Common Crawl, and Europarl v7, with newstest2013 as validation (Bojar et al., 2014). For English to French, we use the standard 2015 training data with newstest2014 for validation (Bojar et al., 2015). For English to Romanian, a common low-resource machine translation benchmark, we utilize the training and validation sets from WMT 2016 (Bojar et al., 2016).",A,1
1196,Language Models are Few-Shot Learners,"One potential route towards addressing these issues is meta-learning1 – which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [RWC+19] attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.","A possible way to tackle these problems is meta-learning - which for language models means the model learns a wide range of skills and pattern recognition capabilities during training, and then utilizes those capabilities during inference to quickly adapt to or identify the desired task (shown in Figure 1.1). Recent work [RWC+19] tries to do this through what we term ""in-context learning"", using the text input of a pre-trained language model as a form of task description: the model is conditioned on a natural language instruction and/or a few examples of the task and is then expected to complete more instances of the task just by predicting what comes next.","One avenue to address these challenges is meta-learning - where for language models this means the model develops a diverse set of abilities and pattern recognition skills during training, and subsequently leverages those skills during inference to swiftly tailor to or discern the target task (depicted in Figure 1.1). Recent efforts [RWC+19] attempt this via what we dub ""in-context learning"", utilizing the text input of a pre-trained language model as a type of task specification: the model is primed on a natural language directive and/or a few demonstrations of the task and is then anticipated to complete more cases of the task simply by forecasting what follows. ","A promising approach to tackling these difficulties is meta-learning – which for language models signifies the model acquires a wide repertoire of capabilities and pattern recognition aptitudes during training, then capitalizes on those aptitudes during inference to rapidly accommodate or identify the intended task (portrayed in Figure 1.1). Recent work [RWC+19] seeks to accomplish this through what we call “in-context learning”, leveraging the text input of a pre-trained language model as a form of task delineation: the model is conditioned on a natural language instruction and/or a few instances of the task and is then expected to complete further examples of the task simply by predicting the next steps.",A,1
1404,U-Net_Convolutional Networks for Biomedical Image Segmentation,"Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. In the last two years, deep convolutional networks have outperformed the state of the art in many visual recognition tasks, e.g. [ 7 , 3]. While convolutional networks have already existed for a long time [ 8], their success was limited due to the size of the available training sets and the size of the considered networks.","Utilizing an identical neural network educated on pictures from transmitted light microscopy (phase contrast and DIC), we prevailed in the ISBI cell tracking challenge 2015 in these groups decisively. Furthermore, the network operates rapidly. Segmentation of a 512x512 image requires less than one second on a current GPU. In the previous two years, deep convolutional networks have surpassed the state-of-the-art in numerous visual identification tasks, e.g. [7, 3]. Although convolutional networks have been present for a while [8], their triumph was constrained because of the scale of accessible training sets and the size of the studied networks.","Using the same neural network trained on images from light microscopy that passes through the sample (phase contrast and DIC), we won the ISBI cell tracking challenge 2015 in these categories by a wide margin. Also, the network is fast, segmenting a 512x512 image in under a second using a modern GPU. Over the past two years, deep convolutional networks have beaten the state-of-the-art in many visual recognition tasks, for example [7, 3]. While convolutional networks have existed for some time [8], their success was limited by the size of available training sets and networks.  ","Employing an identical deep neural network educated on pictures from transmitted light microscopy (phase contrast and DIC) modalities, we were victorious in the ISBI cell tracking challenge 2015 in these groups by a substantial difference. Furthermore, the network has high throughput, segmenting a 512x512 image in under one second utilizing a contemporary GPU. In the previous two years, deep convolutional networks have exceeded the state-of-the-art in numerous visual recognition tasks, e.g. [7, 3]. Although convolutional networks have been around for a while [8], their achievement was constrained by the scale of existing training sets and network sizes.",A,1
871,Deep contextualized word representations,"In contrast, after pretraining the biLM with unlabeled data, we fix the weights and add additional taskspecific model capacity, allowing us to leverage large, rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model. 3 ELMo: Embeddings from Language Models Unlike most widely used word embeddings (Pennington et al., 2014), ELMo word representations are functions of the entire input sentence, as described in this section. They are computed on top of two-layer biLMs with character convolutions (Sec. 3.1), as a linear function of the internal network states (Sec. 3.2).","Conversely, after pre-training the bidirectional language model with unlabeled information, we stabilize the weights and supplement further task-specific model ability, enabling us to take advantage of large, rich and universal bidirectional language model representations for situations where downstream training data volume necessitates a smaller supervised model. ","In contrast, after pre-conditioning the two-directional language model with non-labeled data, we cement the loads and contribute extra assignment-particular model capacity, permitting us to leverage immense, wealthy and universal two-directional language model depictions for cases where downstream preparing information size commands a smaller supervised model.","However, after pre-teaching the two-way language model with unlabeled information, we set the weights and provide extra task-specific model potential, allowing us to use large, abundant and universal two-way language model representations for cases where downstream training data amount requires a smaller supervised model.",A,1
841,BERT,"Model Architecture BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as “The Annotated Transformer.”2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. 3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).","The architecture of BERT's model is constructed using multiple layers of a bidirectional Transformer encoder, which is based on the original design described in Vaswani et al. (2017) and implemented in the tensor2tensor library. Since the use of Transformers is now commonplace and our implementation closely resembles the original, we will omit a comprehensive background of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides like ""The Annotated Transformer."" In our work, we denote the number of layers (i.e. Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).","The model design of BERT employs multiple tiers of a bidirectional Transformer encoder, built upon the initial blueprint outlined in Vaswani et al. (2017) and coded in the tensor2tensor library. Since Transformers are now widely used and our implementation closely matches the original, we omit an exhaustive overview of the model design and refer readers to Vaswani et al. (2017) as well as great guides like ""The Annotated Transformer."" In our work, we signify the number of tiers (i.e. Transformer blocks) as L, the hidden dimension as H, and the quantity of self-attention heads as A. We primarily document outcomes on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).","BERT's model structure consists of multiple layers of a bidirectional Transformer encoder, which was based on the initial implementation defined in Vaswani et al. (2017) and implemented in the tensor2tensor library. Since the use of Transformers is now widespread and our implementation is nearly identical to the original, we omit a detailed background of the model structure and refer readers to Vaswani et al. (2017) as well as great guides like ""The Annotated Transformer."" In our work, we denote the number of layers (i.e. Transformer blocks) as L, the hidden dimension as H, and the number of self-attention heads as A. We primarily present results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).",A,1
1120,Going deeper with convolutions,"For technical reasons (memory efficiency during training), it seemed beneficial to start using Inception modules only at higher layers while keeping the lower layers in traditional convolutional fashion. This is not strictly necessary, simply reflecting some infrastructural inefficiencies in our current implementation. One of the main beneficial aspects of this architecture is that it allows for increasing the number of units at each stage significantly without an uncontrolled blow-up in computational complexity. The ubiquitous use of dimension reduction allows for shielding the large number of input filters of the last stage to the next layer, first reducing their dimension before convolving over them with a large patch size.","Due to technical constraints with memory during training, it was helpful to only use Inception modules in the higher layers while keeping the early layers as standard convolutional layers. This was not essential, just showing some inadequacies in our current system. A key benefit of this design is it lets us substantially increase the number of units at each stage without computational complexity spiraling out of control. Using dimension reduction everywhere protects later layers from the large number of input filters of the last stage, first shrinking them before convolving over them with a large patch size.","For efficiency reasons during learning, we found it useful to have Inception modules only in higher tiers with regular convolutional tiers below. This wasn't mandatory, just oversights in our present tools. A major plus of this layout is it enables largely expanding the count of nodes at each tier without computational intricacy rising rapidly. Employing downscaling everywhere shields subsequent tiers from the numerous input filters of the final tier, first compressing them before crossing over them with a big patch extent.","Due to limitations in memory during training, it was advantageous to utilize Inception modules solely in higher levels while retaining standard convolutional layers in lower levels. This was not imperative, simply highlighting some inefficiencies in our current implementation. A primary benefit of this design is it permits substantially increasing the quantity of units at each stage without an uncontrolled surge in computational complexity. The widespread application of dimension reduction shields later layers from the numerous input filters of the final stage, first condensing them before convolving over them with a large patch size.",A,1
1501,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Comparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e., only predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all tokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT and XLNet, partial prediction plays a role of reducing optimization difficulty by only predicting tokens with sufficient context. However, the independence assumption discussed in Section 2.1 disables BERT to model dependency between targets. To better understand the difference, let’s consider a concrete example [New, York, is, a, city].","By examining Equation 2 and 5, we see that BERT and XLNet both do partial forecasting, meaning they only predict some of the tokens in the sequence. This is essential for BERT since predicting all tokens is not possible if they are all masked. Furthermore, for BERT and XLNet, partial prediction helps make optimization less difficult by only predicting tokens with enough context. However, as discussed in Section 2.1, the independence assumption prevents BERT from modeling dependencies between targets. To illustrate the difference, consider the concrete example [New, York, is, a, city].","Analyzing Equations 2 and 5 reveals that BERT and XLNet both make incomplete predictions, predicting just some tokens in the sequence rather than all of them. This incomplete prediction is necessary for BERT because predicting every token is impossible if they are all obscured. Additionally, for both BERT and XLNet, incomplete prediction plays a role in decreasing the difficulty of optimization by only predicting tokens with adequate context. However, as mentioned in Section 2.1, the assumption of independence stops BERT from modeling dependencies between predictions. To better grasp the difference, examine the specific example [New, York, is, a, city].","Looking at Equation 2 and Equation 5 shows that BERT and XLNet both do partial foretelling, meaning they only anticipate a subset of the tokens in the sequence. This is essential for BERT since anticipating all tokens is impossible if they are all concealed. Also, for both BERT and XLNet, partial foretelling helps make optimization less troublesome by only anticipating tokens with sufficient context. However, as talked about in Section 2.1, the assumption of autonomy hinders BERT from modeling connections between anticipations. To illustrate the difference, think about the concrete example [New, York, is, a, city].",A,1
1356,Sequence to Sequence Learning with Neural Networks,"We chose this translation task and this specific training set subset because of the public availability of a tokenized training and test set together with 1000-best lists from the baseline SMT [29]. As typical neural language models rely on a vector representation for each word, we used a fixed vocabulary for both languages. We used 160,000 of the most frequent words for the source language and 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was replaced with a special “UNK” token.","We selected this translation assignment and this particular training set portion because the tokenized training and test set were publicly available together with 1000-best lists from the baseline SMT [29]. Since typical neural language models depend on a vector representation for each word, we utilized a fixed vocabulary for both tongues. We employed 160,000 of the most frequent terms for the source tongue and 80,000 of the most frequent terms for the target tongue. We substituted every out-of-vocabulary word with a special “UNK” token.","We opted for this translation task and this specific training set segment due to the public access to a tokenized training and test set along with 1000-best lists from the baseline SMT [29]. As conventional neural language models rely on a vector depiction for each term, we used a fixed vocabulary for both languages. We utilized 160,000 of the most common words for the source language and 80,000 of the most common words for the target language. We replaced every out-of-vocabulary word with a special “UNK” token. ","We chose this translation assignment and this particular training set portion because the tokenized training and test set were available to the public together with 1000-best lists from the baseline SMT [29]. Since typical neural language models depend on a vector representation for each term, we used a fixed vocabulary for both tongues. We utilized 160,000 of the most frequent words for the source tongue and 80,000 of the most frequent words for the target tongue. We substituted every out-of-vocabulary word with a special “UNK” token.",A,1
1505,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"After the initial publication of our manuscript, a few other pretrained models were released such as RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from 1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we exclude ALBERT from the following results as it is hard to lead to scientific conclusions. To obtain relatively fair comparison with RoBERTa, the experiment in this section is based on full data and reuses the hyper-parameters of RoBERTa, as described in section 3.1.","Following the first publication of our paper, some other pre-trained models were made public like RoBERTa [21] and ALBERT [19]. We do not include ALBERT in the subsequent results given it greatly expands the model hidden dimension from 1024 to 2048/4096 which substantially raises the computational cost in FLOPs. Hence it is problematic to derive scientific conclusions. For a relatively equitable comparison with RoBERTa, the experiment here utilizes full data and recycles the hyper-parameters of RoBERTa, as delineated in section 3.1.","After our initial manuscript release, additional pretrained models emerged such as RoBERTa [21] and ALBERT [19]. Since ALBERT grows the model hidden size substantially from 1024 to 2048/4096, drastically increasing computational load in FLOPs, we omit ALBERT from the ensuing outcomes as drawing scientific conclusions is difficult. To obtain a relatively fair benchmark against RoBERTa, the trial in this portion leverages full data and reuses the hyperparameters of RoBERTa, as described in section 3.1.  ","Subsequent to the first publication of our paper, other pretrained models like RoBERTa [21] and ALBERT [19] were made public. As ALBERT expands the model hidden dimension markedly from 1024 to 2048/4096, leading to a major rise in computational cost per FLOPs, we do not include ALBERT in the following results as scientific conclusions are problematic. For a relatively equitable evaluation against RoBERTa, the experiment here employs full data and recycles the hyperparameters of RoBERTa, as outlined in section 3.1.",A,1
884,Deep contextualized word representations,"Our baseline, the ESIM sequence model from Chen et al. (2017), uses a biLSTM to encode the premise and hypothesis, followed by a matrix attention layer, a local inference layer, another biLSTM inference composition layer, and finally a pooling operation before the output layer. Overall, adding ELMo to the ESIM model improves accuracy by an average of 0.7% across five random seeds. A five member ensemble pushes the overall accuracy to 89.3%, exceeding the previous ensemble best of 88.9% (Gong et al., 2018). ","Our starting point was the ESIM sequence model created by Chen and colleagues in 2017. This model utilizes a biLSTM to encode the premise and hypothesis. After that, there is a matrix attention layer, a local inference layer, another biLSTM layer for inference composition, and finally pooling before the output layer. On average, integrating ELMo into the ESIM model increased accuracy by 0.7% over 5 random seeds. A 5 member ensemble pushed the total accuracy to 89.3%, surpassing the previous best ensemble accuracy of 88.9% from Gong et al., 2018.","Our baseline was the ESIM sequence model developed by Chen's research team in 2017. It uses a biLSTM to encode the premise and hypothesis, then has a matrix attention layer, local inference layer, an additional biLSTM inference composition layer, and pooling before the output. Overall, adding ELMo to the ESIM model improved accuracy by 0.7% on average across 5 random seeds. A 5 member ensemble increased the total accuracy to 89.3%, beating the prior best ensemble accuracy of 88.9% from Gong and colleagues' 2018 paper.  ","Our starting model was the ESIM sequence model published by Chen and coauthors in 2017. It utilizes a biLSTM to encode the premise and hypothesis, followed by a matrix attention layer, local inference layer, another biLSTM inference composition layer, and pooling before output. Incorporating ELMo into the ESIM model increased accuracy by 0.7% on average over 5 random seeds. A 5 member ensemble pushed the total accuracy to 89.3%, surpassing the previous top ensemble accuracy of 88.9% from Gong et al.'s 2018 paper.",A,1
820,Bag of Tricks for Efficient Text Classification,"This means that the probability of a node is always lower than the one of its parent. Exploring the tree with a depth first search and tracking the maximum probability among the leaves allows us to discard any branch associated with a small probability. In practice, we observe a reduction of the complexity to O(h log2 (k)) at test time. This approach is further extended to compute the T-top targets at the cost of O(log(T)), using a binary heap. Bag of words is invariant to word order but taking explicitly this order into account is often computationally very expensive. Instead, we use a bag of n-grams as additional features to capture some partial information about the local word order.","This signifies that a node's likelihood is always less than its parent's. Searching the tree depth-first while tracking the max probability of the leaves enables pruning branches with low probability. We see test time complexity fall to O(h log2 (k)). We extend this to find the T-top targets in O(log(T)) with a binary heap. Bag of words ignores word order, which is expensive to model explicitly. We add bag of n-grams as features to partially capture local order.","In other words, a node's chance is inferior to its parent's. Scouring the tree top-down and noting the best leaf probability lets us cut branches with small likelihood. We observe test time complexity of O(h log2 (k)). We further this to get the T-top targets in O(log(T)) using a binary heap. Bag of words disregards word order, though modeling it is computationally costly. We use bag of n-grams as extra features to partially capture local order.  ","To clarify, a node's probability is less than its parent's. Traversing the tree depth-first and tracking the maximum leaf probability enables pruning low probability branches. We see test time complexity of O(h log2 (k)). We extend this to find the T-top targets in O(log(T)) with a binary heap. Bag of words ignores word order, though modeling it is expensive. We add bag of n-grams as features to partially capture local word order.",A,1
991,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"We leverage the Mesh TensorFlow library (Shazeer et al., 2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky, 2014). Much of the previous work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning. In this paper, we are interested in measuring the effect of the quality, characteristics, and size of this unlabeled data. To generate data sets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web. Common Crawl is a publicly-available web archive that provides “web extracted text” by removing markup and other non-text content from the scraped HTML files. ","We use the Mesh TensorFlow code library (Shazeer et al., 2018) to easily implement both model parallelism and data parallelism (Krizhevsky, 2014). A lot of previous work on transfer learning for NLP utilizes large unlabeled data sets for unsupervised learning. In this paper, we want to measure the impact of the quality, traits, and size of this unlabeled data. To create data sets that meet our needs, we use Common Crawl as a source of text scraped from the web. Common Crawl is a public web archive that provides ""web extracted text"" by taking out markup and other non-text content from the scraped HTML files.","We take advantage of the Mesh TensorFlow library (Shazeer et al., 2018) to conveniently implement both model parallelism and data parallelism (Krizhevsky, 2014). Much previous research on transfer learning for NLP employs large unlabeled data sets for unsupervised learning. In this paper, we are interested in assessing the effect of the quality, features, and size of this unlabeled data. To generate data sets that fulfill our requirements, we use Common Crawl as a source of text scraped from the web. Common Crawl is a publicly available web archive that furnishes ""web extracted text"" by removing markup and other non-text content from the scraped HTML files.  ","We utilize the Mesh TensorFlow code library (Shazeer et al., 2018) for easy implementation of both model parallelism and data parallelism (Krizhevsky, 2014). A great deal of prior work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning. In this paper, we want to gauge the impact of the quality, attributes, and size of this unlabeled data. To create data sets that meet our needs, we leverage Common Crawl as a source of text scraped from the web. Common Crawl is a public web archive that provides ""web extracted text"" by taking out markup and other non-text content from the scraped HTML files.",A,1
980,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. As such, our work primarily comprises a survey, exploration, and empirical comparison of existing techniques. We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks we consider. In order to perform experiments at this scale, we introduce the “Colossal Clean Crawled Corpus” (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web. ","We want to stress that our intention is not to put forward novel techniques, but rather to give a thorough overview of the current state of the field. Therefore, our work is chiefly a review, investigation, and empirical juxtaposition of present methods. We also probe the boundaries of existing approaches by leveraging the insights from our systematic analysis (training models with up to 11 billion parameters) to achieve cutting-edge performance on many of the tasks we examine. To conduct experiments on this scale, we present the ""Colossal Clean Crawled Corpus"" (C4), a dataset containing hundreds of gigabytes of pristine English language text gathered from the internet.","We emphasize that our aim is not to introduce original approaches, but rather to provide a comprehensive perspective on the status quo of the field. As such, our work is primarily a survey, exploration, and comparative evaluation of current techniques. We also push the limits of existing methods by scaling up the insights from our systematic study (training models with up to 11 billion parameters) to achieve state-of-the-art results on many of the tasks we look at. To carry out experiments at this scale, we introduce the ""Colossal Clean Crawled Corpus"" (C4), a dataset comprising hundreds of gigabytes of uncontaminated English language text scraped from the web.  ","We want to make clear that our objective is not to put forward novel methods, but rather to give a thorough overview of where the field currently stands. Therefore, our work chiefly comprises a review, investigation, and empirical comparison of present techniques. We also extend the boundaries of current approaches by leveraging the insights from our systematic analysis (training models with up to 11 billion parameters) to obtain cutting-edge performance on many of the tasks we consider. In order to conduct experiments at this scale, we present the ""Colossal Clean Crawled Corpus"" (C4), a dataset containing hundreds of gigabytes of unpolluted English text gathered from the internet.",A,1
1272,Neural Machine Translation by Jointly Learning To Align and Translate,"For instance, Schwenk (2012) proposed using a feedforward neural network to compute the score of a pair of source and target phrases and to use the score as an additional feature in the phrase-based statistical machine translation system. More recently, Kalchbrenner and Blunsom (2013) and Devlin et al. (2014) reported the successful use of the neural networks as a sub-component of the existing translation system. Traditionally, a neural network trained as a target-side language model has been used to rescore or rerank a list of candidate translations (see, e.g., Schwenk et al., 2006).","As an illustration, Schwenk (2012) suggested utilizing a feedforward neural network for calculating the score of a pair of source and target phrases. He proposed incorporating the score as a supplementary characteristic in the phrase-based statistical machine translation framework. More recently, Kalchbrenner and Blunsom (2013) as well as Devlin et al. (2014) announced the fruitful application of neural networks as a sub-module of current translation systems. Historically, a neural network educated as a target-side language model has been leveraged for rescoring or reranking a collection of candidate translations (refer to, for example, Schwenk et al., 2006).","To give an example, Schwenk (2012) recommended harnessing a feedforward neural network to determine the rating of a source and target phrase pair. He advised integrating the rating as an extra feature in the phrase-based statistical machine translation structure. More recently, Kalchbrenner and Blunsom (2013) together with Devlin et al. (2014) declared the successful utilization of neural networks as a sub-component of existing translation frameworks. Conventionally, a neural network trained as a target-language model has been exploited to rescore or rerank a set of translation options (see, for instance, Schwenk et al., 2006).  ","As a case in point, Schwenk (2012) put forth employing a feedforward neural network for evaluating the score of a source and target phrase pair. He suggested incorporating the score as an additional characteristic in the phrase-based statistical machine translation model. More recently, Kalchbrenner and Blunsom (2013) along with Devlin et al. (2014) proclaimed the effective application of neural networks as a sub-part of current translation architectures. Traditionally, a neural network conditioned as a target-tongue model has been leveraged to rescore or rerank a collection of translation candidates (refer to, for example, Schwenk et al., 2006).",A,1
1515,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"More interestingly, in Fig. 3, we present 3 patterns that only appear in XLNet but not BERT: (a) The self-exclusion pattern attends to all other tokens but itself, probably offering a fast way to gather global information; (b) The relative-stride pattern attends to positions every a few stride apart relative to the query position; (c) The one-side masked pattern is very similar to the lower-left part of Fig. 1-(d), with the upper-right triangle masked out. It seems that the model learns not to attend the relative right half.","In a more fascinating way, Fig. 3 displays 3 models that are unique to XLNet but absent in BERT: (a) The self-exclusion model pays attention to all tokens except itself, likely providing a rapid approach to collect global details; (b) The relative-stride model attends to locations periodically spaced compared to the query location; (c) The one-side masked model closely resembles the lower-left section of Fig. 1-(d), with the upper-right triangle concealed. It appears the model learns to not focus on the relative right half.","More intriguingly, Fig. 3 exhibits 3 designs existent only in XLNet and not BERT: (a) The self-exclusion design regards all other symbols but itself, probably offering a quick means to accumulate comprehensive information; (b) The relative-stride design considers positions spaced apart at a fixed interval relative to the query position; (c) The one-side masked design highly resembles the lower-left area of Fig. 1-(d), with the upper-right triangle obscured. It seems the model learns to not pay attention to the relative right section.","In a more interesting way, Fig. 3 displays 3 patterns present only in XLNet and absent in BERT: (a) The self-exclusion pattern notes all other marks except itself, likely providing a fast approach to gather global details; (b) The relative-stride pattern focuses on spots periodically spaced compared to the query spot; (c) The one-side masked pattern is very similar to the lower-left part of Fig. 1-(d), with the upper-right triangle concealed. It appears the model learns to not concentrate on the relative right portion.",A,1
1269,Neural Machine Translation by Jointly Learning To Align and Translate,"A similar approach of aligning an output symbol with an input symbol was proposed recently by Graves (2013) in the context of handwriting synthesis. Handwriting synthesis is a task where the model is asked to generate handwriting of a given sequence of characters. In his work, he used a mixture of Gaussian kernels to compute the weights of the annotations, where the location, width and mixture coefficient of each kernel was predicted from an alignment model.","A related method of matching an output character to an input character was put forward not long ago by Graves (2013) for generating handwritten text. Handwriting generation involves getting the model to synthesize handwriting for a particular sequence of letters. He made use of a combination of Gaussian kernels to determine the weights of the annotations, where the placement, size and blend coefficient of each kernel was inferred from an alignment algorithm.","Recently Graves (2013) proposed an analogous system of associating output symbols with input symbols for handwriting synthesis. In handwriting synthesis the model aims to produce handwritten versions of given strings of letters. His approach utilized a mixture of Gaussian kernels for computing annotation weights, with the site, width and mixture weight of each kernel predicted by an alignment scheme. ","A comparable tactic of pairing output characters with input characters was described in 2013 by Graves for handwriting generation. Handwriting generation requires the model to create handwritten forms of specific letter sequences. Graves' method involved Gaussian kernel mixtures to ascertain annotation weights, where the location, scale and mixture component of each kernel was deduced from an alignment model.",A,1
1264,Neural Machine Translation by Jointly Learning To Align and Translate,"In Table 1, we list the translation performances measured in BLEU score. It is clear from the table that in all the cases, the proposed RNNsearch outperforms the conventional RNNencdec. More importantly, the performance of the RNNsearch is as high as that of the conventional phrase-based translation system (Moses), when only the sentences consisting of known words are considered. This is a significant achievement, considering that Moses uses a separate monolingual corpus (418M words) in addition to the parallel corpora we used to train the RNNsearch and RNNencdec.","The data in Table 1 demonstrates that our proposed RNNsearch model achieves higher BLEU scores than the standard RNNencdec across all experiments. Notably, the RNNsearch reaches performance on par with the Moses phrase-based system, which utilizes extra monolingual data, when evaluating only on sentences containing known words. This is a substantial accomplishment given that the RNNsearch and RNNencdec were trained on the same parallel corpora without additional monolingual data.","The BLEU scores listed in Table 1 make it evident that the RNNsearch we put forward surpasses the typical RNNencdec in every case. Most significantly, the RNNsearch's performance rivals that of the conventional phrase-based Moses system, when restricting to sentences with only familiar words. This is an important success, since Moses uses an extra standalone monolingual corpus (418M words) on top of the parallel corpora we utilized to train both the RNNsearch and RNNencdec.  ","Analyzing the BLEU scores in Table 1, we can clearly see our proposed RNNsearch model outperforms the standard RNNencdec model in all experiments. Crucially, the RNNsearch achieves comparable performance to the Moses phrase-based system, which uses additional monolingual data, when evaluating only on sentences with known words. This represents a major achievement given the RNNsearch and RNNencdec were trained on the same parallel corpora without extra monolingual data, unlike Moses.",A,1
797,Attention is All You Need,"The Transformer uses multi-head attention in three different ways: In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.","The Transformer utilizes multi-head consideration in three distinct manners: In ""encoder-decoder consideration"" layers, the inquiries originate from the past decoder layer, and the memory keys and qualities come from the yield of the encoder. This permits each position in the decoder to focus over all positions in the info succession. This copies the common encoder-decoder consideration components in sequence-to-sequence models like [38, 2, 9]. The encoder contains self-consideration layers. In a self-consideration layer all of the keys, qualities and inquiries come from a similar spot, for this situation, the yield of the past layer in the encoder. Each position in the encoder can focus to all positions in the past layer of the encoder.","The Transformer makes use of multi-head notice in three unique ways: In ""encoder-decoder notice"" layers, the questions come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This enables every position in the decoder to pay attention over all positions in the input sequence. This imitates the typical encoder-decoder notice mechanisms in sequence-to-sequence models like [38, 2, 9]. The encoder contains self-notice layers. In a self-notice layer all of the keys, values and questions come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can pay attention to all positions in the previous layer of the encoder.  ","The Transformer utilizes multi-head regard in three distinct manners: In ""encoder-decoder regard"" layers, the inquiries originate from the past decoder layer, and the memory keys and qualities come from the yield of the encoder. This permits each position in the decoder to look over all positions in the info succession. This copies the normal encoder-decoder regard components in sequence-to-sequence models like [38, 2, 9]. The encoder contains self-regard layers. In a self-regard layer all of the keys, qualities and inquiries come from a similar spot, for this situation, the yield of the past layer in the encoder. Each position in the encoder can look to all positions in the past layer of the encoder.",A,1
1418,U-Net_Convolutional Networks for Biomedical Image Segmentation,"The training data is a set of 30 images (512x512 pixels) from serial section transmission electron microscopy of the Drosophila first instar larva ventral nerve cord (VNC). Each image comes with a corresponding fully annotated ground truth segmentation map for cells (white) and membranes (black). The test set is publicly available, but its segmentation maps are kept secret. An evaluation can be obtained by sending the predicted membrane probability map to the organizers. The evaluation is done by thresholding the map at 10 different levels and computation of the “warping error”, the “Rand error” and the “pixel error” [14].","The source material consists of 30 high-resolution images of cell tissue from a fruit fly larva's ventral nerve cord. Each image has a matching fully labeled map outlining the cells and membranes. There is a publicly available test set of images, but the annotation maps are confidential. You can get scored by submitting your predicted membrane layouts to the organizers. They will threshold your maps at 10 levels and calculate the warping, Rand, and pixel errors.","The training images are 30 electron microscopy photos (512x512 pixels) of the ventral nerve cord from a young fruit fly. The images have fully marked ground truth segmentations showing cells (white) and membranes (black). There is a public test set but the segmentations are private. You can get evaluated by sending your predicted membrane probability maps to the organizers. They will threshold the maps at 10 different points and compute the warping, Rand, and pixel errors. ","The data consists of 30 high-resolution electron microscope images of the ventral nerve cord from a Drosophila larva. Each image has a corresponding complete annotation map marking cells in white and membranes in black. There is a test set available publicly but its annotation maps are confidential. You can get scored by submitting your predicted membrane likelihoods to the organizers. They will binarize the maps at 10 thresholds and calculate the warping error, Rand error, and pixel error.",A,1
1530,"You Only Look Once_Unified, Real-Time Object Detection","We parametrize the bounding box x and y coordinates to be offsets of a particular grid cell location so they are also bounded between 0 and 1. We use sum-squared error because it is easy to optimize, however it does not perfectly align with our goal of maximizing average precision. It weights localization error equally with classification error which may not be ideal. Also, in every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, causing training to diverge early on.","We express the x and y coordinates of the bounding box as offsets of a specific grid cell position. This constrains them to be between 0 and 1. We apply sum of squared errors since it's simple to enhance, although it's not a flawless match to our aim of maximizing average precision. It equally weights localization and classification mistakes, which may not be best. Furthermore, in each image numerous grid cells lack any object. This suppresses the ""confidence"" values of those cells towards zero, frequently overpowering the gradient from cells containing objects. This can prompt model variability, resulting in training divergence early on.","We formulate the x and y bounding box coordinates as shifts of a particular grid cell location, bounding them between 0 and 1. We utilize sum of squared errors for straightforward optimization, but it's imperfect for our goal of maximizing average precision. It equates localization and classification errors, potentially suboptimal. Also, many grid cells in each image lack objects. This diminishes their ""confidence"" scores towards zero, often exceeding the gradient from occupied cells. This may cause model instability and early training divergence. ","The bounding box x and y are expressed as offsets of a specific grid cell, constraining them between 0 and 1. Sum of squared errors is used for easy optimization but does not fully match the goal of maximizing average precision. It equally weights localization and classification errors, possibly suboptimally. Additionally, many grid cells in each image are empty. This reduces their ""confidence"" towards zero, frequently surpassing the gradient of filled cells. This can create model instability and premature training divergence.",A,1
1375,Transformer-XL,"As a consequence of the fixed context length, the model cannot capture any longer-term dependency beyond the predefined context length. In addition, the fixed-length segments are created by selecting a consecutive chunk of symbols without respecting the sentence or any other semantic boundary. Hence, the model lacks necessary contextual information needed to well predict the first few symbols, leading to inefficient optimization and inferior performance. We refer to this problem as context fragmentation. To address the aforementioned limitations of fixed-length contexts, we propose a new architecture called Transformer-XL (meaning extra long).","Because of the fixed context length, the model is unable to capture any longer-term dependency that goes beyond the predefined context length. Also, the fixed-length segments are made by choosing a consecutive chunk of symbols without considering the sentence or any other semantic boundary. As a result, the model does not have the necessary contextual information required to accurately predict the first few symbols, resulting in inefficient optimization and poorer performance. We refer to this issue as context fragmentation. To address the limitations of fixed-length contexts that were mentioned, we propose a new architecture called Transformer-XL (meaning extra long).","Due to the constant context length, the model cannot grasp any longer-term connection further than the pre-defined context length. Furthermore, the fixed-length chunks are formed by taking a successive piece of symbols without regarding the sentence or any other semantic limit. Therefore, the model does not have the required contextual data to properly foresee the first few symbols, causing inefficient enhancement and second-rate presentation. We name this problem context disintegration. To tackle the stated constraints of fixed-length contexts, we suggest a new design called Transformer-XL (meaning additional long). ","Because the context length is fixed, the model is not able to understand any longer-term relationship beyond the pre-set context length. Also, the fixed-length segments are made by taking a series of symbols in a row without considering the sentence or any other semantic boundary. As a result, the model lacks the contextual information needed to accurately predict the first few symbols, leading to ineffective optimization and lower performance. We call this issue context fragmentation. To address the limitations of fixed-length contexts that were described, we put forward a new architecture called Transformer-XL (meaning extra long).",A,1
995,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Additionally, since most of our downstream tasks are focused on English-language text, we used langdetect7 to filter out any pages that were not classified as English with a probability of at least 0.99. Our heuristics are inspired by past work on using Common 6. For example, Grave et al. (2018) also filter text using an automatic language detector and discard short lines and Smith et al. (2013); Grave et al. (2018) both perform line-level deduplication. ","Moreover, as the majority of our future work involves analyzing English text, we utilized langdetect7 to remove any pages not identified as English with 99% certainty or higher. Our methods are based on previous research using Common Crawl. For instance, Grave et al. (2018) also filtered text using automated language recognition and ignored short lines, while Smith et al. (2013) and Grave et al. (2018) both carried out deduplication at the line level.","Furthermore, since most of our upcoming assignments focus on English language content, we used langdetect7 to eliminate any pages that were not classified as English with a minimum probability of 0.99. Our techniques are inspired by earlier work using Common Crawl. For example, Grave et al. (2018) also filtered out text using computerized language identification and skipped short lines, and Smith et al. (2013) along with Grave et al. (2018) both performed duplication removal on a per line basis.  ","In addition, as the bulk of our future undertakings center around English verbiage, we leveraged langdetect7 to dispense with any folios not pinpointed as English with a likelihood of no less than 0.99. Our methodologies are roused by past work utilizing Common Crawl. For instance, Grave et al. (2018) likewise sifted text utilizing mechanized language recognition and disregarded short lines, while Smith et al. (2013) and Grave et al. (2018) both did duplication end on a line-by-line premise.",A,1
961,"DistilBERT, a distilled version of BERT","We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTMs.","We evaluate the natural language comprehension and generalizability of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], which contains 9 different datasets for testing natural language processing systems. We present results on the development sets for each task by tuning DistilBERT individually on each task without using ensembling or multi-task tuning schemes (which are largely separate from our current work). We then compare these results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two bidirectional LSTM layers.","We measure the language understanding and extrapolation abilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for assessing natural language processing systems. We show scores on the development sets for each task by adapting DistilBERT on each task separately without utilizing ensembling or multi-task adaptation techniques (which are mostly independent of our current work). We then contrast these scores to the baseline provided by the creators of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two bidirectional LSTM layers.","We test the language comprehension and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a set of 9 datasets for evaluating natural language systems. We present results on the development sets for each task by fine-tuning DistilBERT on each task in isolation without using ensembling or multi-task fine-tuning schemes (which are largely separate from our present work). We then compare these results against the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTM layers.",A,1
1118,Going deeper with convolutions,"This leads to the second idea of the proposed architecture: judiciously applying dimension reductions and projections wherever the computational requirements would increase too much otherwise. This is based on the success of embeddings: even low dimensional embeddings might contain a lot of information about a relatively large image patch. However, embeddings represent information in a dense, compressed form and compressed information is harder to model.","This brings us to the second concept of the suggested design: carefully using dimensionality decreases and projections wherever the computing needs would escalate excessively otherwise. This relies on the success of embeddings: even low dimensional embeddings may encompass ample details about a relatively large image area. However, embeddings denote facts in a packed, condensed form and condensed knowledge is tougher to exemplify.","This guides us to the second principle of the advised framework: prudently employing dimension cutbacks and projections where the calculation demands would swell immoderately otherwise. This depends on the prosperity of embeddings: even low dimensional embeddings could comprise abundant particulars regarding a fairly big image patch. Though, embeddings characterize intelligence in a thick, compressed way and compressed data is more difficult to model. ","This leads into the second tenet of the recommended plan: sensibly applying dimension shrinkages and projections where the computing necessities would rise overly otherwise. This is founded on the triumph of embeddings: even low dimensional embeddings can hold copious information regarding a relatively immense image patch. However, embeddings denote facts in a dense, compacted form and compacted info is harder to demonstrate.",A,1
1408,U-Net_Convolutional Networks for Biomedical Image Segmentation,"The main idea in [9] is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators. Hence, these layers increase the resolution of the output. In order to localize, high resolution features from the contracting path are combined with the upsampled output. A successive convolution layer can then learn to assemble a more precise output based on this information. One important modification in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers.","The primary concept in [9] is to augment a standard contracting network through progressive tiers, where pooling operators are substituted with upsampling operators. Thus, these strata amplify the resolution of the output. To localize, superior resolution characteristics from the contracting course are integrated with the upsampled production. A following convolution layer can then ascertain to assemble a more precise output founded on this data. One vital modification in our framework is that in the upsampling element we have also a substantial amount of feature channels, which enable the network to broadcast context information to elevated resolution tiers.","The main notion in [9] is to supplement a conventional contracting network using sequential layers, where pooling operators are supplanted by upsampling operators. Accordingly, these layers expand the determination of the output. To pinpoint, high-resolution attributes from the contracting path are combined with the upsampled yield. A resulting convolution layer can then figure out how to assemble a more accurate output based on this knowledge. One crucial change in our design is that in the upsampling portion we also have a large quantity of feature channels, which permit the network to disseminate context information to higher resolution layers.  ","The primary idea in [9] is to enhance a standard contracting network through progressive strata, where pooling operators are replaced with upsampling operators. Therefore, these levels increase the resolution of the output. To localize, high-resolution features from the contracting course are integrated with the upsampled product. A subsequent convolution layer can then learn to construct a more precise output founded on this data. One important alteration in our model is that in the upsampling element we also have a substantial number of feature channels, which allow the network to broadcast context information to higher resolution strata.",A,1
1168,ImageNet Classification with Deep Convolutional Neural Networks,"ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images. ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments.","ImageNet is a large dataset containing over 15 million high-resolution images that are categorized into around 22,000 different classes. The images were found on the internet and labeled by human workers using Amazon's Mechanical Turk online platform. Beginning in 2010, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held as part of the Pascal Visual Object Challenge. ILSVRC uses a subset of ImageNet with about 1000 images in each of 1000 categories. Overall, there are about 1.2 million training images, 50,000 validation images, and 150,000 test images. ILSVRC-2010 is the only version of ILSVRC where the test set labels are available, so this is the version we used for most of our experiments.","ImageNet is a dataset with over 15 million high-quality images split into roughly 22,000 different categories. The images were sourced from the internet and classified by human labelers utilizing Amazon's Mechanical Turk crowdsourcing service. Starting in 2010, a yearly competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held as part of the Pascal Visual Object Challenge. ILSVRC uses a subset of ImageNet with around 1000 images in each of 1000 categories. There are about 1.2 million training images, 50,000 validation images, and 150,000 test images total. ILSVRC-2010 is the only ILSVRC version where the test set labels are accessible, so we performed most experiments on this version.  ","ImageNet is a dataset containing more than 15 million high-resolution images organized into approximately 22,000 classes. The images were found online and labeled by human workers using Amazon's Mechanical Turk crowdworking platform. Since 2010, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held as part of the Pascal Visual Object Challenge. ILSVRC uses a subset of ImageNet with around 1000 images in each of 1000 categories. There are roughly 1.2 million training images, 50,000 validation images, and 150,000 test images overall. ILSVRC-2010 is the sole ILSVRC version with available test set labels, hence we conducted most experiments on this version.",A,1
852,BERT,"A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.","A unidirectional model that was trained using a conventional Left-to-Right language model instead of a masked language model. The left-context restriction was also enforced during fine-tuning, as eliminating it led to a pre-training/fine-tuning inconsistency that worsened downstream results. Furthermore, this model was pre-trained without the next sentence prediction task. This can be directly contrasted with OpenAI GPT, but leveraging our larger training data, our input representation, and our fine-tuning methodology.","A model that can only see left context and was educated utilizing a standard Left-to-Right language model rather than a masked language model. The left-only limit was also imposed during fine-tuning, since removing it introduced a mismatch between pre-training and fine-tuning that degraded later performance. Additionally, this model was pre-trained without the next sentence prediction exercise. This can be directly compared to OpenAI's GPT, but uses our bigger training data set, our input representation, and our fine-tuning scheme. ","A model trained to only look left using a conventional Left-to-Right language model instead of a masked language model. The left-only constraint was also applied during fine-tuning, as eliminating it created a discrepancy between pre-training and fine-tuning that hurt later results. Furthermore, this model was pre-trained without the next sentence prediction task. This is directly comparable to OpenAI's GPT, but leverages our larger training dataset, our input representation, and our fine-tuning approach.",A,1
1422,Universal Language Model Fine-tuning for Text Classification,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18- 24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We opensource our pretrained models and code.","Transfer learning with induction has made a big difference in computer vision, but current methods in natural language processing still need customization for each task and full training from the beginning. We suggest Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning technique that can be applied to any natural language processing task, and present methods that are key for adjusting a language model. Our approach substantially surpasses the state-of-the-art on six text classification tasks, lowering the error by 18-24% on most datasets. Also, with only 100 labeled examples, it equals the performance of training from nothing on 100× more data. We make our pretrained models and code publicly available.","Inductive transfer learning has greatly helped computer vision, however existing natural language processing approaches still require task-specific changes and complete training from scratch. We put forward Universal Language Model Fine-tuning (ULMFiT), a successful transfer learning approach usable for any natural language task, and introduce techniques crucial for fine-tuning a language model. Our method significantly exceeds the state-of-the-art on six text classification tasks, reducing the error by 18-24% on most datasets. Furthermore, with just 100 labeled examples, it matches the performance of training from nothing on 100× more data. We open source our pretrained models and code.","Transfer learning using induction has had a big positive impact on computer vision, but current natural language processing methods still need customization for each task and full training from the beginning. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning technique applicable to any natural language task, and present techniques key for adjusting a language model. Our approach substantially outperforms the state-of-the-art on six text classification tasks, lowering the error by 18-24% on the majority of datasets. Also, with only 100 labeled examples, it equals the performance of training from scratch on 100× more data. We make our pretrained models and code publicly available.",A,1
1472,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"At test time, given a trained ConvNet and an input image, it is classified in the following way. First, it is isotropically rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect. 4, using several values of Q for each S leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sermanet et al., 2014). Namely, the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 × 7 conv. layer, the last two FC layers to 1 × 1 conv. layers).","When classifying a new image using a trained ConvNet model, the image is first resized to have its smallest side be length Q (the test scale). Note that Q may not match the training scale S. The resized image is then processed by applying the ConvNet densely across the image, similarly to (Sermanet et al., 2014). Specifically, the fully connected layers are converted to convolutional layers (the first FC layer becomes a 7x7 conv layer, and the last two FC layers become 1x1 conv layers).","To classify a new image with a trained ConvNet, the image is first isotropically rescaled so its shortest side is length Q (the test scale). Q may differ from the training scale S. The network is then applied densely over the rescaled image as in (Sermanet et al., 2014). That is, the fully-connected layers are converted to convolutional layers (the first FC layer becomes a 7x7 conv layer, and the last two FC layers become 1x1 conv layers). ","When classifying a new image using a trained convolutional neural network model, the image is first resized isotropically so its smallest dimension is Q (the test scale). Note that Q may not equal the training scale S. The resized image is then processed by applying the model densely across the image, as in (Sermanet et al., 2014). Specifically, the fully-connected layers are converted to convolutional layers (the first fully-connected layer becomes a 7x7 convolutional layer, and the last two fully-connected layers become 1x1 convolutional layers).",A,1
1529,"You Only Look Once_Unified, Real-Time Object Detection","Ren et al. show that adding both convolutional and connected layers to pretrained networks can improve performance [29]. Following their example, we add four convolutional layers and two fully connected layers with randomly initialized weights. Detection often requires fine-grained visual information so we increase the input resolution of the network from 224 × 224 to 448 × 448. Our final layer predicts both class probabilities and bounding box coordinates. We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1.","Ren and colleagues demonstrate that appending convolutional and fully connected layers to pre-trained networks enhances system performance [29]. Pursuing their approach, we supplement four convolutional strata and two fully connected strata possessing arbitrarily initialized weights. Identifying objects frequently necessitates fine-grained visual data, so we boost the input resolution of the network from 224 × 224 to 448 × 448. Our final stratum predicts class likelihoods and bounding box coordinates. We standardize the bounding box width and height by the image width and height so they range between 0 and 1.","The study by Ren et al. exhibits that integrating both convolutional and dense layers into pretrained models can improve outcomes [29]. Mirroring their methodology, we incorporate four convolutional tiers and two fully connected tiers with randomly assigned weights. Detection often needs precise visual information hence we expand the input dimensions of the network from 224 × 224 to 448 × 448. Our final tier estimates both class probabilities and bounding box coordinates. We normalize the bounding box width and height against the image width and height so they fall in the interval 0 to 1.","Research by Ren and coauthors reveals that appending both convolutional and fully connected strata to pretrained architectures can enhance performance [29]. Emulating their approach, we add four convolutional layers and two fully connected layers possessing arbitrarily initialized weights. Identifying objects frequently necessitates fine-grained visual data, so we increase the input size of the network from 224 × 224 to 448 × 448. Our final layer predicts both class likelihoods and bounding box coordinates. We standardize the bounding box width and height by the image width and height so they are bounded between 0 and 1.",A,1
868,Deep contextualized word representations,"context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora. ","context2vec (Melamud et al., 2016) utilizes a bidirectional Long Short Term Memory neural network (Hochreiter and Schmidhuber, 1997) to encode the surroundings of a central word. Other techniques for learning contextual representations also include the central word itself in the depiction and are calculated with the encoder of either a supervised neural machine translation system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these methodologies benefit from large datasets, however the machine translation approach is constrained by the size of parallel corpora.","context2vec (Melamud et al., 2016) employs a bidirectional Long Short Term Memory neural network (Hochreiter and Schmidhuber, 1997) to encode the context around a focal word. Additional approaches for acquiring contextual representations also incorporate the focal word itself in the representation and are derived using the encoder of either a supervised neural machine translation model (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these techniques benefit from substantial datasets, though the machine translation technique is limited by the extent of parallel corpora.  ","context2vec (Melamud et al., 2016) makes use of a bidirectional Long Short Term Memory neural network (Hochreiter and Schmidhuber, 1997) to encode the surroundings of a key word. Other methods for obtaining contextual representations also include the key word itself in the representation and are generated using the encoder of either a supervised neural machine translation system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these approaches take advantage of large datasets, although the machine translation approach is constrained by the size of parallel corpora.",A,1
1249,Language Models are Unsupervised Multitask Learners,"The LAMBADA dataset (Paperno et al., 2016) tests the ability of systems to model long-range dependencies in text. The task is to predict the final word of sentences which require at least 50 tokens of context for a human to successfully predict. GPT-2 improves the state of the art from 99.8 (Grave et al., 2016) to 8.6 perplexity and increases the accuracy of LMs on this test from 19% (Dehghani et al., 2018) to 52.66%. Investigating GPT-2’s errors showed most predictions are valid continuations of the sentence, but are not valid final words.","The LAMBADA dataset (Paperno et al., 2016) evaluates the capability of systems to represent long-range reliances in text. The objective is to predict the last word of sentences which need at least 50 tokens of context for a human to successfully predict. GPT-2 enhances the state of the art from 99.8 (Grave et al., 2016) to 8.6 perplexity and expands the precision of LMs on this evaluation from 19% (Dehghani et al., 2018) to 52.66%. Examining GPT-2's mistakes showed most forecasts are legitimate continuations of the sentence, but are not valid final words.","The LAMBADA dataset (Paperno et al., 2016) tests the ability of models to capture long-distance dependencies in language. The goal is to foresee the final term of sentences which require a minimum of 50 tokens of context for a person to accurately predict. GPT-2 improves the previous best from 99.8 (Grave et al., 2016) to 8.6 perplexity and increases the performance of LMs on this benchmark from 19% (Dehghani et al., 2018) to 52.66%. Analyzing GPT-2's errors revealed most predictions are plausible extensions of the sentence, but are not correct final terms.","The LAMBADA dataset (Paperno et al., 2016) evaluates the capacity of systems to represent long-span associations in text. The challenge is to predict the concluding word of sentences which need at least 50 tokens of context for a human to successfully predict. GPT-2 enhances the prior best from 99.8 (Grave et al., 2016) to 8.6 perplexity and boosts the accuracy of LMs on this assessment from 19% (Dehghani et al., 2018) to 52.66%. Reviewing GPT-2's mistakes showed most guesses are valid progressions of the sentence, but are not valid concluding words.",A,1
1499,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in question answering. We now discuss how we pretrain XLNet to model multiple segments in the autoregressive framework. During the pretraining phase, following BERT, we randomly sample two segments (either from the same context or not) and treat the concatenation of two segments as one sequence to perform permutation language modeling. We only reuse the memory that belongs to the same context.","Numerous later tasks have more than one input part, like a question and background section in querying. We will now talk about how we pre-train XLNet to represent multiple parts in the self-regressing structure. During pre-training, after BERT, we arbitrarily choose two parts (either from the same context or not) and handle putting the two parts together as one sequence to do permutation language modeling. We only reuse the memory that is part of the same context.","Many subsequent jobs have multiple input chunks, for instance, an ask and a context paragraph in asking. We will now examine how we pre-prepare XLNet to model multiple chunks in the self-referencing framework. During the pre-prep phase, following BERT, we randomly pick two chunks (either from the same context or not) and treat the combination of two chunks as one sequence to perform permutation language modeling. We only reuse the memory that belongs to the same context.","A lot of later tasks have multiple input segments, like a question and background paragraph in questioning. We will now discuss how we pre-train XLNet to represent multiple segments in the self-referential structure. During pre-training, after BERT, we randomly select two segments (either from the same context or not) and handle combining the two segments as one sequence to do permutation language modeling. We only reuse the memory that is part of the same context.",A,1
1251,Language Models are Unsupervised Multitask Learners,"The Winograd Schema challenge (Levesque et al., 2012) was constructed to measure the capability of a system to perform commonsense reasoning by measuring its ability to resolve ambiguities in text. Recently Trinh & Le (2018) demonstrated significant progress on this challenge using LMs, by predicting the resolution of the ambiguity with higher probability. We follow their problem formulation and visualize the performance of our models with both full and partial scoring techniques in Figure 3. GPT-2 improves state of the art accuracy by 7%, achieving 70.70%. The dataset is quite small with only 273 examples so we recommend reading Trichelair et al. (2018) to help contextualize this result.","The Winograd Schema challenge (Levesque et al., 2012) was made to gauge the ability of a system to use common sense by testing its skill to clear up unclear parts in writing. Not long ago, Trinh & Le (2018) showed major progress on this challenge by using LMs, by guessing the resolution of the ambiguity with greater probability. We follow their formulation of the problem and visualize the performance of our models with both complete and partial scoring techniques in Figure 3. GPT-2 gets better state of the art accuracy by 7%, getting 70.70%. The dataset is quite small with only 273 examples so we suggest reading Trichelair et al. (2018) to help put this result in context.","The Winograd Schema challenge (Levesque et al., 2012) was designed to quantify the skill of a system at using commonsense reasoning by evaluating its competence to resolve ambiguities in text. Recently, Trinh & Le (2018) exhibited significant advancements on this challenge by employing LMs, by predicting the clarification of the ambiguity with higher probability. We adopt their formulation of the problem and depict the performance of our models with both full and partial scoring techniques in Figure 3. GPT-2 improves the state-of-the-art accuracy by 7%, achieving 70.70%. Since the dataset contains only 273 examples, we recommend referring to Trichelair et al. (2018) to assist with contextualizing this result.  ","The Winograd Schema challenge (Levesque et al., 2012) was made to gauge the ability of a system to reason practically by testing its skill to resolve unclear meanings in writing. Not long ago, Trinh & Le (2018) showed major improvements on this challenge by employing LMs, by forecasting the explanation of the ambiguity with greater probability. We take on their formulation of the problem and picture the performance of our models with both complete and incomplete scoring techniques in Figure 3. GPT-2 gets better the best accuracy so far by 7%, getting 70.70%. Because the dataset has only 273 examples, we suggest looking at Trichelair et al. (2018) to help put this result in context.",A,1
1138,ImageNet A Large_Scale Hierarchical Image Database,"We would like to offer a clean dataset at all levels of the WordNet hierarchy. Fig. 4 demonstrates the labeling precision on a total of 80 synsets randomly sampled at different tree depths. An average of 99.7% precision is achieved on average. Achieving a high precision for all depths of the ImageNet tree is challenging because the lower in the hierarchy a synset is, the harder it is to classify, e.g. Siamese cat versus Burmese cat.","We want to provide a pristine dataset at all tiers of the WordNet structure. Fig. 4 shows the labeling accuracy on a total of 80 synsets arbitrarily chosen at various tree profundities. An normal of 99.7% accuracy is accomplished on average. Attaining a high accuracy for all depths of the ImageNet tree is tough because the lower in the hierarchy a synset is, the more problematic it is to categorize, e.g. Siamese cat versus Burmese cat.","Our goal is to make available an uncontaminated dataset across all levels of the WordNet taxonomy. Fig. 4 displays the labeling precision on 80 synsets randomly selected from different tree heights. On average, 99.7% precision is reached on average. Getting high precision across all layers of the ImageNet tree is challenging because the further down the hierarchy a synset is, the harder it is to classify it correctly, for example Siamese cat vs Burmese cat.","We aspire to furnish an unpolluted dataset at every grade of the WordNet categorization. Fig. 4 exhibits the labeling accuracy on a sum of 80 synsets arbitrarily picked from various tree depths. An standard of 99.7% accuracy is realized on average. Attaining high accuracy across all tiers of the ImageNet tree is difficult since the lower in the categorization a synset is, the more complicated it is to identify correctly, e.g. Siamese cat versus Burmese cat.",A,1
1406,U-Net_Convolutional Networks for Biomedical Image Segmentation,"Hence, Ciresan et al. [ 1] trained a network in a sliding-window setup to predict the class label of each pixel by providing a local region (patch) around that pixel as input. First, this network can localize. Secondly, the training data in terms of patches is much larger than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a large margin. Obviously, the strategy in Ciresan et al. [1] has two drawbacks. First, it is quite slow because the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-off between localization accuracy and the use of context.","Therefore, Ciresan and colleagues [1] taught a neural network in a sliding window configuration to predict the class label of every pixel by feeding a local area (patch) surrounding that pixel as input. Firstly, this network is capable of localization. Secondly, the training data as patches is much more abundant than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a wide margin. Clearly, the strategy in Ciresan et al. [1] has two flaws. First, it is quite slow since the network must be run independently for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-off between localization accuracy and the use of context.","As such, Ciresan and co-authors [1] trained a neural network using a sliding window method to forecast the class label of each pixel by providing a local region (patch) around that pixel as input. For one, this network is able to localize. Additionally, the training data as patches is far more plentiful than the number of training images. The ensuing network won the EM segmentation challenge at ISBI 2012 by a substantial margin. Obviously, the strategy in Ciresan et al. [1] has two downsides. Firstly, it is quite slow as the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-off between localization accuracy and the use of context.  ","Therefore, Ciresan and fellow researchers [1] conditioned a neural network using a sliding window technique to predict the class label of every pixel by feeding a local area (patch) surrounding that pixel as input. On one hand, this network has localization capabilities. On the other hand, the training data as patches is much more abundant than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a wide margin. Clearly, the strategy in Ciresan et al. [1] has two disadvantages. For one, it is quite slow since the network must be run independently for each patch, and there is redundancy due to overlapping patches. For another, there is a trade-off between localization accuracy and context usage.",A,1
863,Deep contextualized word representations,"Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus. For this reason, we call them ELMo (Embeddings from Language Models) representations. Unlike previous approaches for learning contextualized word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM. ","Our representations are different from standard word embeddings because each word is given a representation based on the full input sentence. We utilize vectors from a bidirectional LSTM trained with a combined language model goal on a large text dataset. Therefore, we name them ELMo representations. In contrast to past approaches for learning context-dependent word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, meaning they depend on all the internal layers of the biLM.","Our representations diverge from conventional word type embeddings since every token receives a depiction reliant on the whole input statement. We employ vectors derived from a bidirectional LSTM cultivated with a coupled language model aim on an expansive text corpus. For this rationale, we entitle them ELMo delineations. Dissimilar to preceding methodologies for acquiring contextualized word vectors (Peters et al., 2017; McCann et al., 2017), ELMo delineations are profound, in the sense that they are a capacity of all of the internal layers of the biLM.  ","Our representations differ from traditional word embeddings in that each token gets a depiction based on the full input sentence. We utilize vectors from a bidirectional LSTM trained with a combined language modeling goal on a large text dataset. Therefore, we term them ELMo representations. Unlike past approaches for acquiring context-dependent word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, meaning they depend on all the internal layers of the biLM.",A,1
1286,RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We reimplement BERT in FAIRSEQ (Ott et al., 2019). We primarily follow the original BERT optimization hyperparameters, given in Section 2, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting. We additionally found training to be very sensitive to the Adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it. Similarly, we found setting β2 = 0.98 to improve stability when training with large batch sizes. We pretrain with sequences of at most T = 512 tokens.","We re-create BERT using the FAIRSEQ framework (Ott et al., 2019). We mostly use the same optimization hyperparameters from the original BERT, listed in Section 2, except we adjust the maximum learning rate and number of warmup steps for each scenario. We also found that tweaking the Adam epsilon parameter improved performance and stability in some cases. Likewise, setting β2 = 0.98 helped stabilize training when using large batch sizes. Our pretraining used sequences with up to T = 512 tokens.","We implement BERT again in the FAIRSEQ library (Ott et al., 2019). We follow most of the original BERT optimization hyperparameters given in Section 2, but tune the peak learning rate and warmup steps separately for each setting. We also discovered that modifying the Adam epsilon term improved performance and stability sometimes. Similarly, setting β2 = 0.98 enhanced stability during training with large batches. Our pretraining used sequences with a maximum of T = 512 tokens.  ","We build BERT again using FAIRSEQ (Ott et al., 2019). We adhere to most of the original BERT optimization hyperparameters from Section 2, except we customize the maximum learning rate and number of warmup steps per setting. We also found that adjusting the Adam epsilon parameter boosted performance and stability in certain cases. Likewise, using β2 = 0.98 helped stabilize training when employing large batch sizes. Our pretraining utilized sequences capped at T = 512 tokens.",A,1
1268,Neural Machine Translation by Jointly Learning To Align and Translate,"The strength of the soft-alignment, opposed to a hard-alignment, is evident, for instance, from Fig. 3 (d). Consider the source phrase [the man] which was translated into [l’ homme]. Any hard alignment will map [the] to [l’] and [man] to [homme]. This is not helpful for translation, as one must consider the word following [the] to determine whether it should be translated into [le], [la], [les] or [l’]. Our soft-alignment solves this issue naturally by letting the model look at both [the] and [man], and in this example, we see that the model was able to correctly translate [the] into [l’]. We observe similar behaviors in all the presented cases in Fig. 3.","The power of the flexible alignment, as opposed to a rigid alignment, is clear, for example, from Fig. 3 (d). Look at the source phrase [the man] which was translated into [l' homme]. Any rigid alignment will map [the] to [l'] and [man] to [homme]. This is not useful for translation, as one must consider the word after [the] to decide if it should be translated into [le], [la], [les] or [l']. Our flexible alignment solves this issue naturally by letting the model look at both [the] and [man], and in this example, we see that the model was able to correctly translate [the] into [l']. We notice similar behaviors in all the presented cases in Fig. 3.","The advantage of the adaptable matching, in contrast to an inflexible matching, is obvious, as seen in Fig. 3 (d). Examine the source phrase [the man] which was converted into [l' homme]. Any inflexible matching will connect [the] to [l'] and [man] to [homme]. This is not beneficial for translation, as one must look at the word following [the] to decide if it should be translated into [le], [la], [les] or [l']. Our adaptable matching resolves this issue naturally by permitting the model to examine both [the] and [man], and in this example, we observe that the model was able to accurately translate [the] into [l']. We notice similar behaviors in all the shown cases in Fig. 3.  ","The benefit of the adjustable alignment, as opposed to a fixed alignment, is apparent, as shown in Fig. 3 (d). Focus on the source phrase [the man] which was rendered as [l' homme]. Any fixed alignment will link [the] to [l'] and [man] to [homme]. This is not useful for translation, as one must look at the word after [the] to determine if it should be translated as [le], [la], [les] or [l']. Our adjustable alignment resolves this issue organically by allowing the model to analyze both [the] and [man], and in this example, we discern that the model was able to accurately translate [the] into [l']. We detect similar behaviors in all the presented cases in Fig. 3.",A,1
859,BERT,"To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer. Results are presented in Table 7. BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both finetuning and feature-based approaches.","To evaluate the fine-tuning method, we use the feature-extraction approach by taking out the activations from one or more tiers without adjusting any BERT parameters. These context-based embeddings are utilized as inputs to an arbitrarily started two-layer 768-dimensional BiLSTM before the categorization layer. Outcomes are exhibited in Table 7. BERTLARGE acts competitively with cutting edge techniques. The best performing technique concatenates the token representations from the top four concealed layers of the pre-prepared Transformer, which is just 0.3 F1 behind fine-tuning the whole model. This shows that BERT is compelling for both finetuning and feature-based methodologies.","To assess the fine-tuning procedure, we implement the feature-extraction method by extracting the activations from one or more levels without modifying any BERT parameters. These context-dependent embeddings are fed into a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer. Results are presented in Table 7. BERTLARGE performs comparably with state-of-the-art approaches. The top performing approach joins together the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the whole model. This proves that BERT is effective for both finetuning and feature-extraction methods.  ","To evaluate the fine-tuning technique, we use the feature-based approach by taking out the activations from one or more strata without changing any BERT parameters. These context-reliant embeddings are utilized as inputs to an arbitrarily begun two-layer 768-dimensional BiLSTM before the categorization layer. Outcomes are shown in Table 7. BERTLARGE acts competitively with cutting edge strategies. The best performing strategy consolidates the token representations from the top four concealed layers of the pre-prepared Transformer, which is just 0.3 F1 behind fine-tuning the entire model. This exhibits that BERT is compelling for both finetuning and feature-based methodologies.",A,1
1022,Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.1 We model this process with a fully convolutional network [14], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN object detection network [5], we assume that both nets share a common set of conv layers. In our experiments, we investigate the Zeiler and Fergus model [23] (ZF), which has 5 shareable conv layers and the Simonyan and Zisserman model [19] (VGG), which has 13 shareable conv layers.","A Region Proposal Network (RPN) accepts an image (of any dimension) as input and generates a collection of rectangular object proposals, each with an objectness score. We represent this process with a fully convolutional neural network [14], which we elucidate in this section. Because our final aim is to share computation with a Fast R-CNN object detection network [5], we presume that both networks share a common set of convolutional layers. In our experiments, we analyze the Zeiler and Fergus model [23] (ZF), which has 5 shareable convolutional layers and the Simonyan and Zisserman model [19] (VGG), which has 13 shareable convolutional layers.","A Region Proposal Network (RPN) takes an image (any size) and outputs rectangular region proposals, each with a score indicating if it contains an object. We implement this with a fully convolutional network [14], described here. Our goal is to share computation with Fast R-CNN [5], so we assume both networks share convolutional layers. We test the Zeiler and Fergus [23] model (ZF), with 5 shareable convolutional layers, and the Simonyan and Zisserman [19] model (VGG), with 13 shareable convolutional layers.  ","A Region Proposal Network (RPN) inputs an image (any dimensions) and generates rectangular object proposals, each scored for object presence. We model this with a fully convolutional network [14], detailed here. Our aim is computational sharing with Fast R-CNN [5], so both networks share convolutional layers. We evaluate the Zeiler and Fergus [23] model (ZF), with 5 shareable convolutional layers, and the Simonyan and Zisserman [19] model (VGG), with 13 shareable convolutional layers.",A,1
1175,ImageNet Classification with Deep Convolutional Neural Networks,"This scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al. [11], but ours would be more correctly termed “brightness normalization”, since we do not subtract the mean activity. Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer CNN achieved a 13% test error rate without normalization and 11% with normalization.","This plan has some similarities to the local contrast standardization approach of Jarrett and colleagues [11], however ours would be more accurately called ""brightness standardization"", since we do not subtract the average activity. Response standardization decreases our top-1 and top-5 error percentages by 1.4% and 1.2%, respectively. We also confirmed the effectiveness of this approach on the CIFAR-10 data set: a four-layer CNN attained a 13% test error rate without standardization and 11% with standardization.","This system has some parallels to the local contrast normalization method of Jarrett's group [11], but ours would be more precisely termed ""brightness normalization"", as we do not subtract the mean activity. Normalizing the response reduces our top-1 and top-5 error rates by 1.4% and 1.2%, in that order. We also verified the usefulness of this system on the CIFAR-10 data set: a four-layer CNN achieved a 13% test error rate without normalization and 11% with normalization. ","This approach has some similarities to the local contrast normalization technique of Jarrett and co-authors [11], however ours would be more accurately called ""brightness normalization"", since we do not subtract the average activity. Normalizing the response decreases our top-1 and top-5 error percentages by 1.4% and 1.2%, respectively. We also confirmed the effectiveness of this technique on the CIFAR-10 dataset: a four-layer CNN obtained a 13% test error rate without normalization and 11% with normalization.",A,1
1270,Neural Machine Translation by Jointly Learning To Align and Translate,"More specifically, his alignment was restricted to predict the location such that the location increases monotonically. The main difference from our approach is that, in (Graves, 2013), the modes of the weights of the annotations only move in one direction. In the context of machine translation, this is a severe limitation, as (long-distance) reordering is often needed to generate a grammatically correct translation (for instance, English-to-German). Our approach, on the other hand, requires computing the annotation weight of every word in the source sentence for each word in the translation. This drawback is not severe with the task of translation in which most of input and output sentences are only 15–40 words.","To be more precise, his method was limited to forecasting the position in a way that increased steadily. The main contrast with our technique is that, in Graves (2013), the peaks of the annotation weights were only permitted to shift in one direction. For machine translation, this severely restricts the approach, since reordering words significantly (over long distances) is frequently necessary to produce a grammatically correct translation (for example, from English to German). Our method, on the other hand, needs to calculate the annotation weight for every word in the source sentence for each word in the translation. This downside is not very problematic for translation tasks where most input and output sentences are only 15-40 words long.","Specifically, his system could only predict the location while enforcing that it increased monotonically. The primary difference from our method is that, in Graves (2013), the modes of the annotation weights were constrained to move in just one direction. In machine translation, this greatly limits the approach, because extensive reordering of words (over long distances) is often essential to generate a grammatically correct translation (for instance, English to German). Our approach, conversely, requires computing the annotation weight for every source sentence word for each translated word. This disadvantage is not too concerning for translation tasks where most input and output sentences are only 15-40 words in length.  ","More exactly, his system was limited to predicting the position in a way that increased monotonically. The main divergence from our technique is that, in Graves (2013), the peaks of the annotation weights were restricted to shift in only one direction. For machine translation, this seriously hampers the method, since substantially reordering words (over long spans) is often crucial for producing a grammatically correct translation (for example, English to German). Our technique, on the other hand, necessitates calculating the annotation weight for every source sentence word for each translated word. This drawback is not very problematic for translation tasks in which most input and output sentences are only 15-40 words long.",A,1
850,BERT,"The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference (Zellers et al., 2018). Given a sentence, the task is to choose the most plausible continuation among four choices. When fine-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). ","The Situations With Adversarial Generations (SWAG) dataset has 113,000 sentence-pair completion samples that assess sensible commonsense deduction (Zellers et al., 2018). With a given sentence, the objective is to select the most probable continuation from four options. When fine-tuning on the SWAG dataset, we make four input sequences, each containing the combination of the provided sentence (sentence A) and a potential continuation (sentence B).","The Situations With Adversarial Generations (SWAG) dataset possesses 113,000 sentence-pair completion instances that evaluate grounded commonsense inference (Zellers et al., 2018). Provided a sentence, the goal is to choose the most plausible continuation out of four choices. When adapting on the SWAG dataset, we build four input sequences, each holding the fusion of the given sentence (sentence A) and a feasible continuation (sentence B).  ","The Situations With Adversarial Generations (SWAG) dataset has 113,000 sentence-pair completion examples that assess sensible commonsense reasoning (Zellers et al., 2018). Given a sentence, the aim is to select the most probable continuation from four options. When tuning on the SWAG dataset, we construct four input sequences, each containing the union of the provided sentence (sentence A) and a potential continuation (sentence B).",A,1
1440,Universal Language Model Fine-tuning for Text Classification,"Fine-tuning the target classifier is the most critical part of the transfer learning method. Overly aggressive fine-tuning will cause catastrophic forgetting, eliminating the benefit of the information captured through language modeling; too cautious fine-tuning will lead to slow convergence (and resultant overfitting). Besides discriminative finetuning and triangular learning rates, we propose gradual unfreezing for fine-tuning the classifier. Rather than fine-tuning all layers at once, which risks catastrophic forgetting, we propose to gradually unfreeze the model starting from the last layer as this contains the least general knowledge (Yosinski et al., 2014): We first unfreeze the last layer and fine-tune all unfrozen layers for one epoch.","Adjusting the target classifier is the most important part of the transfer learning technique. Being too aggressive with adjusting will result in forgetting everything that was learned, removing any benefit from the information obtained through language modeling. Being too cautious will lead to slow improvements (and overfitting). In addition to discriminative fine-tuning and triangular learning rates, we suggest steadily unfreezing for adjusting the classifier. Rather than adjusting all layers at once, which risks forgetting everything, we propose to slowly unfreeze the model starting with the last layer since it contains the least general knowledge (Yosinski et al., 2014): We first unfreeze the final layer and tune all unfrozen layers for one epoch.","Modifying the target classifier is the most crucial aspect of the transfer learning process. Modifying too drastically will make the model forget everything it learned, eliminating the usefulness of the information from language modeling. Modifying too little will result in slow progress (and overfitting). On top of discriminative modification and triangular learning rates, we recommend gradually unfreezing for modifying the classifier. Instead of modifying all layers simultaneously, which risks forgetting, we suggest gradually unfreezing the model beginning with the last layer as it has the least general knowledge (Yosinski et al., 2014): We first unfreeze the final layer and adjust all unfrozen layers for one epoch. ","Tuning the target classifier is the most important part of transfer learning. Tuning too aggressively will make the model forget everything, removing any benefit from the language modeling information. Tuning too cautiously will lead to slow improvements (and overfitting). In addition to discriminative tuning and triangular learning rates, we propose progressively unfreezing for tuning the classifier. Rather than tuning all layers at once, which risks forgetting everything, we suggest progressively unfreezing the model starting with the last layer since it has the least general knowledge (Yosinski et al., 2014): We first unfreeze the final layer and refine all unfrozen layers for one epoch.",A,1
1545,"You Only Look Once_Unified, Real-Time Object Detection","First we compare YOLO with other real-time detection systems on PASCAL VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.","Initially we contrast YOLO with additional real-time identification frameworks on PASCAL VOC 2007. To comprehend the variances between YOLO and R-CNN variants we investigate the mistakes on VOC 2007 created by YOLO and Fast R-CNN, one of the highest functioning iterations of R-CNN [14]. Grounded on the divergent error profiles we exhibit that YOLO can be utilized to rescore Fast R-CNN identifications and diminish the errors from background false positives, providing a significant performance enhancement. We also proffer VOC 2012 outcomes and compare mAP to current state-of-the-art techniques. Finally, we demonstrate that YOLO generalizes to new domains superior to other detectors on two artwork datasets.","We first juxtapose YOLO with other instantaneous detection systems on PASCAL VOC 2007. To grasp the differences between YOLO and R-CNN versions we explore the inaccuracies on VOC 2007 made by YOLO and Fast R-CNN, one of the top performing variants of R-CNN [14]. Based on the contrasting error profiles we show that YOLO can be employed to rescore Fast R-CNN detections and decrease the errors from background false positives, providing a significant performance boost. We also present VOC 2012 results and contrast mAP to current most advanced methods. Finally, we exhibit that YOLO generalizes to new domains better than other detectors on two artwork datasets. ","Initially we compare YOLO with additional real-time identification systems on PASCAL VOC 2007. To comprehend the divergences between YOLO and R-CNN variants we inspect the mistakes on VOC 2007 created by YOLO and Fast R-CNN, one of the highest functioning iterations of R-CNN [14]. Grounded on the differing error profiles we demonstrate that YOLO can be utilized to rescore Fast R-CNN identifications and reduce the errors from background false positives, providing a significant performance enhancement. We also offer VOC 2012 outcomes and contrast mAP to current most advanced techniques. Finally, we establish that YOLO generalizes to new domains superior to other detectors on two artwork datasets.",A,1
955,"DistilBERT, a distilled version of BERT","The last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns.","In the previous 24 months, there has been an increase in the use of Transfer Learning techniques in Natural Language Processing (NLP). Large pre-trained language models have become a fundamental instrument in numerous NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. Although these models result in considerable enhancement, they frequently contain hundreds of millions of parameters. Current research on pre-trained models shows that even larger models lead to superior performance on downstream assignments. The tendency to create bigger models raises multiple issues.","Over the past two years, Transfer Learning methods have become more popular in Natural Language Processing (NLP). Massive pre-trained language models are now a standard tool for many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models significantly improve performance, they often have hundreds of millions of parameters. Existing research on pre-trained models indicates that developing even larger models continues to improve performance on downstream tasks. The trend toward larger models raises several concerns.","In the previous 24 months, there has been growing usage of Transfer Learning techniques in Natural Language Processing (NLP). Gigantic pre-trained language models have turned into a fundamental tool in numerous NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. Despite the fact that these models result in major enhancement, they frequently contain hundreds of millions of parameters. Current research on pre-trained models demonstrates that bigger models still lead to superior performance on downstream assignments. The tendency toward larger models brings up several concerns.",A,1
836,BERT,"Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Turian et al., 2010). To pretrain word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al., 2013)","Studying representations of words that can be applied in many contexts has been a dynamic research area for many years. This includes techniques that do not use neural networks (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) as well as neural techniques (Mikolov et al., 2013; Pennington et al., 2014). Pre-trained word vectors are a key component of modern natural language processing systems, providing major gains compared to vectors learned from the beginning (Turian et al., 2010). To pre-train word embedding vectors, language modeling objectives from left to right have been utilized (Mnih and Hinton, 2009), along with objectives to tell apart accurate words from inaccurate words given left and right context (Mikolov et al., 2013).","Investigating widely useful representations of words has been a lively field of study for multiple decades. This comprises non-neural methods (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural network methods (Mikolov et al., 2013; Pennington et al., 2014). Pre-trained word embeddings are a fundamental piece of modern NLP systems, giving huge improvements over embeddings learned from nothing (Turian et al., 2010). To pre-train word embedding vectors, left-to-right language modeling goals have been employed (Mnih and Hinton, 2009), together with goals to differentiate correct words from incorrect words given left and right context (Mikolov et al., 2013).  ","Exploring representations of words that can be widely applied has been an energetic area of research for many years. This includes techniques not involving neural networks (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and techniques using neural networks (Mikolov et al., 2013; Pennington et al., 2014). Pre-trained word vectors are an essential component of modern natural language systems, providing major enhancements compared to vectors learned from scratch (Turian et al., 2010). To pre-train word embedding vectors, language modeling aims from left to right have been used (Mnih and Hinton, 2009), along with aims to distinguish accurate words from inaccurate words given surrounding left and right context (Mikolov et al., 2013).",A,1
1403,U-Net_Convolutional Networks for Biomedical Image Segmentation,"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks.","There is widespread agreement that effectively training deep neural networks necessitates numerous annotated examples. Here, we introduce a network design and training methodology that strongly leverages data augmentation to more efficiently utilize the available labeled data. The model comprises a contracting path to capture context and a symmetric expanding path enabling accurate localization. We demonstrate that such a network can be trained end-to-end from very few images and surpasses the previous best approach (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopy stacks.",It is broadly accepted that thousands of annotated training instances are needed to successfully train deep learning models. We present a network architecture and training procedure that heavily relies on data augmentation to get more value out of the available labeled data. The design has a contracting portion to capture context and a matching expanding portion for precise localization. We show this network can be trained end-to-end from very little data and beats the former top method (a sliding-window convolutional network) on the ISBI challenge for segmenting neuronal structures in electron microscopy stacks.,There is widespread consensus that training deep neural networks well requires many thousands of labeled training examples. We introduce a network design and training strategy that makes strong use of data augmentation to utilize the available annotated data more efficiently. The architecture has a contracting path to capture context and a symmetric expanding path that allows precise localization. We demonstrate this network can be trained end-to-end from very few images and outperforms the previous best approach (a sliding-window convolutional network) on the ISBI challenge for segmenting neuronal structures in electron microscopy image stacks.,A,1
924,Deep Residual Learning for Image Recognition,"When the net is “not overly deep” (18 layers here), the current SGD solver is still able to find good solutions to the plain net. In this case, the ResNet eases the optimization by providing faster convergence at the early stage. We have shown that 3x3, 64 1x1, 64 relu 1x1, 256 relu relu 3x3, 64 3x3, 64 relu relu 64-d 256-d Figure 5. A deeper residual function F for ImageNet. Left: a building block (on 56×56 feature maps) as in Fig. 3 for ResNet34. Right: a “bottleneck” building block for ResNet-50/101/152. parameter-free, identity shortcuts help with training.","Even when the neural network is relatively shallow (18 layers in this case), the existing stochastic gradient descent algorithm is still capable of finding good solutions for the plain network. Here, the ResNet facilitates optimization by enabling faster convergence early on. We have demonstrated that using identity shortcuts without extra parameters aids training.","When the neural net has limited depth (18 tiers in this instance), the current stochastic gradient descent solver still manages to identify effective solutions for the vanilla network. The ResNet simplifies optimization by providing quicker convergence initially. We've exhibited that parameter-free, identity shortcuts assist with training. ","When the neural network does not have great depth (18 layers here), the present stochastic gradient descent optimizer can still locate good solutions for the basic network. In this situation, the ResNet makes optimization easier by delivering faster convergence at first. We have revealed that identity shortcuts with no parameters help training.",A,1
1143,ImageNet A Large_Scale Hierarchical Image Database,"LabelMe [21] and the Lotus Hill dataset [27] provide 30k and 50k labeled and segmented images, respectively. These two datasets provide complementary resources for the vision community compared to ImageNet. Both only have around 200 categories, but the outlines and locations of objects are provided. ImageNet in its current form does not provide detailed object outlines (see potential extensions in Sec. 5.1), but the number of categories and the number of images per category already far exceeds these two datasets. In addition, images in these two datasets are largely uploaded or provided by users or researchers of the dataset, whereas ImageNet contains images crawled from the entire Internet. The Lotus Hill dataset is only available through purchase.","The LabelMe dataset [21] and the Lotus Hill collection [27] offer 30,000 and 50,000 images respectively that are labeled and segmented. These two resources are complementary for computer vision experts compared to ImageNet. Both only contain about 200 categories, but they include outlines and locations of objects. ImageNet currently does not have detailed object outlines (refer to possible extensions in Section 5.1), however the quantity of categories and images per category already greatly exceeds these two datasets. Also, ImageNet has images crawled from across the internet, while the images in the other two datasets are largely provided by users or researchers involved with those datasets. The Lotus Hill dataset is only accessible through purchase.","The LabelMe [21] and Lotus Hill [27] datasets provide 30k and 50k images with labels and segmentation. These are complementary for computer vision versus ImageNet. They have ~200 categories each but include object outlines and locations, which ImageNet lacks (see Section 5.1). However, ImageNet has far more categories and images per category, with images crawled from the full internet rather than provided by dataset users/researchers. Lotus Hill requires purchase for access.  ","The LabelMe [21] and Lotus Hill [27] datasets have 30,000 and 50,000 labeled and segmented images. These are complementary for computer vision compared to ImageNet. They only have about 200 categories each, but they provide object outlines and locations, unlike ImageNet (see potential additions in Section 5.1). However, ImageNet already has far more categories and images per category, with images crawled from across the internet rather than provided by dataset users or researchers. Lotus Hill is only available through purchase.",A,1
942,Deep Residual Learning for Image Recognition,"In Faster R-CNN, the system is designed to learn region proposals and also object classifiers, so an ensemble can be used to boost both tasks. We use an ensemble for proposing regions, and the union set of proposals are processed by an ensemble of per-region classifiers. Table 9 shows our result based on an ensemble of 3 networks. The mAP is 59.0% and 37.4% on the test-dev set. This result won the 1st place in the detection task in COCO 2015.","The Faster R-CNN system is engineered to develop region recommendations and object classifiers. Therefore, an assembly can enhance both jobs. We utilize an assembly for suggesting areas, and the union collection of proposals are handled by an assembly of per-area categorizers. Table 9 displays our outcome founded on an assembly of 3 systems. The mAP is 59.0% and 37.4% on the test-dev collection. This outcome was victorious in the detection assignment in COCO 2015.","In Faster R-CNN, the framework is intended to learn region ideas and object classifiers too, so a group can boost both tasks. We employ a group for proposing zones, and the joined set of proposals are processed by a group of per-zone categorizers. Table 9 exhibits our result based on a group of 3 networks. The mAP is 59.0% and 37.4% on the test-dev set. This result won 1st place in the detection job in COCO 2015.","The Faster R-CNN framework is built to develop region suggestions and object classifiers as well. Therefore, a coalition can enhance both jobs. We use a coalition for proposing areas, and the combined set of proposals are handled by a coalition of per-area categorizers. Table 9 shows our outcome based on a coalition of 3 networks. The mAP is 59.0% and 37.4% on the test-dev set. This outcome was victorious in the detection task in COCO 2015.",A,1
1509,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"XLNet is a generalized AR pretraining method that uses a permutation language modeling objective to combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to work seamlessly with the AR objective, including integrating Transformer-XL and the careful design of the two-stream attention mechanism. XLNet achieves substantial improvement over previous pretraining objectives on various tasks.","XLNet is a general autoregressive pretraining technique that utilizes a permutation language modeling goal to combine the benefits of autoregressive and autoencoding approaches. The neural network architecture of XLNet is designed to seamlessly work with the autoregressive objective, including integrating Transformer-XL and the meticulous design of the two-stream attention system. XLNet accomplishes considerable enhancement over prior pretraining goals on a variety of tasks.","XLNet is a broad autoregressive pretraining procedure that leverages a permutation language modeling aim to unite the strengths of autoregressive and autoencoding methodologies. The neural network design of XLNet is engineered to smoothly interoperate with the autoregressive aim, encompassing assimilating Transformer-XL and the diligent conception of the two-stream attention framework. XLNet reaches significant betterment over earlier pretraining aims on numerous undertakings. ","XLNet is a wide-ranging autoregressive pretraining technique that harnesses a permutation language modeling purpose to combine the advantages of autoregressive and autoencoding techniques. The neural network architecture of XLNet is constructed to seamlessly collaborate with the autoregressive purpose, encompassing integrating Transformer-XL and the careful creation of the two-stream attention structure. XLNet accomplishes substantial enhancement over prior pretraining purposes on a variety of tasks.",A,1
1185,ImageNet Classification with Deep Convolutional Neural Networks,"We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron biases in the remaining layers with the constant 0. We used an equal learning rate for all layers, which we adjusted manually throughout training.","We set the weights in every layer randomly from a normal distribution with a mean of zero and standard deviation of 0.01. We set the neuron biases in the second, fourth, fifth convolutional layers, and in the fully-connected hidden layers, to 1. This speeds up the initial phase of learning by giving the ReLUs positive inputs. We set the neuron biases in the other layers to 0. We utilized the same learning rate for all layers, which we tuned by hand during training.","We initialized the parameters in each layer by sampling from a Gaussian with zero mean and 0.01 standard deviation. We set the biases in the second, fourth, fifth conv layers and fully connected hidden layers to 1. This boosts early learning by providing ReLUs with positive inputs. We set the biases in the other layers to 0. We used a uniform learning rate for all layers, manually adjusting it over the course of training.  ","We randomly initialized the weights in every layer based on a normal distribution with zero mean and standard deviation of 0.01. We set the biases in the second, fourth, fifth conv layers and fully connected hidden layers to 1. This accelerates the initial learning phase by giving ReLUs positive values. We initialized the biases in the remaining layers to 0. We employed the same learning rate for all layers, tuning it by hand during training.",A,1
794,Attention is All You Need,"Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.","The majority of the most effective neural sequence transduction systems have a structure consisting of an encoder and a decoder [5, 2, 35]. In these systems, the encoder changes an input series of symbolic representations (x1, ..., xn) into a series of constant depictions z = (z1, ..., zn). Given z, the decoder then produces an output series (y1, ..., ym) of symbols one at a time. At each step, the model is auto-regressive [10], using the previously created symbols as extra input when making the next symbol. The Transformer employs this general design using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown on the left and right halves of Figure 1, respectively.","Most of the top-performing neural sequence transduction architectures have an encoder-decoder form [5, 2, 35]. The encoder turns an input sequence of symbol representations (x1, ..., xn) into a sequence of continuous representations z = (z1, ..., zn). With z as input, the decoder generates an output sequence (y1, ..., ym) of symbols one at a time. At each generation step, the model is auto-regressive [10], taking in the previously generated symbols as extra context when producing the next symbol. The Transformer uses this high-level design with stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, as shown in the left and right halves of Figure 1.","A majority of the most effective neural sequence transduction systems utilize an encoder-decoder architecture [5, 2, 35]. In these models, the encoder converts an input series of symbol representations (x1, ..., xn) into a series of continuous latent representations z = (z1, ..., zn). Conditioned on z, the decoder then iteratively generates an output sequence (y1, ..., ym) of symbols one element at a time. At each generation step, the model is auto-regressive [10], leveraging the previously generated symbols as additional context when producing the next symbol. The Transformer employs this overall framework using stacked self-attention and pointwise fully connected layers for both the encoder and decoder, depicted in the left and right halves of Figure 1.",A,1
1198,Language Models are Few-Shot Learners,"Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale [KMH+20]. Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.","Every boost has resulted in enhancements in text creation and/or subsequent natural language processing assignments, and there are signs hinting that log loss, which aligns well with many subsequent tasks, pursues a smooth trajectory of enhancement with scale [KMH+20]. Because in-context learning consists of taking in many abilities and assignments within the boundaries of the model, it is believable that in-context learning capabilities might exhibit similarly robust increases with scale.","Each expansion has produced refinements in text synthesis and/or following natural language processing jobs, and there are clues indicating that log loss, which correlates appropriately with many following tasks, follows a smooth course of refinement with scale [KMH+20]. Since in-context learning entails assimilating numerous skills and jobs within the parameters of the model, it is plausible that in-context learning aptitudes might demonstrate similarly formidable gains with scale. ","Every addition has yielded advancements in text generation and/or ensuing natural language processing undertakings, and there are signs denoting that log loss, which aligns suitably with many ensuing undertakings, pursues a smooth trajectory of advancement with scale [KMH+20]. Because in-context learning consists of absorbing many capabilities and undertakings within the confines of the model, it is credible that in-context learning capacities might exhibit similarly sturdy increases with scale.",A,1
810,Attention is All You Need,"We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting.","We conducted just a few tests to choose the dropout, attention and residual connections (part 5.4), learning speeds and beam width using the Section 22 development set. All other settings stayed the same as the English-to-German base translation system. When making predictions, we expanded the maximum output length to input length + 300. We utilized a beam width of 21 and α = 0.3 for both the WSJ only and semi-supervised configurations.","We performed a small number of experiments to select the dropout, attention and residual connections (section 5.4), learning rates and beam size using the Section 22 development set. All other hyperparameters were unchanged from the English-to-German base translation model. During inference, we increased the max output length to input length + 300. We used a beam width of 21 and α = 0.3 for both the WSJ only and semi-supervised settings.","We carried out a limited number of tests to choose the dropout, attention and residual links (part 5.4), learning velocities and beam breadth utilizing the Section 22 development set. All other parameters stayed the same as the English-to-German base translation system. When making forecasts, we expanded the maximum output length to input length + 300. We employed a beam width of 21 and α = 0.3 for both the WSJ only and semi-supervised configurations.",A,1
1238,Language Models are Unsupervised Multitask Learners,All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text. We removed all Wikipedia documents from WebText since it is a common data source for other datasets and could complicate analysis due to overlapping training data with test evaluation tasks.,"The findings detailed in this report utilize an early form of WebText that does not have links formed after December 2017. After eliminating duplicate and unsuitable documents using automated methods, it retains a bit over 8 million texts totaling 40 GB. We excluded all Wikipedia pages from this WebText version since Wikipedia is frequently used to create other datasets, which could muddle analysis because of overlapping training and test data.","All discoveries presented in this publication depend on a preliminary variant of WebText lacking links made after December of 2017. Following duplication removal and some heuristic-powered cleansing, it incorporates somewhat over 8 million documents totaling 40 GB of content. We took out all Wikipedia pages from this WebText edition given Wikipedia's prevalent role in other datasets, which could obscure investigation due to overlapping preparation and assessment information. ","The results outlined in this paper employ an early iteration of WebText without links added after December 2017. Once duplicate and unsuitable content is eliminated using automated techniques, it retains slightly above 8 million documents amounting to 40 GB of text. We omitted all Wikipedia articles from this WebText version since Wikipedia is often utilized in other datasets, which could complicate analysis because of overlapping training and testing data.",A,1
888,Deep contextualized word representations,"As shown in Sec. 5.1, using all layers instead of just the last layer improves performance across multiple tasks. Sentiment analysis The fine-grained sentiment classification task in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) involves selecting one of five labels (from very negative to very positive) to describe a sentence from a movie review. The sentences contain diverse linguistic phenomena such as idioms and complex syntac- Task Baseline Last Only All layers λ=1 λ=0.001 SQuAD 80.8 84.7 85.0 85.2 SNLI 88.1 89.1 89.3 89.5 SRL 81.6 84.1 84.6 84.8 Table 2: Development set performance for SQuAD, SNLI and SRL comparing using all layers of the biLM (with different choices of regularization strength λ) to just the top layer. ","The results shown in Section 5.1 demonstrate that utilizing all layers instead of only the final layer improves performance on multiple tasks. Sentiment analysis The intricate sentiment classification task in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) requires choosing one of five labels (from very negative to very positive) to characterize a sentence from a movie review. The sentences include diverse linguistic phenomena like idioms and complex syntax. Task Baseline Last Layer Only All Layers λ=1 λ=0.001 SQuAD 80.8 84.7 85.0 85.2 SNLI 88.1 89.1 89.3 89.5 SRL 81.6 84.1 84.6 84.8 Table 2: Development set results for SQuAD, SNLI and SRL comparing usage of all layers of the biLM (with different λ values) to just the top layer.","The data presented in Section 5.1 shows that leveraging all layers instead of only the final layer enhances performance across various tasks. Sentiment classification The nuanced sentiment classification challenge in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) requires selecting one of five labels (from very negative to very positive) to summarize a sentence from a movie review. The sentences contain varied linguistic phenomena including idioms and intricate syntax. Task Baseline Final Layer Only All Layers λ=1 λ=0.001 SQuAD 80.8 84.7 85.0 85.2 SNLI 88.1 89.1 89.3 89.5 SRL 81.6 84.1 84.6 84.8 Table 2: Development set accuracy for SQuAD, SNLI and SRL comparing utilization of all layers of the biLM (with different λ values) to just the top layer.","As evidenced in Section 5.1, leveraging all layers instead of solely the final layer boosts performance on several tasks. Sentiment analysis The nuanced sentiment classification challenge in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) entails choosing one of five labels (from very negative to very positive) to summarize a sentence from a movie review. The sentences feature varied linguistic phenomena like idioms and complex syntax. Task Baseline Final Layer Only All Layers λ=1 λ=0.001 SQuAD 80.8 84.7 85.0 85.2 SNLI 88.1 89.1 89.3 89.5 SRL 81.6 84.1 84.6 84.8 Table 2: Development set results for SQuAD, SNLI and SRL comparing use of all layers of the biLM (with different λ values) to just the top layer.",A,1
1544,"You Only Look Once_Unified, Real-Time Object Detection","Our work is similar in design to work on grasp detection by Redmon et al [27]. Our grid approach to bounding box prediction is based on the MultiGrasp system for regression to grasps. However, grasp detection is a much simpler task than object detection. MultiGrasp only needs to predict a single graspable region for an image containing one object. It doesn’t have to estimate the size, location, or boundaries of the object or predict it’s class, only find a region suitable for grasping. YOLO predicts both bounding boxes and class probabilities for multiple objects of multiple classes in an image.","Our work has a similar structure to the grasp detection work done by Redmon and colleagues [27]. Our grid-based approach to predicting bounding boxes is modeled after the MultiGrasp system for regressing to grasps. However, grasp detection is a much more straightforward task than object detection. MultiGrasp only needs to identify a single graspable area in an image with one object. It doesn’t need to estimate the object's size, location, boundaries, or predict its class, only find a region good for grasping. YOLO predicts both bounding boxes and class probabilities for multiple objects of multiple classes in one image.","Our research has a comparable design to the grasp detection research by Redmon's group [27]. Our grid methodology for bounding box prediction is based on the MultiGrasp framework for regression to grasps. However, grasp detection is a much more basic task than object detection. MultiGrasp only needs to predict one graspable zone in an image with a single object. It doesn’t need to estimate the object's dimensions, position, edges, or anticipate its class, only identify an area suitable for grasping. YOLO anticipates both bounding boxes and class probabilities for numerous objects of numerous classes in a single image.  ","Our work has a similar structure to the grasp detection research conducted by Redmon and team [27]. Our grid-structured approach to bounding box prediction is modeled on the MultiGrasp system for regressing to grasps. However, grasp detection is a far simpler task than object detection. MultiGrasp only needs to determine one graspable region in an image containing a single object. It doesn’t need to estimate the object's size, location, boundaries, or predict its class, only identify an area good for grasping. YOLO predicts both bounding boxes and class probabilities for multiple objects across multiple classes in one image.",A,1
907,Deep Residual Learning for Image Recognition,"We adopt residual learning to every few stacked layers. A building block is shown in Fig. 2. Formally, in this paper we consider a building block defined as: y = F(x, {Wi}) + x. (1) Here x and y are the input and output vectors of the layers considered. The function F(x, {Wi}) represents the residual mapping to be learned. For the example in Fig. 2 that has two layers, F = W2σ(W1x) in which σ denotes 2This hypothesis, however, is still an open question. See [28]. ReLU [29] and the biases are omitted for simplifying notations.","We apply the technique of residual learning to groups of a small number of stacked layers. A single component is depicted in Fig. 2. Specifically, in this document we examine a component defined by: y = F(x, {Wi}) + x. (1) Here x and y are the input and output vectors for the layers of interest. The function F(x, {Wi}) denotes the residual mapping that must be determined. For the instance in Fig. 2 containing two layers, F = W2σ(W1x) where σ represents the ReLU [29] activation function and the bias terms are left out to simplify the expressions.","We use residual learning for every few adjacent layers that are stacked together. One building block is illustrated in Fig. 2. Formally, we analyze a building block characterized as: y = F(x, {Wi}) + x. (1) In this equation, x and y are the input and output vectors for the selected layers. The function F(x, {Wi}) characterizes the residual mapping that needs to be learned. For the two layer example in Fig. 2, F = W2σ(W1x) where σ is the ReLU [29] activation function and the bias terms are omitted to simplify the notation. ","We apply residual learning to groups of a small number of adjoining stacked layers. A single building block is pictured in Fig. 2. Specifically, we study a building block defined by the equation: y = F(x, {Wi}) + x. (1) Here, x and y represent the input and output vectors for the layers of interest. The function F(x, {Wi}) denotes the residual mapping to be determined through training. For the two-layer instance in Fig. 2, F = W2σ(W1x) where σ is the ReLU [29] activation and the bias terms are excluded to simplify the expressions.",A,1
1037,Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"We also evaluate the effects of more powerful networks on the proposal quality of RPN alone. We use VGG-16 to train the RPN, and still use the above detector of SS+ZF. The mAP improves from 56.8% (using RPN+ZF) to 59.2% (using RPN+VGG). This is a promising result, because it suggests that the proposal quality of RPN+VGG is better than that of RPN+ZF. Because proposals of RPN+ZF are competitive with SS (both are 58.7% when consistently used for training and testing), we may expect RPN+VGG to be better than SS. The following experiments justify this hypothesis.","We also assess the impacts of more capable networks on just the proposal quality of RPN. We utilize VGG-16 to train RPN, and still employ the above detector of SS+ZF. The mAP increases from 56.8% (utilizing RPN+ZF) to 59.2% (utilizing RPN+VGG). This is an encouraging outcome, since it implies that the proposal quality of RPN+VGG is superior to that of RPN+ZF. Because proposals of RPN+ZF are competitive with SS (both are 58.7% when consistently utilized for training and testing), we may anticipate RPN+VGG to surpass SS. The ensuing experiments justify this hypothesis.","In addition, we evaluate how more powerful neural networks affect the proposal quality of RPN on its own. We use VGG-16 to train RPN, while still employing the previous detector of SS+ZF. The mAP rises from 56.8% (with RPN+ZF) to 59.2% (with RPN+VGG). This is a promising finding, as it indicates the proposal quality of RPN+VGG is better than RPN+ZF. Since proposals from RPN+ZF are on par with SS (both at 58.7% when consistently used for training and evaluation), we can expect RPN+VGG to exceed SS. The next experiments confirm this hypothesis.","Furthermore, we assess the impacts of more capable networks solely on the proposal quality of RPN. We make use of VGG-16 to train RPN, while still applying the earlier detector of SS+ZF. The mAP increases from 56.8% (using RPN+ZF) to 59.2% (using RPN+VGG). This is an encouraging result, because it hints that the proposal quality of RPN+VGG is superior to RPN+ZF. As proposals from RPN+ZF are competitive with SS (both at 58.7% when steadily used for training and testing), we can anticipate RPN+VGG to outperform SS. The following experiments validate this hypothesis.",A,1
1052,Generative Adversarial Nets,"The most relevant work is predictability minimization [26]. In predictability minimization, each hidden unit in a neural network is trained to be different from the output of a second network, which predicts the value of that hidden unit given the value of all of the other hidden units. This work differs from predictability minimization in three important ways: 1) in this work, the competition between the networks is the sole training criterion, and is sufficient on its own to train the network. Predictability minimization is only a regularizer that encourages the hidden units of a neural network to be statistically independent while they accomplish some other task; it is not a primary training criterion. 2) The nature of the competition is different.","The most pertinent prior work is minimizing predictability [26]. In minimizing predictability, every concealed unit in a neural network is conditioned to diverge from the production of a second network, which estimates the value of that concealed unit given the values of all the other concealed units. This work is distinct from minimizing predictability in three major aspects: 1) here, the rivalry between the networks is the only training standard, and is adequate by itself to train the network. Minimizing predictability is only a regularizer that promotes the concealed units of a neural network to be statistically autonomous while they accomplish some other task; it is not a primary training standard. 2) The essence of the rivalry is different.","The most relevant previous research is predictability reduction [26]. In predictability reduction, each hidden node in a neural network is trained to differ from the output of a second network, which predicts the value of that hidden node given the value of all the other hidden nodes. This work is different from predictability reduction in three key ways: 1) here, the competition between the networks is the sole training objective, and is sufficient by itself to train the network. Predictability reduction is only a regularizer that encourages the hidden nodes of a neural network to be statistically independent while accomplishing some other task; it is not a primary training objective. 2) The nature of the competition is different.","The most applicable prior effort is unpredictability maximization [26]. In unpredictability maximization, every concealed unit in a neural network is conditioned to diverge from the production of a second network, which approximates the value of that concealed unit given the values of all the other concealed units. This effort differs from unpredictability maximization in three crucial aspects: 1) here, the contention between the networks is the sole training criterion, and is adequate on its own to train the network. Unpredictability maximization is only a regularizer that promotes the concealed units of a neural network to be statistically autonomous while they accomplish some other task; it is not a primary training criterion. 2) The essence of the contention is different.",A,1
1031,Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"At this point the two networks do not share conv layers. In the third step, we use the detector network to initialize RPN training, but we fix the shared conv layers and only fine-tune the layers unique to RPN. Now the two networks share conv layers. Finally, keeping the shared conv layers fixed, we fine-tune the fc layers of the Fast R-CNN. As such, both networks share the same conv layers and form a unified network. We train and test both region proposal and object detection networks on single-scale images [7, 5]. We re-scale the images such that their shorter side is s = 600 pixels [5]. Multi-scale feature extraction may improve accuracy but does not exhibit a good speed-accuracy trade-off [5].","Currently the two networks do not share convolutional layers. In step three, we utilize the detector network to initialize RPN training, but we keep the shared convolutional layers fixed and only fine-tune the layers unique to RPN. Now the two networks share convolutional layers. Lastly, with the shared convolutional layers fixed, we fine-tune the fully connected layers of Fast R-CNN. Therefore, both networks share the same convolutional layers and form a unified network. We train and evaluate both region proposal and object detection networks on single-scale images [7, 5]. We resize the images so their shorter side is s = 600 pixels [5]. Multi-scale feature extraction may improve accuracy but does not show a good speed-accuracy trade-off [5].","At this point the two networks do not have any convolutional layers in common. In the third step, we leverage the detector network to initialize RPN training, but we stabilize the shared convolutional layers and only fine-tune the layers exclusive to RPN. Now the two networks share convolutional layers. Finally, keeping the shared convolutional layers stabilized, we fine-tune the fully connected layers of Fast R-CNN. Consequently, both networks share the same convolutional layers and form a combined network. We train and test both region proposal and object detection networks on single-scale images [7, 5]. We rescale the images so their shorter side is s = 600 pixels [5]. Multi-scale feature extraction may improve accuracy but does not demonstrate a good speed-accuracy trade-off [5].  ","Currently the two networks do not share any conv layers. In step three, we use the detector network to initialize RPN training, but we fix the shared conv layers and only fine-tune the layers unique to RPN. Now the two networks share conv layers. Lastly, keeping the shared conv layers fixed, we fine-tune the fc layers of Fast R-CNN. Therefore, both networks share the same conv layers and form a unified network. We train and evaluate both region proposal and object detection networks on single-scale images [7, 5]. We resize the images so their shorter side is s = 600 pixels [5]. Multi-scale feature extraction may improve accuracy but does not exhibit a good speed-accuracy compromise [5].",A,1
1034,Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"To investigate the behavior of RPNs as a proposal method, we conducted several ablation studies. First, we show the effect of sharing conv layers between the RPN and Fast R-CNN detection network. To do this, we stop after the second step in the 4-step training process. Using separate networks reduces the result slightly to 58.7% (RPN+ZF, unshared, Table 1). We observe that this is because in the third step when the detector-tuned features are used to fine-tune the RPN, the proposal quality is improved. Next, we disentangle the RPN’s influence on training the Fast R-CNN detection network.","To examine how RPNs work as a proposal technique, we did some experiments removing components. First, we demonstrate the impact of using the same conv layers for both the RPN and Fast R-CNN detection network. We stop after step 2 of the 4-step training process to test this. Using separate networks somewhat reduces the result to 58.7% (RPN+ZF, unshared, Table 1). We see this is because in step 3 when the detector-tuned features fine-tune the RPN, the proposal quality improves. Next, we separate out the RPN's effect on training the Fast R-CNN detection network.","To study the performance of RPNs as a proposal approach, we conducted some ablation experiments. Initially, we show the consequence of sharing conv layers between the RPN and Fast R-CNN detection model. To do this, we halt after the second phase in the 4-phase training workflow. Employing distinct models slightly decreases the outcome to 58.7% (RPN+ZF, unshared, Table 1). We notice this is because in the third phase when the detector-tuned features are utilized to fine-tune the RPN, the proposal quality is enhanced. Subsequently, we isolate the RPN's influence on training the Fast R-CNN detection model.","To analyze the functioning of RPNs as a proposal technique, we performed some ablation analyses. Firstly, we demonstrate the impact of utilizing the same conv layers for the RPN and Fast R-CNN detection network. We stop after step 2 of the 4-step training process for this. Using separate networks slightly reduces the result to 58.7% (RPN+ZF, unshared, Table 1). We observe this is because in step 3 when the detector-tuned features fine-tune the RPN, the proposal quality improves. Afterward, we detach the RPN's effect on training the Fast R-CNN detection network.",A,1
1517,"You Only Look Once_Unified, Real-Time Object Detection","We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast.","We introduce YOLO, a novel method for recognizing objects in images. Earlier object detection systems modify classifiers for detection tasks. Our approach formulates object detection as a regression problem to locate bounding boxes and compute class probabilities. A single neural network estimates bounding boxes and class probabilities straight from full images in one pass. Since the complete detection system is a single network, it can be trained end-to-end directly for detection accuracy. Our unified design is remarkably fast.","We present YOLO, an innovative approach to detecting objects. Prior object detection systems repurpose classifiers to do detection. We pose object detection as a regression problem to find bounding boxes and class probabilities. One neural net predicts bounding boxes and probabilities straight from entire images in one shot. The full pipeline being one network means it can optimize detection performance end-to-end. Our unified architecture is extremely fast. ","We introduce YOLO, a new technique for object detection. Earlier detection systems adapt classifiers for detection. We formulate detection as regression to localize bounding boxes and predict class probabilities. A single neural network estimates boxes and probabilities directly from whole images in one evaluation. The full pipeline being one network enables end-to-end optimization for detection. Our unified design is very fast.",A,1
873,Deep contextualized word representations,"In the next section, we depart from previous work by introducing a new approach for learning word representations that are a linear combination of the biLM layers. γ is of practical importance to aid the optimization process (see supplemental material for details). Considering that the activations of each biLM layer have a different distribution, in some cases it also helped to apply layer normalization (Ba et al., 2016) to each biLM layer before weighting. ","In the following part, we diverge from prior work by presenting a new technique for learning word representations that are a linear mix of the biLM layers. γ is practically important to help the optimization process (see extra material for specifics). Given that the activations of each biLM layer have a distinct distribution, in some situations it also assisted to use layer normalization (Ba et al., 2016) to each biLM layer before weighting.","In the next portion, we depart from earlier efforts by introducing a novel method for generating word embeddings that are a linear combination of the biLM layers. γ is of practical value to facilitate the optimization procedure (refer to supplementary information for particulars). Considering the activations of every biLM layer possess a different distribution, in certain cases it also proved beneficial to implement layer normalization (Ba et al., 2016) to each biLM layer prior to weighting.  ","In the following section, we diverge from previous undertakings by presenting a new system for producing word representations that are a linear mixture of the biLM layers. γ is of practical importance to promote the optimization process (see additional documentation for specifics). Given that the activations of each biLM layer have a unique distribution, in some situations it also helped to put into action layer normalization (Ba et al., 2016) to every biLM layer before weighting.",A,1
1365,Sequence to Sequence Learning with Neural Networks,"While the decoded translations of the LSTM ensemble do not outperform the best WMT’14 system, it is the first time that a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT task by a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM is within 0.5 BLEU points of the best WMT’14 result if it is used to rescore the 1000-best list of the baseline system.","Although the translated outputs from the LSTM collection don't surpass the top WMT'14 framework, this is the first occasion that a complete neural translation framework beats a phrase-based SMT baseline by a significant edge on a large scale MT assignment, regardless of its failure to deal with out-of-vocabulary words. The LSTM is inside 0.5 BLEU focuses on the best WMT'14 result if it's utilized to rescore the 1000-best rundown of the benchmark framework.","Despite not exceeding the best WMT'14 structure, the interpreted yields of the LSTM gathering are the first where a pure neural translation framework eclipses a phrase-based SMT benchmark by a generous margin on a huge scale MT task, notwithstanding its powerlessness to handle obscure words. The LSTM is inside 0.5 BLEU points of the best WMT'14 result if used to rescore the top 1000 outcomes from the benchmark framework.  ","Although not surpassing the top WMT'14 system, this is the first time a complete neural translation model beats a phrase-based SMT baseline by a wide edge on a large MT job, even with its inability to process unknown words. The LSTM is within 0.5 BLEU points of the best WMT'14 result if utilized to rescore the top 1000 translations from the baseline system.",A,1
789,Attention is All You Need,"Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences.","Neural networks with recurrent connections, especially long short-term memory and gated recurrent types, have proven to be the best techniques for sequence modeling tasks like language modeling and machine translation. Much work has continued to improve recurrent language models and encoder-decoder models. Recurrent networks usually break down computation over the symbol positions in the input and output sequences.","Neural networks with feedback loops, including long short-term memory and gated recurrent variants, are the premier approaches for sequence modeling and sequence transduction challenges such as language modeling and machine translation. Many efforts have kept pushing the limits of recurrent language models and encoder-decoder architectures. Recurrent networks typically separate computation by the symbol positions in the input and output sequences.  ","Neural networks with cyclic connections, particularly long short-term memory and gated recurrent forms, have clearly shown themselves to be the leading techniques for sequence modeling and sequence transduction tasks like language modeling and machine translation. Much progress has been made to advance recurrent language models and encoder-decoder architectures. Recurrent networks tend to divide up computation according to the symbol positions of the input and output sequences.",A,1
1046,Generative Adversarial Nets,"This framework can yield specific training algorithms for many kinds of model and optimization algorithm. In this article, we explore the special case when the generative model generates samples by passing random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this special case as adversarial nets. In this case, we can train both models using only the highly successful backpropagation and dropout algorithms [16] and sample from the generative model using only forward propagation. No approximate inference or Markov chains are necessary.","This system can produce particular training methods for many types of models and optimization algorithms. In this paper, we look at the special case where the generative model makes samples by sending random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We call this specific case adversarial networks. Here, we can train both models using only the very successful backpropagation and dropout algorithms [16] and sample from the generative model using only forward propagation. No approximate inference or Markov chains are needed.","This framework is able to generate tailored training procedures for many kinds of models and optimization techniques. In this article, we examine the special scenario where the generative model produces samples by feeding random noise into a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this particular case as adversarial networks. In this situation, we can educate both models utilizing only the highly effective backpropagation and dropout algorithms [16] and sample from the generative model employing only forward propagation. No approximate inference or Markov chains are required.  ","This structure can produce specialized training methods for many types of models and optimization methods. In this paper, we investigate the specific case where the generative model creates samples by inputting random noise into a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We call this particular case adversarial networks. Here, we can train both models using only the very successful backpropagation and dropout algorithms [16] and take samples from the generative model using only forward propagation. No approximate inference or Markov chains are needed.",A,1
1390,Transformer-XL,"Under the new parameterization, each term has an intuitive meaning: term (a) represents content based addressing, term (b) captures a content dependent positional bias, term (c) governs a global content bias, and (d) encodes a global positional bias. In comparison, the formulation in Shaw et al. (2018) only has terms (a) and (b), dropping the two bias terms (c) and (d). Moreover, Shaw et al. (2018) merge the multiplication WkR into a single trainable matrix Rˆ , which abandons the inductive bias built into the original sinusoid positional encoding (Vaswani et al., 2017). In contrast, our relative positional embedding R adapts the sinusoid formulation.","With the new parameterization, each part has an easy to understand meaning: part (a) stands for content based addressing, part (b) captures a content related positional tendency, part (c) controls a global content inclination, and (d) encodes a global positional tendency. Compared to the formulation in Shaw et al. (2018) which only has parts (a) and (b), dropping the two bias terms (c) and (d). Furthermore, Shaw et al. (2018) combine the multiplication WkR into one trainable matrix Rˆ, which loses the inductive bias constructed into the original sinusoid positional encoding (Vaswani et al., 2017). In contrast, our relative positional embedding R adapts the sinusoid formulation.","In the new parameterization, each component has an intuitive interpretation: component (a) denotes content based addressing, component (b) seizes a content contingent positional bias, component (c) directs a global content bias, and (d) symbolizes a global positional bias. In comparison, the formulation in Shaw et al. (2018) only comprises components (a) and (b), omitting the two bias terms (c) and (d). Additionally, Shaw et al. (2018) consolidate the multiplication WkR into a single trainable matrix Rˆ, which forsakes the inductive bias ingrained in the original sinusoid positional encoding (Vaswani et al., 2017). Conversely, our relative positional embedding R conforms to the sinusoid formulation.  ","Under the new parameterization, each piece has an easy to grasp meaning: piece (a) embodies content based addressing, piece (b) seizes a content related positional tendency, piece (c) governs a global content inclination, and (d) typifies a global positional tendency. Compared to the formulation in Shaw et al. (2018) which only encompasses pieces (a) and (b), excluding the two bias terms (c) and (d). Furthermore, Shaw et al. (2018) coalesce the multiplication WkR into one trainable matrix Rˆ, which relinquishes the inductive bias implanted in the original sinusoid positional encoding (Vaswani et al., 2017). In contrast, our relative positional embedding R adheres to the sinusoid formulation.",A,1
1318,Sentence Embeddings using Siamese BERT-Networks,"The paper is structured in the following way: Section 3 presents SBERT, section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Section 5 evaluates SBERT on SentEval. In section 6, we perform an ablation study to test some design aspect of SBERT. In section 7, we compare the computational efficiency of SBERT sentence embeddings in contrast to other state-of-the-art sentence embedding methods.","The composition of the document is as follows: Part 3 introduces SBERT, part 4 assesses SBERT on prevalent STS tasks and on the demanding Argument Facet Similarity (AFS) collection (Misra et al., 2016). Part 5 assesses SBERT on SentEval. In part 6, we execute an ablation analysis to evaluate some architectural aspects of SBERT. In part 7, we contrast the computational capability of SBERT sentence embeddings versus other cutting-edge sentence embedding techniques.","The paper's structure is like this: Segment 3 presents SBERT, segment 4 judges SBERT on common STS assignments and on the tough Argument Facet Similarity (AFS) corpus (Misra et al., 2016). Segment 5 judges SBERT on SentEval. In segment 6, we do an ablation review to test some design parts of SBERT. In segment 7, we equate the computational proficiency of SBERT sentence embeddings contrasted with other state-of-the-art sentence embedding methodologies.","The composition of the paper is: Portion 3 introduces SBERT, portion 4 appraises SBERT on prevalent STS tasks and on the demanding Argument Facet Similarity (AFS) collection (Misra et al., 2016). Portion 5 appraises SBERT on SentEval. In portion 6, we implement an ablation analysis to evaluate some architectural components of SBERT. In portion 7, we contrast the computational capability of SBERT sentence embeddings versus other leading-edge sentence embedding techniques.",A,1
1254,Neural Machine Translation by Jointly Learning To Align and Translate,"An encoder neural network reads and encodes a source sentence into a fixed-length vector. A decoder then outputs a translation from the encoded vector. The whole encoder–decoder system, which consists of the encoder and the decoder for a language pair, is jointly trained to maximize the probability of a correct translation given a source sentence. A potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus.","A neural network that encodes text reads and converts a source sentence into a vector of a predefined length. After that, a decoder neural network generates a translation using the encoded vector. The whole encoder-decoder framework, made up of the encoder and decoder for a language pair, is jointly optimized to maximize the chance of producing the right translation from a source sentence. A possible problem with this encoder-decoder method is that a neural network has to be capable of condensing all the required information from a source sentence into a fixed-length vector. This might make it tough for the neural network to handle long sentences, particularly those longer than the sentences in the training data.","An encoder neural net processes and encodes a source sentence into a vector of fixed size. Then, a decoder generates a translation from that encoded vector. The entire encoder-decoder model, composed of the encoder and decoder for a language pair, is trained together to increase the probability of generating a correct translation from a source sentence. One issue with this encoder-decoder technique is that a neural network has to compress all necessary information from a source sentence into a fixed-size vector. This can make it problematic for the neural net to work with long sentences, especially those longer than the sentences in the training set.","An encoding neural network analyzes and converts a source sentence into a vector of predetermined length. Afterward, a decoding neural network produces a translation using that encoded vector. The whole encoder-decoder architecture, consisting of the encoder and decoder for a language pair, is co-trained to maximize the likelihood of correct translation generation given a source sentence. One potential problem with this encoder-decoder method is that a neural net must be capable of condensing all requisite information from a source sentence into a fixed-length vector. This may cause difficulties for the neural net to process long sentences, particularly those exceeding the length of sentences in the training data.",A,1
817,Bag of Tricks for Efficient Text Classification,"Common solutions to this problem are to factorize the linear classifier into low rank matrices (Schutze, 1992; Mikolov et al., 2013) or to use multilayer neural networks (Collobert and Weston, 2008; Zhang et al., 2015). Figure 1 shows a simple linear model with rank constraint. The first weight matrix A is a look-up table over the words. The word representations are then averaged into a text representation, which is in turn fed to a linear classifier. The features are embedded and averaged to form the hidden variable. This architecture is similar to the cbow model of Mikolov et al. (2013), where the middle word is replaced by a label.","Popular approaches to addressing this issue include decomposing the linear classifier into low rank matrices (Schutze, 1992; Mikolov et al., 2013) or utilizing multilayer neural networks (Collobert and Weston, 2008; Zhang et al., 2015). Figure 1 illustrates a basic linear model with rank limitation. The initial weight matrix A is a lookup table for the words. The word representations are then averaged into a text representation, which is then input to a linear classifier. The features are embedded and averaged to generate the hidden variable. This structure is comparable to the cbow model of Mikolov et al. (2013), where the middle word is substituted with a label.","Well-known solutions for this problem involve breaking down the linear classifier into low rank matrices (Schutze, 1992; Mikolov et al., 2013) or employing multi-layer neural networks (Collobert and Weston, 2008; Zhang et al., 2015). Figure 1 displays a straightforward linear model with rank constraint. The first weight matrix A is a reference table for the words. The word representations are then consolidated into a text representation, which is subsequently provided to a linear classifier. The features are inserted and consolidated to produce the hidden variable. This framework is similar to the cbow model of Mikolov et al. (2013), where the middle word is replaced with a label.  ","Common techniques to tackle this issue are to decompose the linear classifier into low rank matrices (Schutze, 1992; Mikolov et al., 2013) or use multi-layer neural networks (Collobert and Weston, 2008; Zhang et al., 2015). Figure 1 shows a basic linear model with rank restriction. The first weight matrix A is an index table for the words. The word representations are then combined into a text representation, which is then input into a linear classifier. The features are embedded and combined to generate the hidden variable. This design is comparable to the cbow model of Mikolov et al. (2013), where the middle word is substituted by a label.",A,1
1421,U-Net_Convolutional Networks for Biomedical Image Segmentation,"The u-net architecture achieves very good performance on very different biomedical segmentation applications. Thanks to data augmentation with elastic deformations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVidia Titan GPU (6 GB). We provide the full Caffe[6]-based implementation and the trained networks4 . We are sure that the u-net architecture can be applied easily to many more tasks.","The u-net design is very effective for a wide variety of biomedical image segmentation tasks. With data augmentation using elastic distortions, it only requires a small number of labeled images and has a fast training time of just 10 hours on an NVidia Titan GPU (6 GB). We supply the complete Caffe[6]-based implementation and trained networks4. We are confident the u-net architecture can be easily applied to many additional applications.","The u-net model performs excellently on diverse biomedical image segmentation jobs. Thanks to data expansion using elastic tweaks, it needs just a few annotated pictures and trains swiftly in only 10 hours on an NVidia Titan GPU (6 GB). We provide the full Caffe[6]-based code and trained models4. We believe the u-net design can be simply used for numerous extra tasks. ","The u-net framework is highly successful across various biomedical segmentation use cases. With data enhancement via elastic transformations, it requires merely a small labeled dataset and trains rapidly in just 10 hours using an NVidia Titan GPU (6 GB). We make available the complete Caffe[6]-based implementation and pretrained networks4. We are certain the u-net architecture could be easily leveraged for many additional applications.",A,1
1495,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"According to the comparison above, AR language modeling and BERT possess their unique advantages over the other. A natural question to ask is whether there exists a pretraining objective that brings the advantages of both while avoiding their weaknesses. Borrowing ideas from orderless NADE [32], we propose the permutation language modeling objective that not only retains the benefits of AR models but also allows models to capture bidirectional contexts. Specifically, for a sequence x of length T, there are T! different orders to perform a valid autoregressive factorization. Intuitively, if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides.","The comparison shows that AR language modeling and BERT have unique strengths compared to each other. It's natural to wonder if there's a pretraining goal that combines the benefits of both while avoiding the downsides. Inspired by orderless NADE [32], we suggest the permutation language modeling objective. It keeps the advantages of AR models and lets models use bidirectional contexts. For a sequence x of length T, there are T! valid ways to factorize autoregressively. If parameters are shared across all orders, the model can learn to use info from all positions on both sides.","The preceding analysis indicates AR language modeling and BERT have distinct pros over the other. A logical question is whether some pretraining aim unifies the upsides of both while skipping the weaknesses. Borrowing from orderless NADE [32], we put forward permutation language modeling. This retains AR model strengths and enables capturing bidirectional contexts. Specifically, for a sequence x of length T, there are T! valid autoregressive factorizations. Intuitively, sharing parameters across all orders, the model can gather information from all positions on both sides. ","The earlier comparison shows AR language modeling and BERT have unique benefits over the other. A natural inquiry is if some pretraining goal combines the advantages of both while avoiding the drawbacks. Taking inspiration from orderless NADE [32], we propose permutation language modeling. This keeps AR model strengths and allows capturing bidirectional contexts. Namely, for a sequence x of length T, there are T! valid ways to autoregressively factorize. If parameters are shared across orders, the model can learn to utilize information from all positions on both sides.",A,1
1377,Transformer-XL,"More importantly, we show the necessity of using relative positional encodings rather than absolute ones, in order to enable state reuse without causing temporal confusion. Hence, as an additional technical contribution, we introduce a simple but more effective relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training. Transformer-XL obtained strong results on five datasets, varying from word-level to character level language modeling. Transformer-XL is also able to generate relatively coherent long text articles with thousands of tokens (see Appendix E), trained on only 100M tokens.","Most significantly, we demonstrate the need to use relative positional encodings instead of absolute ones, so that states can be reused without causing temporal disarray. Thus, as an extra technical contribution, we present a simple yet more effective relative positional encoding formulation that extends to attention lengths longer than that seen during training. Transformer-XL achieved strong outcomes on five datasets, ranging from word-level to character level language modeling. Transformer-XL is also capable of generating fairly coherent long text articles with thousands of tokens (see Appendix E), trained on only 100M tokens.","Above all, we exhibit the necessity of utilizing relative positional codings rather than absolute ones, in order to facilitate state reuse without inducing temporal confusion. Therefore, as an additional technical offering, we introduce a simple but more efficacious relative positional coding formulation that generalizes to attention lengths greater than the one observed during training. Transformer-XL obtained robust results on five datasets, varying from word-level to character level language modeling. Transformer-XL is also able to produce relatively coherent long text articles with thousands of tokens (see Appendix E), trained on only 100M tokens.  ","Most importantly, we demonstrate the need to employ relative positional encodings instead of absolute ones, so that states can be reused without causing temporal disarray. Hence, as a further technical contribution, we present a simple yet more effectual relative positional encoding formulation that extends to attention lengths longer than that witnessed during training. Transformer-XL achieved strong outcomes on five datasets, ranging from word-level to character level language modeling. Transformer-XL is also capable of generating relatively coherent long text articles with thousands of tokens (see Appendix E), trained on just 100M tokens.",A,1
1397,Transformer-XL,"Table 6 shows that both the recurrence mechanism and our encoding scheme are necessary to achieve the best performance, as well as generalizing to longer attention sequences during evaluation time. Although the backpropagation length during training is only 128, with the two techniques the attention length can be increased to 640 at test time. In the standard setting with 151M parameters, the perplexity decreases as the attention length increases. Since the recurrence mechanism costs additional memory, we also compare Transformer-XL with baselines under the same GPU memory constraints. As shown in Table 10 in Appendix A, despite using a shorter backpropagation length, Transformer-XL remains superior to the baselines.","Table 6 demonstrates that utilizing both the recurrence technique and our encoding method is imperative for attaining optimal performance, and for generalizing to longer attention sequences during evaluation. Although backpropagation length during training is only 128, the two techniques allow the attention length to rise to 640 at test time. In the standard configuration with 151M parameters, perplexity declines as attention length increases. Because the recurrence mechanism necessitates extra memory, we also contrast Transformer-XL with baselines under identical GPU memory limits. As exhibited in Table 10 in Appendix A, despite employing a shorter backpropagation length, Transformer-XL continues to surpass the baselines.","The data in Table 6 makes clear that leveraging both the recurrence mechanism and our encoding system is crucial for reaching the highest performance, and for extending to longer attention sequences when evaluating. While backpropagation length during training is just 128, the two methods let attention length grow to 640 at test time. With the standard setting of 151M parameters, perplexity lessens as attention length expands. Since the recurrence mechanism needs additional memory, we also compare Transformer-XL to baselines with the same GPU memory constraints. As shown in Table 10 in Appendix A, even using a shorter backpropagation length, Transformer-XL remains superior to the baselines.  ","The information in Table 6 demonstrates that utilizing both the recurrence technique and our encoding framework is vital for achieving optimal performance, and for generalizing to longer attention sequences during evaluation. Although the backpropagation length during training is only 128, the two techniques enable the attention length to increase to 640 at test time. With the standard configuration of 151M parameters, perplexity decreases as attention length increases. Because the recurrence mechanism requires extra memory, we also contrast Transformer-XL with baselines under the same GPU memory limits. As exhibited in Table 10 in Appendix A, despite employing a shorter backpropagation length, Transformer-XL continues to outperform the baselines.",A,1
1191,Language Models are Few-Shot Learners,"Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have been directly fine-tuned, entirely removing the need for task-specific architectures [RNSS18, DCLT18, HR18].","In the field of natural language processing, there has been a recent trend of using pre-trained language models that can be applied flexibly to various downstream tasks. Initially, single-layer word embeddings were learned and input to specialized models. Then, multilayer RNNs were used to build more powerful contextual representations, though still fed into specialized models. Most recently, large pretrained recurrent and transformer language models have been directly fine-tuned for tasks, removing the need for specialized architectures.","Over the past few years, NLP systems have increasingly made use of pre-trained language representations in flexible, task-agnostic ways for transfer learning. First, single-layer word vectors were learned and given to task-specific models. Next, multilayer RNNs formed stronger contextual representations, still applied to specialized models. Now, large pretrained recurrent or transformer LMs are directly fine-tuned, eliminating specialized architectures.","In recent times, NLP has seen a trend of using pre-trained language models in adaptable, task-general ways for transfer. Initially, single-layer word embeddings were learned and used in task-focused models. After that, multilayer RNNs created more powerful contextual representations, although still fed to specialized models. Most recently, large pretrained recurrent or transformer LMs have been directly fine-tuned, removing the need for specialized architectures.",A,1
1121,Going deeper with convolutions,"Another practically useful aspect of this design is that it aligns with the intuition that visual information should be processed at various scales and then aggregated so that the next stage can abstract features from different scales simultaneously. The improved use of computational resources allows for increasing both the width of each stage as well as the number of stages without getting into computational difficulties.  We have found that all the included the knobs and levers allow for a controlled balancing of computational resources that can result in networks that are 2 − 3× faster than similarly performing networks with non-Inception architecture, however this requires careful manual design at this point.","A further practically helpful part of this plan is that it matches the idea that visual data should be handled at different scales and then combined so the next phase can extract features from multiple scales at the same time. The enhanced use of computing resources enables expanding both the width of each step and the quantity of steps without running into computing problems. We've found that all the included controls and adjustments enable a modulated balancing of computing power that can produce networks 2-3 times quicker than equally performing networks without the Inception design, however this necessitates careful manual configuration currently.","An additional practically useful facet of this blueprint is that it aligns with the notion that visual information ought to be processed at various levels and then brought together so the subsequent stage can abstract traits from different levels simultaneously. The improved harnessing of computing assets enables increasing both the breadth of each phase and the amount of phases without encountering computing difficulties. We've discovered that all the included dials and levers allow for a regulated balancing of computing power that can generate networks that are 2-3 times faster than comparably performing networks without the Inception architecture, however this necessitates meticulous manual design for now.","A further practically helpful element of this model is that it conforms with the concept that visual data should be handled at different scales and then consolidated so the next step can extract attributes from multiple scales concurrently. The enhanced utilization of computing resources permits expanding both the width of each step and the number of steps without encountering computing problems. We've found that all the included controls and adjustments enable a controlled balancing of computing capacity that can yield networks that are 2-3 times quicker than equally performing networks without the Inception design, however this necessitates careful manual configuration at present.",A,1
866,Deep contextualized word representations,"Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform arXiv:1802.05365v2 [cs.CL] 22 Mar 2018 those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. ","In conclusion, an examination of both ELMo and CoVe shows that representations derived from deeper layers outperform those from just the top layer of an LSTM. We have published our trained models and code, and we anticipate ELMo will give similar improvements on many other natural language processing tasks. Related work: Because of their capacity to learn syntactic and semantic information about words from large unlabeled corpora, pre-trained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard part of most state-of-the-art NLP systems, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only enable a single context-independent representation per word.","In summary, analyzing both ELMo and CoVe demonstrates that representations from deeper layers are superior to those from just the top layer of an LSTM. We have published our models and code publicly, and we think ELMo will yield similar gains on many other natural language tasks. Prior work: Due to their ability to capture syntactic and semantic properties of words from large unlabeled text, pre-trained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard part of most cutting-edge NLP systems, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these word vector learning approaches only allow one context-independent representation per word.  ","To conclude, studying both ELMo and CoVe shows deep representations are better than those from just the top LSTM layer. We've released our models and code publicly, and expect ELMo will similarly improve many other NLP problems. Previous work: Since they can learn syntactic and semantic word properties from large unlabeled text, pre-trained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are standard in most state-of-the-art NLP, like question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these word vector approaches only enable one context-independent representation per word.",A,1
1512,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the performance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries without fine tuning, and employ a kernel pooling network [36] to rank the documents.","As established in prior research [8], we utilize the ClueWeb09-B data collection to assess performance on ranking documents. The inquiries were formed by the TREC 2009-2012 Web Tracks founded on 50M texts and the objective is to reorganize the top 100 articles obtained utilizing a standard retrieval technique. Because document ranking, or ad-hoc retrieval, chiefly deals with low-level representations rather than high-level semantics, this data collection functions as a testbed for evaluating the caliber of word embeddings. We employ a pre-trained XLNet to derive word embeddings for the documents and questions without fine-tuning, and use a kernel pooling network [36] to rank the documents.","In line with earlier work [8], we make use of the ClueWeb09-B dataset to gauge effectiveness on document ranking. The questions were generated by the TREC 2009-2012 Web Tracks built on 50M papers and the goal is to re-rank the top 100 papers retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, primarily concerns low-level representations instead of high-level semantics, this dataset serves as a testbed for assessing the quality of word embeddings. We utilize a pre-trained XLNet to extract word embeddings for the documents and questions without fine-tuning, and utilize a kernel pooling network [36] to rank the documents.","As established in previous research [8], we employ the ClueWeb09-B dataset to evaluate performance on ranking documents. The queries were formed by the TREC 2009-2012 Web Tracks based on 50M texts and the task is to rearrange the top 100 texts obtained using a standard retrieval approach. Because document ranking, or ad-hoc retrieval, mostly relates to low-level representations rather than high-level semantics, this dataset functions as a testbed for assessing the quality of word embeddings. We use a pre-trained XLNet to derive word embeddings for the documents and queries without fine-tuning, and use a kernel pooling network [36] to rank the documents.",A,1
935,Deep Residual Learning for Image Recognition,"Also similar to the ImageNet cases (Fig. 4, right), our ResNets manage to overcome the optimization difficulty and demonstrate accuracy gains when the depth increases. We further explore n = 18 that leads to a 110-layer ResNet. In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging5 . So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training. The rest of the learning schedule is as done previously. This 110-layer network converges well (Fig. 6, middle).","Likewise as with the ImageNet examples (Fig. 4, right), our ResNets are able to get over the optimization challenges and show precision improvements as the depth grows. We additionally investigate n = 18 which results in a 110-layer ResNet. Here, we find that the initial learning rate of 0.1 is slightly too big to start converging. So we utilize 0.01 to warm up the training until the training error is under 80% (about 400 iterations), then go back to 0.1 and keep training. The rest of the learning schedule is as before. This 110-layer network converges well (Fig. 6, middle).","Similarly to the ImageNet situations (Fig. 4, right), our ResNets can overcome the optimization problems and exhibit accuracy gains as the depth increases. We also explore n = 18 leading to a 110-layer ResNet. In this case, we determine that the initial learning rate of 0.1 is slightly too high to begin converging. Thus we use 0.01 to preheat the training until the training error is below 80% (around 400 iterations), then return to 0.1 and proceed training. The rest of the learning plan is as previously done. This 110-layer network converges successfully (Fig. 6, middle).  ","In the same way as the ImageNet examples (Fig. 4, right), our ResNets are able to conquer the optimization challenges and display precision improvements when the depth is increased. We further investigate n = 18 resulting in a 110-layer ResNet. Here, we find that the initial learning rate of 0.1 is slightly too large to start converging. Therefore, we utilize 0.01 to warm up the training until the training error is under 80% (approximately 400 iterations), then go back to 0.1 and continue training. The rest of the learning schedule is as before. This 110-layer network converges well (Fig. 6, middle).",A,1
1131,ImageNet A Large_Scale Hierarchical Image Database,"The digital era has brought with it an enormous explosion of data. The latest estimations put a number of more than 3 billion photos on Flickr, a similar number of video clips on YouTube and an even larger number for images in the Google Image Search database. More sophisticated and robust models and algorithms can be proposed by exploiting these images, resulting in better applications for users to index, retrieve, organize and interact with these data. But exactly how such data can be utilized and organized is a problem yet to be solved. In this paper, we introduce a new image database called “ImageNet”, a large-scale ontology of images.","The digital age has led to a massive increase in data. Current estimates show over 3 billion photos on Flickr, a comparable amount of videos on YouTube, and even more images in Google Image Search. By using these images in new and better models and algorithms, improved applications could be created to categorize, find, arrange, and interact with this data. However, how to actually use and structure this data remains an unsolved issue. This paper presents a new image database called ""ImageNet"", which is a large-scale organization of images.","The digital era has resulted in a huge explosion of information. Current approximations indicate over 3 billion photographs on Flickr, a similar quantity of videos on YouTube, and an even greater number for pictures in the Google Image Search system. More refined and robust frameworks and formulas can be suggested by leveraging these visuals, resulting in superior applications for users to catalog, access, coordinate and connect with this content. However, precisely how such information can be used and structured is an issue that has yet to be addressed. In this paper, we present a new image repository called ""ImageNet"", a large-scale categorization of images.","The digital age has brought an enormous increase in information. Current estimates show more than 3 billion photos on Flickr, a comparable number of videos on YouTube, and even more images in Google Image Search. By utilizing these images in more advanced and robust models and algorithms, better applications could be created for users to index, find, organize and interact with this information. However, exactly how to use and structure this data remains an unresolved problem. This paper introduces a new image database called ""ImageNet"", which is a large-scale classification of images.",A,1
1213,Language Models are Few-Shot Learners,"K can be any value from 0 to the maximum amount allowed by the model’s context window, which is nctx = 2048 for all models and typically fits 10 to 100 examples. Larger values of K are usually but not always better, so when a separate development and test set are available, we experiment with a few values of K on the development set and then run the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to (or for K = 0, instead of) demonstrations.","The variable K is able to take on any whole number from 0 up to the model's context window maximum, which equals 2048 for all models and generally contains somewhere between 10 and 100 instances. More often than not, larger K values prove superior, so when distinct development and test sets exist, we try out a few K values on the development set and subsequently utilize the best value on the test set. For certain tasks (refer to Appendix G), we also employ a natural language prompt along with (or for K = 0, in place of) examples.","K can be any integer between 0 and the limit set by the model's context window, set at 2048 for all models, which tends to be enough for 10 to 100 samples. In most cases, bigger K is better, so with separate dev and test sets, we experiment with some K values on dev then use the best on test. For some tasks (see Appendix G), we also use a natural language prompt with (or instead of, if K=0) the examples.  ","The variable K is able to take on any integer value from 0 up to the maximum permitted by the model's context window, fixed at 2048 for all models, which is typically sufficient for 10 to 100 instances. Larger values of K are generally superior, so given distinct development and test sets, we try out several K values on development then utilize the optimal one for test. For certain tasks (refer Appendix G), we also utilize a natural language prompt along with (or instead of, if K = 0) demonstrations.",A,1
920,Deep Residual Learning for Image Recognition,"Fig. 4 shows the training procedures. 34-layer plain net has higher training error throughout the whole training procedure, even though the solution space of the 18-layer plain network is a subspace of that of the 34-layer one. We argue that this optimization difficulty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN [16], which ensures forward propagated signals to have non-zero variances. We also verify that the backward propagated gradients exhibit healthy norms with BN. So neither forward nor backward signals vanish.","The instructional diagram displays the preparation processes. The 34-layer elementary system has superior preparation inaccuracies over the entire preparation system, despite the clarification extent of the 18-layer elementary structure being a subarea of that of the 34-layer one. We debate that this enhancement challenge is improbable to be prompted by fading slopes. These elementary systems are prepared with BN [16], which guarantees forward spread signals to have non-zero contrasts. We additionally check that the in reverse spread slopes show solid standards with BN. So neither forward nor in reverse signals disappear.","Fig. 4 outlines the training procedures. The 34-layer basic net has higher training mistakes throughout the whole training process, even though the solution space of the 18-layer basic network is a subset of that of the 34-layer one. We argue that this optimization difficulty is unlikely to be caused by vanishing gradients. These basic networks are trained with BN [16], which ensures forwarded signals have non-zero variances. We also verify that the backward propagated gradients display strong norms with BN. So neither forwarded nor backward signals vanish.","The figure shows the training processes. The 34-layer plain network has more training errors during the whole training procedure, despite the solution space of the 18-layer plain network being a part of that of the 34-layer one. We contend that this optimization challenge is unlikely to be due to fading gradients. These plain networks are trained with BN [16], which guarantees forwarded signals have non-zero variances. We also check that the reversed propagated gradients exhibit robust standards with BN. So neither forwarded nor reversed signals disappear.",A,1
997,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Our goal in this paper is to measure general language learning abilities. As such, we study downstream performance on a diverse set of benchmarks, including machine translation, question answering, abstractive summarization, and text classification. Specifically, we measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation. ","The objective of this report is to evaluate broad language learning skills. Therefore, we analyze subsequent performance on a wide variety of benchmarks, including machine translation, question answering, abstractive summarization, and text classification. In particular, we measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation.","Our aim in this article is to quantify general language learning capabilities. To do so, we examine downstream results on a diverse collection of benchmarks, which include machine translation, question answering, abstractive summarization, and text classification. We specifically quantify performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation.  ","The purpose of this paper is to evaluate broad language learning aptitude. Therefore, we review subsequent outcomes across a varied set of benchmarks, encompassing machine translation, question answering, abstractive summarization, and text classification. We specifically gauge performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation.",A,1
842,BERT,"In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.","To teach a deep two-way representation, we just randomly hide some percent of the input tokens, and then guess those hidden tokens. We call this a ""masked LM"" (MLM), though it's often called a Cloze task in research (Taylor, 1953). Here, the final hidden vectors for the mask tokens go into a vocab softmax, like a normal LM. In all our tests, we randomly mask 15% of all WordPiece tokens per sequence. Unlike denoising autoencoders (Vincent et al., 2008), we only predict the masked words rather than remaking the whole input.","In order to develop a deep bidirectional representation, we simply conceal a random percentage of the input tokens, and then infer those concealed tokens. We term this procedure a ""masked LM"" (MLM), although it is frequently called a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors matching the mask tokens are provided to an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we randomly mask 15% of all WordPiece tokens in every sequence. In contrast to denoising autoencoders (Vincent et al., 2008), we only anticipate the masked words rather than reconstructing the whole input.","To train a deep two-way representation, we just hide some percent of the input tokens randomly, and then deduce those hidden tokens. We name this a ""masked LM"" (MLM), even though it's often termed a Cloze task in research (Taylor, 1953). Here, the final hidden vectors for the mask tokens enter a vocab softmax, as in a normal LM. In all our trials, we randomly mask 15% of all WordPiece tokens per sequence. Unlike denoising autoencoders (Vincent et al., 2008), we only deduce the masked words rather than recreating the whole input.",A,1
1021,Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Their proposal network is applied on a single image or multiple large image crops (e.g., 224×224) [20]. We discuss OverFeat and MultiBox in more depth later in context with our method. Shared computation of convolutions [18, 7, 2, 5] has been attracting increasing attention for efficient, yet accurate, visual recognition. The OverFeat paper [18] computes conv features from an image pyramid for classification, localization, and detection. Adaptively-sized pooling (SPP) [7] on shared conv feature maps is proposed for efficient region-based object detection [7, 16] and semantic segmentation [2]. Fast R-CNN [5] enables end-to-end detector training on shared conv features and shows compelling accuracy and speed.","Their suggested design is implemented on either a sole picture or multiple large cropped images (for instance, 224x224) [20]. We will examine OverFeat and MultiBox more thoroughly later when comparing them to our approach. The technique of sharing computations for convolutions [18, 7, 2, 5] has been gaining interest due to its ability to enable efficient yet precise visual recognition. The OverFeat paper [18] produces convolution features from an image pyramid which are then used for classification, localization, and detection. Adaptive-sized pooling (SPP) [7] on shared convolution feature maps is presented for efficient region-based object detection [7, 16] and semantic segmentation [2]. Fast R-CNN [5] allows end-to-end detector training on shared convolution features and demonstrates compelling accuracy and speed.","Their proposed architecture is implemented on either a single image or multiple large cropped images (for example, 224x224) [20]. We will discuss OverFeat and MultiBox in more detail later when comparing them to our method. The technique of sharing computations of convolutions [18, 7, 2, 5] has been attracting growing attention due to its ability to enable efficient yet accurate visual recognition. The OverFeat paper [18] generates convolution features from an image pyramid which are then used for classification, localization, and detection. Adaptively-sized pooling (SPP) [7] on shared convolution feature maps is proposed for efficient region-based object detection [7, 16] and semantic segmentation [2]. Fast R-CNN [5] permits end-to-end detector training on shared convolution features and exhibits compelling accuracy and speed.","Their suggested system is deployed on either a single image or multiple large cropped images (such as 224x224) [20]. We will examine OverFeat and MultiBox more thoroughly later when contrasting them with our method. The technique of sharing computations of convolutions [18, 7, 2, 5] has been attracting increasing attention due to its ability to allow efficient yet precise visual recognition. The OverFeat paper [18] produces convolution features from an image pyramid which are then utilized for classification, localization, and detection. Adaptively-sized pooling (SPP) [7] on shared convolution feature maps is presented for efficient region-based object detection [7, 16] and semantic segmentation [2]. Fast R-CNN [5] enables end-to-end detector training on shared convolution features and displays compelling accuracy and speed.",A,1
1137,ImageNet A Large_Scale Hierarchical Image Database,"ImageNet organizes the different classes of images in a densely populated semantic hierarchy. The main asset of WordNet [9] lies in its semantic structure, i.e. its ontology of concepts. Similarly to WordNet, synsets of images in ImageNet are interlinked by several types of relations, the “IS-A” relation being the most comprehensive and useful. Although one can map any dataset with category labels into a semantic hierarchy by using WordNet, the density of ImageNet is unmatched by others. For example, to our knowledge no existing vision dataset offers images of 147 dog categories. Fig. 3 compares the “cat” and “cattle” subtrees of ImageNet and the ESP dataset [25]. We observe that ImageNet offers much denser and larger trees.","ImageNet sorts the various types of pictures into a packed semantic structure. The primary value of WordNet [9] is in its semantic organization, meaning its system of concepts. Like WordNet, groups of related images in ImageNet are connected through several kinds of relationships, with ""IS-A"" being the most extensive and beneficial. While any labeled dataset can be mapped to a semantic hierarchy using WordNet, ImageNet's density is unparalleled. For instance, no current visual dataset provides photos of 147 dog types. Fig. 3 contrasts the ""cat"" and ""cattle"" subdivisions of ImageNet and the ESP dataset [25]. We see that ImageNet has much more dense and larger structures.","ImageNet arranges the different image categories into a tightly packed semantic network. The main strength of WordNet [9] is its semantic structure, or ontology of concepts. Similar to WordNet, clusters of related images in ImageNet are linked through various relation types, with ""IS-A"" being the most comprehensive and useful. Although any dataset with labels can be mapped to a semantic hierarchy through WordNet, ImageNet's density is unmatched. For example, no existing visual dataset has images of 147 dog breeds. Fig. 3 compares the ""cat"" and ""cattle"" subgroups of ImageNet and the ESP dataset [25]. We observe that ImageNet has much more dense and extensive subgroups.  ","ImageNet organizes the various image classes into a packed semantic network. The primary value of WordNet [9] is its semantic design, meaning its system of concepts. As with WordNet, groups of related images in ImageNet are interconnected through several relation types, with ""IS-A"" being the broadest and most useful. While any labeled dataset could be mapped to a semantic hierarchy via WordNet, ImageNet's density is unparalleled. For instance, no current visual dataset provides images of 147 dog varieties. Fig. 3 contrasts the ""cat"" and ""cattle"" subcategories of ImageNet and the ESP dataset [25]. We see that ImageNet has much more dense and expansive subcategories.",A,1
1317,Sentence Embeddings using Siamese BERT-Networks,"On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively. SBERT can be adapted to a specific task. It sets new state-of-the-art performance on a challenging argument similarity dataset (Misra et al., 2016) and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article (Dor et al., 2018).","On SentEval (Conneau and Kiela, 2018), a tool for assessing sentence embeddings, we see gains of 2.1 and 2.6 points. SBERT is customizable for a particular task. It establishes top performance on a difficult argument similarity dataset (Misra et al., 2016) and on a triplet dataset for telling apart sentences from various parts of a Wikipedia page (Dor et al., 2018).","With SentEval (Conneau and Kiela, 2018), a toolkit for evaluating sentence vectors, we get improvements of 2.1 and 2.6 points. SBERT is adaptable to a certain objective. It achieves new best results on a tricky argument closeness dataset (Misra et al., 2016) and on a triplet set to differentiate sentences from multiple sections of a Wikipedia entry (Dor et al., 2018).  ","On SentEval (Conneau and Kiela, 2018), a framework for testing sentence representations, we obtain boosts of 2.1 and 2.6 points. SBERT can be tailored to a given task. It establishes unprecedented performance on a challenging argument resemblance dataset (Misra et al., 2016) and on a triplet collection to tell apart sentences from various portions of a Wikipedia page (Dor et al., 2018).",A,1
1429,Universal Language Model Fine-tuning for Text Classification,"In NLP, only recently have methods been proposed that go beyond transferring word embeddings. The prevailing approach is to pretrain embeddings that capture additional context via other tasks. Embeddings at different levels are then used as features, concatenated either with the word embeddings or with the inputs at intermediate layers. This method is known as hypercolumns (Hariharan et al., 2015) in CV and is used by Peters et al. (2017), Peters et al. (2018), Wieting and Gimpel (2017), Conneau et al. (2017), and McCann et al. (2017) who use language modeling, paraphrasing, entailment, and Machine Translation (MT) respectively for pretraining.","In the field of natural language processing, techniques that go past simply transferring word embeddings have only recently started being developed. The most common approach is to pre-train embeddings that capture extra context through other tasks. Embeddings at various levels are then utilized as features, concatenated either with the word embeddings or with the inputs at middle layers. This technique is referred to as hypercolumns (Hariharan et al., 2015) in computer vision and is utilized by Peters et al. (2017), Peters et al. (2018), Wieting and Gimpel (2017), Conneau et al. (2017), and McCann et al. (2017) who use language modeling, paraphrasing, entailment, and machine translation respectively for pretraining.","In NLP, methods that surpass just transferring word embeddings have only been proposed recently. The prevailing technique is to pre-train embeddings that capture more context through other tasks. Embeddings at multiple levels are then employed as features, concatenated either with the word embeddings or with the inputs at intermediate layers. This approach is known as hypercolumns (Hariharan et al., 2015) in computer vision and is utilized by Peters et al. (2017), Peters et al. (2018), Wieting and Gimpel (2017), Conneau et al. (2017), and McCann et al. (2017) who use language modeling, paraphrasing, entailment, and machine translation respectively for pre-training.","In natural language processing, approaches going past simply transferring word embeddings have only been put forward recently. The common technique is to pre-train embeddings capturing additional context through other tasks. Embeddings at various levels are then used as features, concatenated either with the word embeddings or with the inputs at middle layers. This method is referred to as hypercolumns (Hariharan et al., 2015) in computer vision and is employed by Peters et al. (2017), Peters et al. (2018), Wieting and Gimpel (2017), Conneau et al. (2017), and McCann et al. (2017) who utilize language modeling, paraphrasing, entailment, and machine translation respectively for pre-training.",A,1
1101,Going deeper with convolutions,"It is noteworthy that the considerations leading to the design of the deep architecture presented in this paper included this factor rather than having a sheer fixation on accuracy numbers. For most of the experiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds at inference time, so that they do not end up to be a purely academic curiosity, but could be put to real world use, even on large datasets, at a reasonable cost.","It is important to point out that the thoughts that led to creating the deep learning model described here focused on this element rather than just aiming for high accuracy. For most of the tests, the models were made to keep the number of computations needed low (1.5 billion), so that they could actually be used in the real world on big datasets without being too expensive.","It should be emphasized that the reasoning behind developing the deep neural network architecture presented in this report considered this factor more than just maximizing accuracy. For the majority of experiments, the models were designed to require a reasonable computational cost during use (1.5 billion operations), so that they would not just be theoretical, but could actually be utilized in practice, even for large datasets, without being prohibitively expensive.  ","Notably, the motivations for engineering the deep learning architecture outlined in this document revolved around this aspect rather than purely chasing high accuracy metrics. For most experiments, the models were constructed to have an affordable computational budget at prediction time (1.5 billion multiplications), so that they would not merely be academic, but could be deployed in real applications, even on massive datasets, at a viable cost.",A,1
1454,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small ( 3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results.","In this research, we examine how the depth of convolutional networks impacts their precision in large-scale image recognition. Our primary contribution is a comprehensive assessment of networks with increasing depth using an architecture with very small (3 × 3) convolution filters. This demonstrates that major improvements over prior setups can be achieved by increasing the depth to 16-19 weight layers. These discoveries formed the basis for our ImageNet Challenge 2014 submission, where our team earned the first and second places in the localization and classification tracks respectively. We also illustrate that our representations generalize effectively to other data sets, where they produce state-of-the-art outcomes.","This work investigates how the number of layers in convolutional networks affects their accuracy in large-scale image identification. We thoroughly test networks of varying depth using a model with tiny (3 x 3) convolution filters. We find pushing the depth to 16-19 weight layers significantly improves on previous configurations. Our ImageNet Challenge 2014 submission using these findings won first and second place in the localization and classification tracks. Our representations also achieve state-of-the-art results when applied to other datasets, showing they generalize well.  ","In this study, we analyze how convolutional network depth impacts performance on large-scale image recognition tasks. We extensively evaluate networks with increasing layers using an architecture with very small (3x3) convolution filters. We demonstrate that going deeper to 16-19 weight layers substantially outperforms previous setups. Our ImageNet Challenge 2014 submission leveraging these insights took first and second place in the localization and classification tracks. Our representations also produce state-of-the-art outcomes on other datasets, exhibiting strong generalization.",A,1
847,BERT,"We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERTLARGE we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization.","We utilize a batch amount of 32 and refine for 3 time periods over the information for all GLUE assignments. For each task, we chose the most effective refinement learning percentage (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Development set. Furthermore, for BERTLARGE we discovered that finetuning was occasionally unstable on small datasets, so we executed several arbitrary restarts and selected the best model on the Development set. With arbitrary restarts, we employ the same pre-trained checkpoint but execute different fine-tuning data shuffling and classifier layer initialization.","We make use of a batch quantity of 32 and adjust for 3 epochs across the data for all GLUE jobs. For every job, we picked the optimum fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev collection. Also, for BERTLARGE we realized that finetuning was sometimes erratic on small datasets, so we went through multiple random reinitiations and chose the top model on the Dev collection. With random reinitiations, we utilize the same pre-trained checkpoint but do different fine-tuning data shuffling and classifier layer initialization.  ","We utilize a batch amount of 32 and refine for 3 cycles over the data for all GLUE tasks. For each task, we selected the best refinement learning percentage (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Development collection. Furthermore, for BERTLARGE we found that finetuning was occasionally variable on small datasets, so we went through several arbitrary restarts and picked the top model on the Development collection. With arbitrary restarts, we use the same pre-trained checkpoint but do different fine-tuning data shuffling and classifier layer initialization.",A,1
1337,Sentence Embeddings using Siamese BERT-Networks,"We showed that BERT out-of-the-box maps sentences to a vector space that is rather unsuitable to be used with common similarity measures like cosine-similarity. The performance for seven STS tasks was below the performance of average GloVe embeddings. To overcome this shortcoming, we presented Sentence-BERT (SBERT). SBERT fine-tunes BERT in a siamese / triplet network architecture. We evaluated the quality on various common benchmarks, where it could achieve a significant improvement over state-of-the-art sentence embeddings methods.","Our experiments demonstrated that the standard BERT model maps sentences into a vector space not well-suited for typical similarity measures such as cosine similarity. Using seven STS tasks, we found it performed worse than average GloVe embeddings. To address this weakness, we introduced Sentence-BERT (SBERT), which fine-tunes BERT using a siamese/triplet network structure. We tested SBERT on various benchmarks and showed substantial gains over other state-of-the-art sentence embedding techniques.","We established that out-of-the-box BERT transforms sentences into a vector space poorly compatible with common similarity metrics like cosine distance. It was inferior to typical GloVe vectors on seven STS jobs. As a solution, we created Sentence-BERT (SBERT), fine-tuning BERT within a siamese/triplet framework. SBERT was evaluated on diverse test sets, substantially outperforming other leading sentence embedding methods. ","Our studies showed vanilla BERT maps sentences into a space not optimal for standard similarity measures such as cosine closeness. It was worse than regular GloVe on seven STS tasks. To improve on this, we introduced Sentence-BERT (SBERT), fine-tuning BERT in a siamese/triplet structure. We tested SBERT extensively, and it significantly beat other state-of-the-art sentence embedding techniques.",A,1
1169,ImageNet Classification with Deep Convolutional Neural Networks,"Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model. ImageNet consists of variable-resolution images, while our system requires a constant input dimensionality.","Given that our model was also used in the ILSVRC-2012 contest, we present our findings on this variant of the data in Section 6 too, even though the test set labels are not public. For ImageNet, two mistake percentages are usually given: top-1 and top-5, where top-5 is the amount of test photos for which the right tag is not one of the five labels the model thinks are most probable. ImageNet has images of differing resolutions, whereas our system needs a steady input size.","Since our model participated in the ILSVRC-2012 competition as well, we also share our results on this form of the data in Section 6, despite test set labels being private. For ImageNet, people tend to provide two error percentages: top-1 and top-5, with top-5 being the proportion of test images where the accurate label is not in the top five labels the model deems most likely. ImageNet has images of varying sizes, but our system requires a fixed input dimension.","Considering our model was entered in the ILSVRC-2012 contest too, we present our findings on this variant of the information in Section 6 also, even with test set tags being confidential. For ImageNet, two mistake rates are commonly given: top-1 and top-5, where top-5 is the percentage of test pictures where the correct tag is not amongst the top five labels the model thinks are most probable. ImageNet contains images of different resolutions, whereas our system needs a consistent input size.",A,1
1199,Language Models are Few-Shot Learners,"In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) “few-shot learning”, or in-context learning where we allow as many demonstrations as will fit into the model’s context window (typically 10 to 100), (b) “one-shot learning”, where we allow only one demonstration, and (c) “zero-shot” learning, where no demonstrations are allowed and only an instruction in natural language is given to the model.","In this report, we examine this theory by teaching a 175 billion parameter self-regressing language program, which we name GPT-3, and calculating its in-context learning skills. Specifically, we review GPT-3 on over two dozen NLP data sets, as well as several new tasks intended to evaluate fast adaptation to tasks unlikely to be straight in the training set. For each task, we assess GPT-3 under 3 circumstances: (a) ""few-shot learning"", or in-context learning where we permit as many examples as will fit into the model's context window (typically 10 to 100), (b) ""one-shot learning"", where we only allow one example, and (c) ""zero-shot"" learning, where no examples are permitted and only an instruction in natural language is provided to the model.","This paper tests the theory by developing a 175 billion parameter self-learning language system called GPT-3, and measuring its ability to learn in context. We specifically judge GPT-3 on over two dozen NLP data sets, plus several new tasks to test quick tuning to tasks probably not in the training set directly. For each task, we rate GPT-3 in 3 ways: (a) ""few-shot learning"", or in-context learning allowing as many examples as fit the context window (usually 10 to 100), (b) ""one-shot learning"" with only one example, and (c) ""zero-shot"" learning without examples, just a natural language instruction.","Here we examine the hypothesis by making a 175 billion parameter autoregressive language program named GPT-3, and testing its in-context learning capacity. We specifically evaluate GPT-3 on over two dozen NLP datasets, and new tasks to evaluate fast tuning to unfamiliar tasks. For each task, we test GPT-3 3 ways: (a) ""few-shot learning"", allowing many context examples (typically 10-100), (b) ""one-shot learning"" with one example, and (c) ""zero-shot learning"" with no examples, only natural language instructions.",A,1
968,"DistilBERT, a distilled version of BERT","As shown in the ablation study, we found it beneficial to leverage the teacher’s knowledge to pre-train with additional distillation signal. Multi-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation to learn a compact question answering model from a set of large question answering models. An application of multi-distillation is multi-linguality: Tsai et al. [2019] adopts a similar approach to us by pre-training a multilingual model from scratch solely through distillation.",The ablation analysis demonstrated that utilizing the teacher's expertise to pre-train with extra distillation cues was advantageous. Yang et al. [2019] combined the insights of an ensemble of instructors through multi-task learning to regulate distillation. They employed Multi-Task Knowledge Distillation to develop a compact question answering system from a collection of large question answering models. One use of multi-distillation is multi-linguality: Tsai et al. [2019] took a similar approach to us by pre-training a multilingual model from the ground up solely via distillation.,"As evidenced in the ablation research, harnessing the teacher's knowledge to pre-train with supplementary distillation signals was found to be beneficial. Yang et al. [2019] amalgamated the wisdom of a group of teachers through multi-task learning to control distillation. They utilized Multi-Task Knowledge Distillation to construct a compact question answering model from multiple large question answering models. One application of multi-distillation is multi-lingual: Tsai et al. [2019] adopted a similar tactic to us by pre-training a multilingual model from baseline only through distillation.  ",The ablation study showed that using the teacher's expertise to pre-train with extra distillation clues was advantageous. Yang et al. [2019] combined the insights of a collection of teachers via multi-task learning to regulate distillation. They used Multi-Task Knowledge Distillation to develop a compact question answering system from several large question answering models. One use of multi-distillation is multi-lingual: Tsai et al. [2019] took a similar approach to us by pre-training a multilingual model from the ground up only through distillation.,A,1
1122,Going deeper with convolutions,"We chose GoogLeNet as our team-name in the ILSVRC14 competition. This name is an homage to Yann LeCuns pioneering LeNet 5 network [10]. We also use GoogLeNet to refer to the particular incarnation of the Inception architecture used in our submission for the competition. We have also used a deeper and wider Inception network, the quality of which was slightly inferior, but adding it to the ensemble seemed to improve the results marginally. We omit the details of that network, since our experiments have shown that the influence of the exact architectural parameters is relatively minor.","For the ILSVRC14 contest, we selected GoogLeNet to be our team's name. This was a tribute to Yann LeCun's groundbreaking LeNet 5 network [10]. We also use GoogLeNet when referring to the specific version of the Inception architecture we submitted for the competition. We tested a larger and more complex Inception network too, which was slightly worse on its own but helped the ensemble a little when added. We won't go into the details of that network, since our tests showed the exact architecture parameters aren't too important.","When entering the ILSVRC14 challenge, our team chose the name GoogLeNet. This was meant as an homage to Yann LeCun's pioneering LeNet 5 network [10]. GoogLeNet also refers to the particular Inception architecture variation we used for our competition submission. We tried out a bigger and deeper Inception network as well, which was marginally inferior by itself but improved the ensemble results slightly when incorporated. We won't discuss the specifics of that network, since experiments showed the precise architectural details aren't very significant.  ","For the ILSVRC14 competition, our team selected the name GoogLeNet. This was intended as a tribute to Yann LeCun's groundbreaking LeNet 5 network [10]. We also use GoogLeNet to refer to the particular version of the Inception architecture that was part of our submission. Additionally, we tested an even larger and deeper Inception network, which was slightly worse on its own but provided a small boost to the ensemble when added. We won't provide the specifics of that network, since experiments demonstrated the exact architectural parameters are not very important.",A,1
986,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied. After layer normalization, a residual skip connection (He et al., 2016) adds each subcomponent’s input to its output. Dropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack. The decoder is similar in structure to the encoder except that it includes a standard attention 3. ","We utilize a streamlined form of layer normalization that only rescales the activations without any additive predisposition. Post layer normalization, a residual bypass linkage (He et al., 2016) aggregates each subcomponent's input to its output. Dropout (Srivastava et al., 2014) is employed inside the feed-forward network, on the bypass linkage, on the attention weights, and at the input and output of the entire pile. The decoder has a comparable structure to the encoder excluding a standard attention mechanism.","We make use of a simplified adaptation of layer normalization which solely normalizes the activations minus any additive inclination. After layer normalization, a residual skip connection (He et al., 2016) combines each subcomponent's entrance to its exit. Dropout (Srivastava et al., 2014) is utilized inside the feed-forward network, on the skip connection, on the attention loads, and at the inlet and outlet of the entire stack. The decoder has a similar form to the encoder apart from comprising a standard attention procedure.  ","We utilize a streamlined variant of layer normalization that just rescales the activations without applying any additive predisposition. Following layer normalization, a residual bypass link (He et al., 2016) aggregates each subcomponent's entrance to its way out. Dropout (Srivastava et al., 2014) is employed within the feed-forward network, on the bypass link, on the attention burdens, and at the inlet and outlet of the entire pile. The decoder has a similar makeup to the encoder barring including a standard attention means.",A,1
1386,Transformer-XL,"This is analogous to truncated BPTT (Mikolov et al., 2010), a technique developed for training RNNLMs. However, different from truncated BPTT, our method caches a sequence of hidden states instead of the last one, and should be applied together with the relative positional encoding technique described in Section 3.3. Besides achieving extra long context and resolving fragmentation, another benefit that comes with the recurrence scheme is significantly faster evaluation. Specifically, during evaluation, the representations from the previous segments can be reused instead of being computed from scratch as in the case of the vanilla model. In our experiments on enwiki8, Transformer-XL is up to 1,800+ times faster than the vanilla model during evaluation (see Section 4).","This technique is similar to truncated BPTT (Mikolov et al., 2010), a method developed for training RNNLMs. However, unlike truncated BPTT which caches only the last hidden state, our approach stores a sequence of hidden states and should be used with the relative positional encoding method from Section 3.3. In addition to enabling extra long context and resolving fragmentation, another advantage of this recurrent scheme is much faster evaluation. In particular, during evaluation, the representations from previous segments can be reused rather than computed from scratch as with the vanilla model. In our enwiki8 experiments, Transformer-XL was up to 1,800+ times faster than the vanilla model during evaluation (see Section 4).","This approach is analogous to truncated BPTT (Mikolov et al., 2010), a procedure created for training RNNLMs. But our technique differs from truncated BPTT in that it stores a sequence of hidden states instead of just the last one, and needs to be combined with the relative positional encoding approach outlined in Section 3.3. On top of providing extra long context and fixing fragmentation, another benefit of this recurrent system is substantially faster assessment. Specifically, during assessment, the representations from earlier segments can be reused rather than calculated from the beginning as in the vanilla model. In our enwiki8 tests, Transformer-XL was up to 1,800+ times quicker than the vanilla model during assessment (see Section 4).  ","This method is similar to truncated BPTT (Mikolov et al., 2010), a process invented for training RNNLMs. However, our strategy caches a sequence of hidden states rather than only the final one, unlike truncated BPTT, and requires using the relative positional encoding approach described in Section 3.3. Apart from enabling extra long context and solving fragmentation, another advantage of this recurrent framework is much faster appraisal. In particular, during appraisal, the representations from prior segments can be reused instead of computed from zero as in the vanilla model. In our enwiki8 experiments, Transformer-XL was up to 1,800+ times faster than the vanilla model during appraisal (see Section 4).",A,1
1223,Language Models are Unsupervised Multitask Learners,"The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.","The ability of the language model to understand and generate natural language is critical for accomplishing new tasks without additional training data, and expanding its capacity leads to better performance on tasks in a logarithmic way. Our most advanced model, GPT-2, uses 1.5 billion parameters in a Transformer architecture to set new benchmarks on 7 out of 8 language modeling datasets we tested without any task-specific fine-tuning. But there is still room for improvement on modeling a large web text corpus. Text samples from GPT-2 showcase these advancements and include coherent multi-sentence passages. These results indicate a promising approach to creating language systems that can learn to carry out tasks just from seeing examples in naturally occurring text.","The language comprehension and generation strengths of the model are indispensable for successfully transferring to new tasks without extra training, and increasing these strengths boosts task performance exponentially. GPT-2, our biggest model with 1.5 billion Transformer parameters, establishes state-of-the-art results on 7 of 8 language modeling datasets we evaluated in a zero-shot context, but still has difficulty fully modeling WebText. Text excerpts from GPT-2 reflect these enhancements and have coherent paragraph structure. These findings point to a promising method for developing language processing systems capable of learning tasks from demonstrations found in natural text.","The language model's capacity to understand and generate text is vital to accomplishing zero-shot task transfer, and expanding it improves performance across tasks in a logarithmic fashion. GPT-2, our largest 1.5 billion parameter Transformer model, achieves best-in-class results on 7 of 8 language modeling datasets we tested in a zero-shot environment, but still struggles to fully capture WebText. Text samples from GPT-2 exhibit these improvements and contain coherent multi-sentence passages. These results suggest a promising approach to building language systems that learn to perform tasks solely from naturally occurring examples in text.",A,1
877,Deep contextualized word representations,"Finally, we found it beneficial to add a moderate amount of dropout to ELMo (Srivastava et al., 2014) and in some cases to regularize the ELMo weights by adding λkwk 2 2 to the loss. This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers. 3.4 Pre-trained bidirectional language model architecture The pre-trained biLMs in this paper are similar to the architectures in Jozefowicz et al. ´ (2016) and Kim et al. (2015), but modified to support joint training of both directions and add a residual connection between LSTM layers. ","In conclusion, we discovered that applying a moderate amount of dropout to ELMo (Srivastava et al., 2014) and sometimes regularizing the ELMo weights by incorporating λkwk^2_2 into the loss function was beneficial. This enforces an inductive bias on the ELMo weights to remain near an average of all biLM layers. The pre-trained biLMs in this paper have architectures akin to those in Jozefowicz et al. (2016) and Kim et al. (2015), however altered to facilitate joint training of both directions and append a residual link between LSTM layers.","To summarize, adding a moderate degree of dropout to ELMo (Srivastava et al., 2014) and in some situations regularizing the ELMo weights by introducing λkwk^2_2 to the loss proved advantageous. This imposes an inductive predisposition on the ELMo weights to persist near a mean of all biLM layers. The pre-trained biLMs described in this paper possess architectures similar to those detailed in Jozefowicz et al. (2016) and Kim et al. (2015), but modified to enable joint training of both directions and attach a residual connection between LSTM layers.  ","In closing, we established that applying a moderate measure of dropout to ELMo (Srivastava et al., 2014) and occasionally regularizing the ELMo weights by incorporating λkwk^2_2 into the loss was beneficial. This enforces an inductive inclination on the ELMo weights to linger near an average of all biLM layers. The pre-trained biLMs presented in this paper have architectures comparable to those documented in Jozefowicz et al. (2016) and Kim et al. (2015), however altered to facilitate collective training of both directions and append a residual link between LSTM layers.",A,1
1158,ImageNet A Large_Scale Hierarchical Image Database,"ImageNet can be extended to provide additional information about each image. One such information is the spatial extent of the objects in each image. Two application areas come to mind. First, for training a robust object detection algorithm one often needs localized objects in different poses and under different viewpoints. Second, having localized objects in cluttered scenes enables users to use ImageNet as a benchmark dataset for object localization algorithms.","The ImageNet dataset could be expanded to include more details about each of the images. For instance, the locations of objects within each image could be added. This extra information would be useful for two main purposes. First, it would help in developing better object detection algorithms by providing examples of objects shown in different positions and angles. Second, it would allow ImageNet to be used as a benchmark dataset for evaluating object localization techniques in cluttered images.",The ImageNet image database has the potential to be enhanced with additional annotations for each picture. One useful annotation would be marking the spatial boundaries around objects in the images. Providing this object localization data would have two valuable applications. One is that it would assist in training more robust object detection systems that can identify objects posed at varying orientations and viewpoints. Another is that it would enable benchmarking object localization algorithms on ImageNet's images containing many objects.,"ImageNet could be expanded by adding more metadata for each image in the dataset. An example is delineating the location and extent of objects within each image. This extra information would have two main uses. First, it would facilitate developing object detection models that are more resilient to different object poses and camera angles, since examples would be provided. Second, it would allow ImageNet to benchmark performance of object localization techniques on cluttered images with many objects.",A,1
1388,Transformer-XL,"In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a temporal clue or “bias” about how information should be gathered, i.e., where to attend. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer. More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner.","To prevent this problem, the key concept is to only encode the relative position data in the hidden layers. In essence, the position encoding provides the model with a temporal hint or ""prejudice"" about how to collect information, namely where to pay attention. Similarly, rather than statically integrating the bias into the initial embedding, one can inject the same details into the attention score of every layer. Most importantly, it is more intuitive and generalizable to characterize the temporal bias relatively.","In order to steer clear of this failure, the fundamental notion is to exclusively encode the comparative positional knowledge in the concealed states. In principle, the positional encoding furnishes the model with a chronological clue or ""partiality"" regarding how material ought to be compiled, specifically where to direct attention. For the same rationale, in lieu of solidifying prejudice into the initial embedding, one could infuse the same intelligence into the attention tally of each stratum. Above all, it is more instinctive and transferable to delineate the chronological bias relatively.","To sidestep this breakdown, the cardinal tenet is to only transcribe the proportional situational gen in the veiled tiers. At its core, the positional encoding endows the model with a temporal hint or ""penchant"" about how info should be amassed, viz. where to spotlight. Toward the same end, rather than ingraining bias statically into the initial embedding, one can suffuse the same dope into the attention score of each echelon. Critically, it is more intuitive and fungible to delineate the temporal bias relatively.",A,1
945,Deep Residual Learning for Image Recognition,"The ImageNet Localization (LOC) task [36] requires to classify and localize the objects. Following [40, 41], we assume that the image-level classifiers are first adopted for predicting the class labels of an image, and the localization algorithm only accounts for predicting bounding boxes based on the predicted classes. We adopt the “per-class regression” (PCR) strategy [40, 41], learning a bounding box regressor for each class. We pre-train the networks for ImageNet classification and then fine-tune them for localization. We train networks on the provided 1000-class ImageNet training set. Our localization algorithm is based on the RPN framework of [32] with a few modifications.","The ImageNet Localization (LOC) challenge [36] necessitates categorizing and pinpointing the objects. As in [40, 41], we presume image-level classifiers are firstly utilized for forecasting the category labels of an image, and the localization algorithm only handles predicting bounding boxes founded on the anticipated classes. We take on the ""per-class regression"" (PCR) plan [40, 41], educating a bounding box regressor for each class. We pre-train the networks for ImageNet categorization then fine-tune them for localization. We coach networks on the given 1000-class ImageNet training set. Our localization algorithm depends on the RPN structure of [32] with a few tweaks.","The ImageNet Localization (LOC) job [36] entails sorting and situating the objects. As per [40, 41], we think image-level sorters are first adopted for predicting the type names of an image, and the localization formula only deals with foretelling bounding boxes based on the predicted types. We take up the ""per-class regression"" (PCR) policy [40, 41], teaching a bounding box regressor for each type. We pre-train the networks for ImageNet sorting then fine-tune them for localization. We tutor networks on the provided 1000-class ImageNet training set. Our localization formula relies on the RPN form of [32] with a few changes.  ","The ImageNet Localization (LOC) effort [36] necessitates categorizing and pinpointing the objects. As in [40, 41], we believe image-level categorizers are first used for anticipating the category labels of an image, and the localization method only manages predicting bounding boxes founded on the predicted categories. We adopt the ""per-class regression"" (PCR) strategy [40, 41], instructing a bounding box regressor for each category. We pre-train the networks for ImageNet categorization then fine-tune them for localization. We coach networks on the given 1000-class ImageNet training set. Our localization method hinges on the RPN framework of [32] with a few alterations.",A,1
856,BERT,"It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.","For a long time, people have understood that making machine learning models bigger improves performance on large jobs like translation and language modeling. This is shown in Table 6, which displays language model perplexity on held-out training data. However, we think this is the first research to clearly prove that going to really big model sizes also substantially boosts accuracy on tiny tasks, if the model is pre-trained enough.","It has been known for quite some time that expanding the scale of a model results in steady enhancements on large tasks such as machine translation and language modeling. This is exhibited by the language model perplexity of held-out training information found in Table 6. However, our belief is that this is the first piece of work to persuasively demonstrate that increasing to very large model sizes also causes significant improvements on very small tasks, assuming the model has gone through adequate pre-training. ","For many years, researchers have been aware that growing the size of a machine learning model leads to ongoing improvements on big jobs like translation and language modeling. You can see this in Table 6, which shows language model perplexity on held-out training data. But our view is that this is the first study to compellingly prove that moving to really enormous model sizes also produces big gains on tiny tasks, if the model has sufficiently pre-trained.",A,1
1041,Generative Adversarial Nets,"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game.","We present a new system for calculating generative models through an adversarial process. In this system, we concurrently develop two models: a generative model G that represents the data distribution, and a discriminative model D that calculates the likelihood that a sample originated from the training data rather than G. The training process for G is to maximize the probability of D making an error. This framework is equivalent to a minimax two-player game.","We put forward a novel approach for determining generative models using an adversarial technique. Here, we simultaneously construct two models: a generative model G capturing the data distribution, and a discriminative model D evaluating the chance a sample was from the training information not G. The training technique for G is increasing the likelihood of D misjudging. This framework matches a minimax two-player match. ","We present a new method for assessing generative models through an adversarial procedure. We concurrently build two models: a generative model G grasping the data distribution, and a discriminative model D gauging the probability a sample was from the training data instead of G. The training process for G is maximizing the chance of D erring. This framework is tantamount to a minimax two-player contest.",A,1
918,Deep Residual Learning for Image Recognition,"Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Downsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2. 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) plain-18 plain-34 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) ResNet-18 ResNet-34 18-layer 34-layer 18-layer 34-layer Figure 4.","The fundamental components are displayed inside parentheses (refer to Figure 5 as well), with the quantities of components piled up. Subsampling is executed by conv3 1, conv4 1, and conv5 1 with a step of 2. 0 10 20 30 40 50 20 30 40 50 60 iteration (1e4) mistake (%) plain-18 plain-34 0 10 20 30 40 50 20 30 40 50 60 iteration (1e4) mistake (%) ResNet-18 ResNet-34 18-layer 34-layer 18-layer 34-layer Figure 4.","The building blocks are enclosed in brackets (also see Fig. 5), with the number of blocks stacked. Downsampling is done by conv3 1, conv4 1, and conv5 1 with a stride of 2. 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) plain-18 plain-34 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) ResNet-18 ResNet-34 18-layer 34-layer 18-layer 34-layer Figure 4.","The elementary components are shown inside parentheses (refer to Figure 5 too), with the amounts of components piled up. Subsampling is accomplished by conv3 1, conv4 1, and conv5 1 with a pace of 2. 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) plain-18 plain-34 0 10 20 30 40 50 20 30 40 50 60 iter. (1e4) error (%) ResNet-18 ResNet-34 18-layer 34-layer 18-layer 34-layer Figure 4.",A,1
831,BERT,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.","We present a new language model named BERT, which is short for Bidirectional Encoder Representations from Transformers. In contrast to other recent language models (Peters et al., 2018a; Radford et al., 2018), BERT is intended to pre-train deep bidirectional representations using unlabeled text by simultaneously conditioning on context from both directions in all layers. Therefore, the pre-trained BERT model can be fine-tuned with just one extra output layer to produce state-of-the-art models for many different tasks, like question answering and language inference, without considerable task-specific architectural changes.","We put forward a novel language representation called BERT, which represents Bidirectional Encoder Representations from Transformers. Not like other latest language models (Peters et al., 2018a; Radford et al., 2018), BERT's design lets it pre-train deep bidirectional representations from unlabeled texts by together depending on both left and right contexts across all layers. Thus, the pre-trained BERT model can be adjusted with only one more output layer to make cutting-edge models for various tasks, such as question response and language deduction, without major task-specific structural modifications.  ","We present a new language model named BERT, shorthand for Bidirectional Encoder Representations from Transformers. In contrast with other recent language models (Peters et al., 2018a; Radford et al., 2018), BERT is engineered to pre-train deep bidirectional representations from unlabeled text by simultaneously using both left and right context in all layers. Consequently, the pre-trained BERT model can be fine-tuned by adding just one extra output layer to generate state-of-the-art models for many tasks, such as question answering and language inference, without large task-specific architectural changes.",A,1
857,BERT,"All of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added.","The BERT outcomes shown thus far have utilized fine-tuning, where a basic classification layer is appended to the pre-trained model, and all parameters are collectively fine-tuned on a downstream task. However, extracting fixed features from the pretrained model has certain benefits. First, not every task can be easily represented by a Transformer encoder architecture, so a task-specific model design needs to be added.","All prior BERT results used fine-tuning, adding a simple classification layer to the pre-trained model and jointly fine-tuning all parameters on a downstream task. However, extracting static features from the pretrained model has advantages. First, some tasks can't be easily modeled by a Transformer encoder, requiring a task-specific architecture. ","The BERT outputs so far used fine-tuning, appending a basic classification layer to the pre-trained model, and jointly fine-tuning all parameters on a downstream task. However, extracting immutable features from the pretrained model has upsides. First, certain tasks can't be readily depicted by a Transformer encoder design, necessitating a task-specific architecture.",A,1
1306,RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We found the provided NLI-format data to be challenging to work with. Instead we use the reformatted WNLI data from SuperGLUE (Wang et al., 2019a), which indicates the span of the query pronoun and referent. We finetune RoBERTa using the margin ranking loss from Kocijan et al. (2019). For a given input sentence, we use spaCy (Honnibal and Montani, 2017) to extract additional candidate noun phrases from the sentence and finetune our model so that it assigns higher scores to positive referent phrases than for any of the generated negative candidate phrases.","The NLI-format data that was given to us was difficult to utilize. Rather, we make use of the reformatted WNLI information from SuperGLUE (Wang et al., 2019a), which points out the extent of the query pronoun and referent. We fine-tune RoBERTa utilizing the margin ranking loss from Kocijan et al. (2019). For any particular input sentence, we leverage spaCy (Honnibal and Montani, 2017) to extract extra nominee noun phrases from the sentence and fine-tune our model so it assigns elevated scores to affirmative referent phrases over any of the formed negative nominee phrases.","We found the NLI-format data provided to be tricky to work with. As an alternative, we utilize the reformatted WNLI information from SuperGLUE (Wang et al., 2019a), which highlights the range of the query pronoun and referent. We refine RoBERTa employing the margin ranking loss from Kocijan et al. (2019). For any given input sentence, we harness spaCy (Honnibal and Montani, 2017) to extract supplementary candidate noun phrases from the sentence and refine our model so it assigns higher ratings to positive referent phrases than any of the produced negative candidate phrases.  ","The NLI-format data given to us was challenging to utilize. Rather, we employ the reformatted WNLI data from SuperGLUE (Wang et al., 2019a), which indicates the extent of the query pronoun and referent. We tune RoBERTa using the margin ranking loss from Kocijan et al. (2019). For any specific input sentence, we use spaCy (Honnibal and Montani, 2017) to extract additional nominee noun phrases from the sentence and tune our model so that it assigns elevated scores to affirmative referent phrases over any of the generated negative nominee phrases.",A,1
1277,Neural Machine Translation by Jointly Learning To Align and Translate,"For the activation function f of an RNN, we use the gated hidden unit recently proposed by Cho et al. (2014a). The gated hidden unit is an alternative to the conventional simple units such as an element-wise tanh. This gated unit is similar to a long short-term memory (LSTM) unit proposed earlier by Hochreiter and Schmidhuber (1997), sharing with it the ability to better model and learn long-term dependencies. This is made possible by having computation paths in the unfolded RNN for which the product of derivatives is close to 1. These paths allow gradients to flow backward easily without suffering too much from the vanishing effect (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013a).","The activation function we use for the RNN is the gated hidden unit recently developed by Cho et al. (2014a). This gated unit is an alternative to more basic units like element-wise tanh. It is similar to the long short-term memory unit proposed earlier by Hochreiter and Schmidhuber (1997), in that it shares the ability to better model and learn long-term dependencies. This is achieved by having computation paths in the unfolded RNN where the product of derivatives is close to 1. These paths let gradients flow backward without too much vanishing (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013a).","For the RNN activation function, we utilize the gated hidden unit recently created by Cho et al. (2014a). This gated unit is a substitute for conventional simple units like element-wise tanh. It resembles the long short-term memory unit proposed earlier by Hochreiter and Schmidhuber (1997), sharing the capacity to better model and learn long-term dependencies. This is accomplished by having computation paths in the unfolded RNN where the product of derivatives is near 1. These paths enable gradients to flow backward without excessive vanishing (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013a).  ","The activation function we employ for the RNN is the gated hidden unit recently invented by Cho et al. (2014a). This gated unit is an alternative to traditional basic units such as element-wise tanh. It is similar to the long short-term memory unit proposed earlier by Hochreiter and Schmidhuber (1997), in that it shares the ability to better model and learn long-term dependencies. This is achieved by having computation paths in the unfolded RNN where the product of derivatives is close to 1. These paths allow gradients to flow backward without too much vanishing (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013a).",A,1
1068,GloVe_Global Vectors for Word Representation,"Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.","Our system effectively uses statistical data by learning only from the non-zero components in a word-word co-occurrence table, instead of the entire sparse table or separate context windows in a large dataset. The system generates a vector space with meaningful substructure, as shown by its 75% accuracy on a recent word analogy task. It also exceeds the performance of related models on similarity tasks and named entity recognition.","Our algorithm efficiently harnesses statistical patterns by being trained exclusively on the non-null elements in a word-word co-occurrence matrix, rather than the full sparse matrix or individual context windows in an extensive corpus. The algorithm produces a vector space with meaningful latent structure, demonstrated by its 75% score on a recent word analogy evaluation. It also surpasses related algorithms on similarity evaluations and named entity recognition.","Our program successfully leverages statistical information by learning solely from the filled components in a word-word co-occurrence array, rather than the whole sparse array or isolated context windows in a massive dataset. The program generates a vector space with meaningful underlying structure, validated by its 75% result on a recent word analogy assessment. It also outperforms analogous programs on similarity assessments and named entity recognition.",A,1
1005,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"For example, automatic summarization is done by feeding in a document followed by the text “TL;DR:” (short for “too long, didn’t read”, a common abbreviation) and then the summary is predicted via autoregressive decoding. We mainly consider models that explicitly process an input with an encoder before generating an output with a separate decoder and we focus on transfer learning rather than zero-shot learning. Finally, Keskar et al. (2019b) unify many NLP tasks as “span extraction”, where text corresponding to possible output choices are appended to the input and the model is trained to extract the input span corresponding to the correct choice. ","As an illustration, summarizing text automatically involves inputting a document and then the abbreviation ""TL;DR:"" (meaning ""too long, didn't read""), after which the summary is generated via autoregressive decoding. We primarily examine models that explicitly encode an input before generating an output with a distinct decoder, and we emphasize transfer learning over zero-shot learning. Furthermore, Keskar et al. (2019b) consolidate many NLP tasks into ""span extraction"", where text matching potential output options is added to the input, and the model is conditioned to extract the input span that matches the right choice.","To demonstrate, automated summarization works by feeding in a document followed by the shorthand ""TL;DR:"" (standing for ""too long, didn't read""), and then predicting the summary via autoregressive decoding. Our focus is on models that first encode an input explicitly before generating output with a separate decoder, and we prioritize transfer learning instead of zero-shot learning. Additionally, Keskar et al. (2019b) unify many NLP tasks into ""span extraction"", appending text corresponding to possible output choices to the input, and training the model to extract the input span that matches the correct choice.","As an example, automatic summarization involves inputting a document and the abbreviation ""TL;DR:"" (meaning ""too long, didn't read""), and then generating the summary through autoregressive decoding. We concentrate on models that first encode an input explicitly before producing output with a distinct decoder, and we emphasize transfer learning rather than zero-shot learning. Also, Keskar et al. (2019b) consolidate many NLP tasks into ""span extraction"", adding text corresponding to potential output options to the input, and conditioning the model to extract the input span matching the right choice.",A,1
922,Deep Residual Learning for Image Recognition,"In the first comparison (Table 2 and Fig. 4 right), we use identity mapping for all shortcuts and zero-padding for increasing dimensions (option A). So they have no extra parameter compared to the plain counterparts. We have three major observations from Table 2 and Fig. 4. First, the situation is reversed with residual learning – the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%). More importantly, the 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data.","In the initial contrast (Table 2 and Fig. 4 on the right), we utilize the same mapping for all shortcuts and zero-filling for expanding dimensions (choice A). Thus they have no supplementary parameters compared to the plain versions. We have three main notices from Table 2 and Fig. 4. First, the position is turned around with residual learning – the 34-layer ResNet is superior to the 18-layer ResNet (by 2.8%). More significantly, the 34-layer ResNet displays substantially lower training error and is generalizable to the validation information.","In the first juxtaposition (Table 2 and Fig. 4 on the right side), we apply identical projection for all shortcuts and zero-padding for increasing dimensions (option A). Hence they have no extra parameters relative to the plain counterparts. We have three major insights from Table 2 and Fig. 4. Initially, the situation is inverted with residual learning – the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%). More crucially, the 34-layer ResNet exhibits considerably lower training mistake and is generalizable to the validation data.","In the initial comparison (Table 2 and Fig. 4 on the right), we use the same mapping for all shortcuts and zero-filling for expanding dimensions (choice A). Therefore they have no additional parameters compared to the plain versions. We have three major observations from Table 2 and Fig. 4. First, the position is reversed with residual learning – the 34-layer ResNet is superior to the 18-layer ResNet (by 2.8%). More importantly, the 34-layer ResNet displays substantially lower training error and is generalizable to the validation data.",A,1
1000,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Note that we only pre-train on English data, so in order to learn to translate a given model will need to learn to generate text in a new language. In order to train a single model on the diverse set of tasks described above, we cast all of the tasks we consider into a “text-to-text” format—that is, a task where the model is fed some text for context or conditioning and is then asked to produce some output text. This framework provides a consistent training objective both for pre-training and fine-tuning. Specifically, the model is trained with a maximum likelihood objective (using “teacher forcing” (Williams and Zipser, 1989)) regardless of the task. ","Keep in mind that we only do preliminary training using English language data. Therefore, to be able to translate, a given model needs to learn how to generate text in a new tongue. To train just one model on the wide variety of tasks delineated above, we format all the tasks we're looking at into a ""text-to-text"" structure - that is, a task where the model gets some text for context or conditions and is then prompted to generate some output text. This structure gives a steady training goal for both pre-training and fine-tuning. Specifically, the model is trained using a maximum likelihood objective (utilizing ""teacher forcing"" (Williams and Zipser, 1989)) no matter the task.","It should be noted that we only do initial training with data in English. So for a particular model to learn translation, it will need to learn to produce text in a new language. To educate a single model on the diverse set of tasks outlined previously, we put all the tasks we're considering into a ""text-to-text"" form - meaning a task where the model gets some text for context or constraints and is then asked to generate some output text. This format provides a consistent training aim for both pre-training and fine-tuning. In particular, the model is trained using a maximum likelihood goal (employing ""teacher forcing"" (Williams and Zipser, 1989)) regardless of the task.  ","Be aware that we only conduct pre-training using English language data. Therefore, for a given model to learn translation, it will need to learn to create text in a new tongue. To train a single model on the wide variety of tasks described before, we structure all the tasks we're considering into a ""text-to-text"" format - meaning a task where the model receives some text for context or conditions and is then prompted to produce some output text. This structure provides a steady training objective for both pre-training and fine-tuning. Specifically, the model is educated using a maximum likelihood goal (utilizing ""teacher forcing"" (Williams and Zipser, 1989)) no matter the task.",A,1
952,Deep Residual Learning for Image Recognition,"This method reduces the top-5 localization error to 10.6% (Table 13). This is our single-model result on the validation set. Using an ensemble of networks for both classification and localization, we achieve a top-5 localization error of 9.0% on the test set. This number significantly outperforms the ILSVRC 14 results (Table 14), showing a 64% relative reduction of error. This result won the 1st place in the ImageNet localization task in ILSVRC 2015.","This technique decreases the top-5 localization mistake to 10.6% (Table 13). This is our single-model outcome on the validation set. Utilizing a group of networks for both categorization and localization, we accomplish a top-5 localization error of 9.0% on the test set. This number substantially surpasses the ILSVRC 14 results (Table 14), displaying a 64% relative decrease of error. This result was victorious in the 1st position in the ImageNet localization task in ILSVRC 2015.","This approach lowers the top-5 localization inaccuracy to 10.6% (Table 13). This is our single-model finding on the validation set. Employing an assembly of networks for both classification and localization, we achieve a top-5 localization mistake of 9.0% on the test set. This number significantly outdoes the ILSVRC 14 outcomes (Table 14), exhibiting a 64% relative reduction of error. This result won 1st place in the ImageNet localization challenge in ILSVRC 2015.  ","This technique reduces the top-5 localization error to 10.6% (Table 13). This is our single-model outcome on the validation set. Utilizing a collection of networks for both categorization and localization, we accomplish a top-5 localization mistake of 9.0% on the test set. This number substantially beats the ILSVRC 14 results (Table 14), displaying a 64% relative decrease of error. This result was first place in the ImageNet localization competition in ILSVRC 2015.",A,1
951,Deep Residual Learning for Image Recognition,"The image region is cropped from a proposal, warped to 224×224 pixels, and fed into the classification network as in R-CNN [8]. The outputs of this network consist of two sibling fc layers for cls and reg, also in a per-class form. This R-CNN network is fine-tuned on the training set using a mini-batch size of 256 in the RoI-centric fashion. For testing, the RPN generates the highest scored 200 proposals for each predicted class, and the R-CNN network is used to update these proposals’ scores and box positions.","The image area is cut out from a proposed region, resized to 224x224 pixels, and input into the classification neural network as described in R-CNN [8]. The outputs of this network are two fully connected layers for classification and regression, also organized by class. This R-CNN network is tuned on the training data using mini-batches of 256 examples in a region of interest-focused way. For testing, the RPN produces the top 200 highest scoring proposals for each detected class, and the R-CNN network is utilized to refine these proposals' confidence scores and bounding box locations.","A section of the image is extracted from a proposed bounding box, scaled to 224x224 pixels, and fed into the classification model similar to R-CNN [8]. This model generates two sibling fully connected layers for predicting the class and bounding box, in a per-class layout. The R-CNN model is optimized on the training images with mini-batches of 256 centered on the region of interest. During testing, the RPN generates the 200 top-ranked proposals for every identified class, then the R-CNN model adjusts these proposals' probabilities and box coordinates.  ","An image patch is isolated from a proposed region, resized to 224x224, and input to the classification network as in R-CNN [8]. This network produces two parallel fully connected layers for classification and bounding box regression, organized by class. The R-CNN network is fine-tuned on the training images in batches of 256 focused on the region of interest. For testing, the RPN provides the 200 highest scoring proposals per detected class, then the R-CNN network refines these proposals' confidence estimates and bounding box locations.",A,1
815,Bag of Tricks for Efficient Text Classification,"They also have the potential to scale to very large corpus (Agarwal et al., 2014). In this work, we explore ways to scale these baselines to very large corpus with a large output space, in the context of text classification. Inspired by the recent work in efficient word representation learning (Mikolov et al., 2013; Levy et al., 2015), we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes, while achieving performance on par with the state-of-the-art.","These methods also can expand to cover an extremely large collection of data (Agarwal et al., 2014). In this research, we investigate approaches to enlarge these baseline models to very big collections of data with a substantial output area, in text categorization. Drawing inspiration from current advancements in efficient word representation learning (Mikolov et al., 2013; Levy et al., 2015), we demonstrate that linear models with a rank limitation and a fast loss estimation can learn from a billion words in ten minutes, while matching state-of-the-art performance.","They have the ability to grow to encompass a very large dataset (Agarwal et al., 2014). In this work, we explore techniques to increase these baseline models to massive datasets with a wide output space, for text classification tasks. Inspired by latest work in efficient word embedding learning (Mikolov et al., 2013; Levy et al., 2015), we show linear models with a rank constraint and quick loss function approximation can train on one billion words in ten minutes, achieving on par with state-of-the-art results.","These methods can also scale up to an extremely large body of text (Agarwal et al., 2014). In this research, we investigate ways to expand these baseline models to gigantic text collections with a wide output domain, for text classification. Drawing from recent advancements in efficient word embedding learning (Mikolov et al., 2013; Levy et al., 2015), we demonstrate linear models with a rank limit and fast loss estimation can learn from one billion words in ten minutes, while reaching state-of-the-art performance.",A,1
1475,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"At the same time, using a large set of crops, as done by Szegedy et al. (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured.","Furthermore, utilizing a large collection of crops, as implemented by Szegedy et al. (2014), can enhance precision, since it enables finer examination of the input image versus the fully-convolutional structure. Additionally, multi-crop assessment complements dense evaluation owing to differing convolution limits: applying a ConvNet to a crop involves zero-padding the convolved attribute charts, whereas dense evaluation provides natural padding for the same crop from adjacent image regions (due to both the convolutions and spatial pooling), substantially expanding the overall network receptive scope, capturing more context.","Moreover, leveraging a substantial crop set, per Szegedy et al. (2014), may improve performance, because it allows more thorough sampling of the input image compared to the fully-convolutional architecture. Also, multi-crop testing provides complementary benefits to dense testing due to different convolution constraints: convnet crop testing pads convolved maps with zeros, but dense testing naturally pads the same crop using surrounding image areas (via convolutions and spatial pooling), greatly increasing the overall receptive range and capturing more context.","Additionally, employing numerous crops, following Szegedy et al. (2014), can boost accuracy, since it enables finer probing of the input image versus fully-convolutional models. Furthermore, multi-crop analysis complements dense analysis owing to distinct convolution limits: applying a ConvNet to a crop zero-pads the convolved maps, but dense analysis organically pads the same crop using adjacent image areas (through convolutions and spatial pooling), substantially extending the overall receptive scope and incorporating more context.",A,1
979,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Every task we consider—including translation, question answering, and classification—is cast as feeding our model text as input and training it to generate some target text. This allows us to use the same model, loss function, hyperparameters, etc. across our diverse set of tasks. It also provides a standard testbed for the methods included in our empirical survey. “T5” refers to our model, which we dub the “Text-to-Text Transfer Transformer”. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered. ","All the jobs we think about - like translating, answering questions, and categorizing - are framed as feeding text into our system and teaching it to create some desired text. This lets us utilize the same system, error function, hyperparameters, etc. across our diverse set of jobs. It also gives a standard test environment for the techniques included in our empirical review. ""T5"" refers to our system, which we name the ""Text-to-Text Transfer Transformer"". With this unified approach, we can compare the effectiveness of different transfer learning goals, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up systems and data sets beyond what has been considered before.","Every task we examine—including translation, question answering, and classification—is formulated as providing our model text as input and training it to generate some target text. This enables us to use the same model, loss function, hyperparameters, etc. across our diverse set of tasks. It also provides a standard testbed for the methods included in our empirical survey. “T5” refers to our model, which we dub the “Text-to-Text Transfer Transformer”. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered.","All the jobs we look at—like translating, answering questions, and categorizing—are set up as feeding text into our model and teaching it to produce some desired text. This allows us to use the same model, loss function, hyperparameters, etc. across our diverse set of jobs. It also gives a standard testing environment for the techniques included in our empirical review. ""T5"" refers to our model, which we call the ""Text-to-Text Transfer Transformer"". With this unified approach, we can compare the effectiveness of different transfer learning goals, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has been considered before.",A,1
1414,U-Net_Convolutional Networks for Biomedical Image Segmentation,"In total the network has 23 convolutional layers. To allow a seamless tiling of the output segmentation map (see Figure 2), it is important to select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size. The input images and their corresponding segmentation maps are used to train the network with the stochastic gradient descent implementation of Caffe [6]. Due to the unpadded convolutions, the output image is smaller than the input by a constant border width. To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image.","The network contains 23 layers in total that use convolutional operations. To enable a smooth tiling of the output segmentation image (refer to Figure 2), it is vital to choose an input tile dimension so that all 2x2 maximum pooling layers are applied to a layer with even x and y dimensions. The input photos and corresponding segmentation images are utilized to train the network using the stochastic gradient descent implementation from Caffe [6]. Because there is no padding on the convolutions, the output image is smaller than the input by a constant border width. To minimize overhead and maximize GPU memory usage, we prefer large input tiles over a large batch size, so the batch is reduced to a single image.","In full, the network possesses 23 layers that leverage convolutional computations. For seamless tessellation of the output segmentation visual (consult Figure 2), it is essential to elect an input tile measurement so all 2x2 max pooling operations are leveraged on a layer with even x and y magnitudes. The input graphics and associated segmentation graphics are harnessed to drill the network employing stochastic gradient descent from Caffe [6]. Since there is no padding on the convolutions, the output graphic is more compact than the input by a fixed border breadth. To minimize burden and optimize GPU memory utilization, we favor substantial input tiles over a substantial batch amount, hence the batch is narrowed to a single graphic.","The network has a total of 23 layers that use convolutional functions. To enable a smooth covering of the output segmentation image (see Figure 2), it is important to choose an input tile size so that all 2x2 maximum pooling operations are applied to a layer with even x and y dimensions. The input images and matching segmentation images are utilized to train the network using the stochastic gradient descent tool from Caffe [6]. Because the convolutions have no padding, the output image is smaller than the input by a fixed border width. To minimize overhead and maximize GPU memory use, we prefer large input tiles over a large batch size, so the batch is reduced to a single image.",A,1
1181,ImageNet Classification with Deep Convolutional Neural Networks,"We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images and training our network on these extracted patche . This increases the size of our training set by a factor of 2048, though the resulting training examples are, of course, highly interdependent. Without this scheme, our network suffers from substantial overfitting, which would have forced us to use much smaller networks. At test time, the network makes a prediction by extracting five 224 × 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all), and averaging the predictions made by the network’s softmax layer on the ten patches.","We expand the training set data by taking out random 224 x 224 image segments (plus their horizontal flips) from the 256x256 photos and using those segments to train the neural network. This grows the training set by 2048 times, although the resulting examples are quite interrelated. Without this method, our network has considerable overfitting, which would have necessitated much smaller networks. During testing, the network makes a forecast by taking five 224 x 224 segments (the four corner segments and the center segment) and their horizontal flips (so ten segments total), and averaging the predictions from the softmax layer across the ten segments.","We augment the training information by extracting arbitrary 224 x 224 patches (and their left-right reflections) from the 256x256 images and utilizing those patches to optimize our model. This amplifies the size of the training data by 2048 times, despite the resulting training samples being highly dependent. Without this approach, our model suffers from major overfitting, which would have compelled us to employ much smaller models. At prediction time, the model makes a forecast by cropping five 224 x 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (thus ten patches total), and taking the average of the predictions made by the model's softmax layer on the ten patches.  ","We expand the training data by taking random 224 x 224 sections (plus mirrored versions) from the 256x256 images and feeding those sections into the network during training. This grows the training data by 2048 times, although the resulting samples are highly related. Without this technique, our network has substantial overfitting, necessitating much smaller networks. During inference, the network generates a prediction by extracting five 224 x 224 crops (the four corner crops and the center crop) and their horizontal flips (10 crops total), then averaging the predictions from the softmax layer over the 10 crops.",A,1
923,Deep Residual Learning for Image Recognition,"This indicates that the degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth. Second, compared to its plain counterpart, the 34-layer 3We have experimented with more training iterations (3×) and still observed the degradation problem, suggesting that this problem cannot be feasibly addressed by simply using more iterations. This comparison verifies the effectiveness of residual learning on extremely deep systems. Last, we also note that the 18-layer plain/residual nets are comparably accurate (Table 2), but the 18-layer ResNet converges faster (Fig. 4 right vs. left).","This shows that the issue of performance decline is properly handled in this scenario and we succeed in getting accuracy improvements from more layers. Also, compared to the plain version, the 34-layer model demonstrates that residual learning is effective for very deep networks. Finally, we see that the 18-layer plain and residual networks have similar accuracy (Table 2), but the 18-layer ResNet learns faster (Fig. 4 right vs. left).","This implies that the problem of decreasing accuracy is adequately addressed here and we obtain gains in precision from increased depth. Secondly, in contrast to the basic model, the 34-layer one proves the usefulness of residual learning for extremely complex architectures. Additionally, the 18-layer standard and residual networks have comparable precision (Table 2), however the 18-layer ResNet converges more rapidly (Fig. 4 right vs. left).  ","This shows that the challenge of performance deterioration is properly managed in this case and we get improvements in correctness from more layers. Also, compared to the unmodified version, the 34-layer one verifies the value of residual learning on very intricate systems. Lastly, we observe that the 18-layer plain and residual networks have similar accuracy (Table 2), however the 18-layer ResNet learns quicker (Fig. 4 right vs. left).",A,1
1419,U-Net_Convolutional Networks for Biomedical Image Segmentation,"The u-net (averaged over 7 rotated versions of the input data) achieves without any further pre- or postprocessing a warping error of 0.0003529 (the new best score, see Table 1) and a rand-error of 0.0382. This is significantly better than the sliding-window convolutional network result by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing algorithms on this data set use highly data set specific post-processing methods1 applied to the probability map of Ciresan et al. [1]. We also applied the u-net to a cell segmentation task in light microscopic images. This segmentation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13].","The u-net (averaged over 7 rotated versions of the input information) accomplishes without any additional pre- or postprocessing a warping error of 0.0003529 (the new top score, refer to Table 1) and a rand-error of 0.0382. This is noticeably superior to the sliding-window convolutional network outcome by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing algorithms on this data set utilize highly data set particular post-processing techniques1 applied to the probability map of Ciresan et al. [1]. We also used the u-net on a cell segmentation task in light microscopic images. This segmentation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13].","The u-net (averaged over 7 rotated versions of the input data) achieves without any extra pre- or postprocessing a warping error of 0.0003529 (the new top score, see Table 1) and a rand-error of 0.0382. This is significantly superior to the sliding-window convolutional network result by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing algorithms on this data set use highly data set specific post-processing techniques1 applied to the probability map of Ciresan et al. [1]. We also applied the u-net to a cell segmentation task in light microscopic images. This segmentation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13]. ","The u-net (averaged over 7 rotated versions of the input information) accomplishes without any further pre- or postprocessing a warping error of 0.0003529 (the new highest score, refer to Table 1) and a rand-error of 0.0382. This is noticeably better than the sliding-window convolutional network result by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only superior performing algorithms on this data set use highly data set particular post-processing methods1 applied to the probability map of Ciresan et al. [1]. We also utilized the u-net on a cell segmentation task in light microscopic images. This segmentation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13].",A,1
1376,Transformer-XL,"We introduce the notion of recurrence into our deep self-attention network. In particular, instead of computing the hidden states from scratch for each new segment, we reuse the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments. As a result, modeling very longterm dependency becomes possible because information can be propagated through the recurrent connections. Meanwhile, passing information from the previous segment can also resolve the problem of context fragmentation.","We bring in the concept of recurrence into our deep self-attention framework. Specifically, rather than generating the hidden states completely from the beginning for each new portion, we reuse the hidden states obtained in prior portions. The reused hidden states function as memory for the current portion, which constructs a recurrent link between the portions. Consequently, modeling very long-range dependency is achievable because information can be propagated via the recurrent connections. Moreover, transferring information from the previous portion can also fix the issue of context fragmentation.","We introduce the idea of repetition into our deep self-attention model. In particular, instead of producing the hidden states entirely from scratch for every new section, we recycle the hidden states acquired in earlier sections. The recycled hidden states act as remembrance for the current section, which builds a repetitive connection between the sections. As a result, modeling very distant dependency becomes feasible since information can be spread through the repetitive connections. Meanwhile, passing information from the prior section can also resolve the problem of context separation.","We bring in the concept of recurrency into our deep self-attention structure. Specifically, rather than generating the hidden states completely from zero for each new part, we reuse the hidden states obtained in previous parts. The reused hidden states serve as memory for the current part, which constructs a recurrent link between the parts. As a result, modeling very long-span dependency becomes possible because information can be propagated through the recurrent connections. Meanwhile, transferring information from the prior part can also fix the issue of context fragmentation.",A,1
1396,Transformer-XL,"We conduct two sets of ablation studies to examine the effects of two proposed techniques used in Transformer-XL: the recurrence mechanism and the new positional encoding scheme. The first study is performed on WikiText-103, which requires modeling long-term dependency. The results are reported in Table 6. Among the compared encoding schemes, Shaw et al. (2018) is relative, while Vaswani et al. (2017) and Al-Rfou et al. (2018) are absolute. “Full” and “half” losses refer to applying a cross entropy loss to all or the recent half positions in the segment. We found that absolute encodings only work well with half losses because half losses exclude positions with very short attention lengths during training for better generalization.","We perform two groups of analysis studies to investigate the impacts of the two suggested methods utilized in Transformer-XL: the recurrence system and the new positional encoding plan. The initial investigation is led on WikiText-103, which necessitates demonstrating long haul reliance. The discoveries are accounted for in Table 6. Among the thought about encoding plans, Shaw et al. (2018) is relative, while Vaswani et al. (2017) and Al-Rfou et al. (2018) are outright. ""Full"" and ""half"" misfortunes allude to applying a cross entropy misfortune to all or the new half positions in the fragment. We observed that outright encodings just work well with half misfortunes since half misfortunes bar positions with exceptionally short consideration lengths during preparing for better generalization.","We lead two arrangements of removal studies to analyze the consequences of the two proposed procedures utilized in Transformer-XL: the repeat instrument and the new positional encoding plan. The principal investigation is performed on WikiText-103, which requires displaying long haul reliance. The outcomes are accounted for in Table 6. Among the analyzed encoding plans, Shaw et al. (2018) is relative, while Vaswani et al. (2017) and Al-Rfou et al. (2018) are outright. ""Full"" and ""half"" misfortunes allude to applying a cross entropy misfortune to all or the new half positions in the portion. We observed that outright encodings just work well with half misfortunes since half misfortunes prohibit positions with exceptionally short consideration lengths during preparing for better generalization.  ","We lead two sets of end studies to inspect the impacts of the two proposed systems utilized in Transformer-XL: the repeat component and the new positional encoding plan. The initial investigation is acted in WikiText-103, which requires displaying long haul reliance. The discoveries are detailed in Table 6. Among the looked at encoding plans, Shaw et al. (2018) is relative, while Vaswani et al. (2017) and Al-Rfou et al. (2018) are outright. ""Full"" and ""half"" misfortunes allude to applying a cross entropy misfortune to all or the later half positions in the fragment. We observed that outright encodings just work well with half misfortunes since half misfortunes bar positions with exceptionally short consideration lengths during preparing for better generalization.",A,1
1547,"You Only Look Once_Unified, Real-Time Object Detection","Fast YOLO is the fastest object detection method on PASCAL; as far as we know, it is the fastest extant object detector. With 52.7% mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to 63.4% while still maintaining real-time performance. We also train YOLO using VGG-16. This model is more accurate but also significantly slower than YOLO. It is useful for comparison to other detection systems that rely on VGG-16 but since it is slower than real-time the rest of the paper focuses on our faster models.","Fast YOLO is the quickest object detection approach on PASCAL; as far as we are aware, it is the swiftest existing object detector. With 52.7% mAP, it is over two times as precise as previous work on real-time detection. YOLO pushes mAP to 63.4% while still keeping real-time speed. We also educate YOLO employing VGG-16. This prototype is more correct but also considerably slower than YOLO. It is beneficial for contrast to other detection systems that depend on VGG-16 but since it is slower than real-time the rest of the paper concentrates on our faster models.","Fast YOLO is the most rapid object identification technique on PASCAL; to our knowledge, it is the most expeditious current object detector. With 52.7% mAP, it is more than double as accurate as prior real-time detection work. YOLO increases mAP to 63.4% while still retaining real-time velocity. We also train YOLO with VGG-16. This model is more precise but also significantly slower than YOLO. It is useful for comparison to other detection frameworks that use VGG-16 but since it is slower than real-time the remainder of the paper focuses on our swifter models.  ","Fast YOLO is the most speedy object spotting approach on PASCAL; as far as we're aware, it is the most fleet present object detector. With 52.7% mAP, it is over twice as precise as previous real-time detection work. YOLO pushes mAP to 63.4% while still keeping real-time pace. We also school YOLO employing VGG-16. This prototype is more accurate but also considerably slower than YOLO. It is beneficial for contrast to other detection systems that utilize VGG-16 but since it is slower than real-time the rest of the paper concentrates on our faster models.",A,1
793,Attention is All You Need,"Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.","Self-regard, also known as inner-regard, is an attention system connecting different points of one sequence to create a depiction of that sequence. Self-regard has been effectively used in many tasks including reading comprehension, summarizing, inferring from text, and learning representations of sentences independent of task [4, 27, 28, 22]. End-to-end memory networks depend on a repetitive attention system instead of sequence-aligned repetition and have shown good performance on basic language question answering and language modeling [34]. However, as far as we know, the Transformer is the first transduction model that fully depends on self-regard to compute representations of its input and output without using sequence-aligned RNNs or convolution.","Self-focus, sometimes called intra-focus, is a focus mechanism relating different positions of a single sequence to generate a representation of the sequence. Self-focus has been successfully used in various tasks like reading comprehension, abstract summarization, textual implication, and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a repetitive focus mechanism rather than sequence-aligned repetition and have proven effective on simple language question answering and language modeling tasks [34]. However, to our knowledge, the Transformer is the first transduction model that entirely depends on self-focus to compute representations of its input and output without using sequence-aligned RNNs or convolution.  ","Self-attention, also called inner-attention, is an attention system connecting different points of one sequence to create a depiction of that sequence. Self-attention has been effectively used in many tasks including reading comprehension, abstract summarizing, inferring from text, and learning representations of sentences independent of task [4, 27, 28, 22]. End-to-end memory networks depend on a repetitive attention system instead of sequence-aligned repetition and have proven effective on basic language question answering and language modeling tasks [34]. However, to our knowledge, the Transformer is the first transduction model that fully depends on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.",A,1
1150,ImageNet A Large_Scale Hierarchical Image Database,"At least 10 users are asked to vote on each of these images. We then obtain a confidence score table, indicating the probability of an image being a good image given the user votes (Fig. 7(right) shows examples for “Burmese cat” and “cat”). For each of remaining candidate images in this synset, we proceed with the AMT user labeling until a pre-determined confidence score threshold is reached. It is worth noting that the confidence table gives a natural measure of the “semantic difficulty” of the synset. For some synsets, users fail to reach a majority vote for any image, indicating that the synset cannot be easily illustrated by images . Fig. 4 shows that our algorithm successfully filters the candidate images, resulting in a high percentage of clean images per synset.","At minimum 10 people are requested to cast a vote on all of these photos. We then get a confidence score table, showing the likelihood of a photo being a good photo given the user votes (Fig. 7(right) displays examples for ""Burmese cat"" and ""cat""). For every other nominee photo in this synset, we continue with the AMT user tagging until a predetermined confidence score limit is met. Notably, the confidence table provides a natural gauge of the ""semantic difficulty"" of the synset. For some synsets, users fail to achieve a majority vote for any photo, signifying that the synset can't be easily illustrated by photos. Fig. 4 demonstrates that our algorithm successfully filters the candidate photos, resulting in a high percentage of clean photos per synset.","We ask at least 10 users to provide a vote for each of these images. This gives us a confidence score table, which shows the probability that an image is good based on the user votes (Fig. 7(right) has examples for ""Burmese cat"" and ""cat""). For any other candidate images in this synset, we keep having AMT users label them until we reach a predetermined confidence score threshold. Importantly, the confidence table naturally measures the ""semantic difficulty"" of the synset. For some synsets, users can't agree on any image, meaning the synset probably can't be easily illustrated with images. Fig. 4 shows our algorithm successfully filters the candidate images, giving a high percentage of clean images per synset.","We get 10 or more people to vote on all of these pictures. This produces a confidence score table, displaying the chance a picture is good according to the user votes (Fig. 7(right) provides examples for ""Burmese cat"" and ""cat""). For any other possible pictures in this synset, we continue having AMT users label them until we meet a set confidence score limit. Notably, the confidence table naturally assesses the ""semantic difficulty"" of the synset. For some synsets, users can't agree on any picture, suggesting the synset likely can't be easily illustrated with pictures. Fig. 4 shows our algorithm successfully filters the candidate pictures, resulting in a high percentage of clean pictures per synset.",A,1
860,BERT,"Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.","The latest experimental advancements using transfer learning with language models have shown that ample, unsupervised pre-training is a key component of many language comprehension systems. Specifically, these findings allow even tasks with limited resources to benefit from deep one-directional architectures. Our main contribution is expanding these results to deep two-directional architectures, enabling the same pre-trained model to effectively address a wide range of natural language processing tasks.","Recent empirical enhancements utilizing transfer learning with language models have demonstrated that abundant, unsupervised pre-training is a vital part of many language understanding systems. In particular, these outcomes enable even tasks with scarce resources to gain from deep one-way architectures. Our major contribution is additionally generalizing these conclusions to deep two-way architectures, permitting the same pre-trained model to successfully tackle a wide variety of NLP tasks. ","The latest experimental improvements leveraging transfer learning with language models have shown that rich, unsupervised pre-training is an integral part of many language comprehension systems. Specifically, these findings allow even tasks with limited data to benefit from deep one-directional architectures. Our main contribution is further extending these results to deep bidirectional architectures, enabling the same pre-trained model to effectively handle a diverse range of natural language processing tasks.",A,1
1139,ImageNet A Large_Scale Hierarchical Image Database,"In an attempt to tackle the difficult problem of quantifying image diversity, we compute the average image of each synset and measure lossless JPG file size which reflects the amount of information in an image. Our idea is that a synset containing diverse images will result in a blurrier average image, the extreme being a gray image, whereas a synset with little diversity will result in a more structured, sharper average image. We therefore expect to see a smaller JPG file size of the average image of a more diverse synset. Fig. 5 compares the image diversity in four randomly sampled synsets in Caltech101 [8] 3 and the mammal subtree of ImageNet.","To address the challenging issue of measuring image variety, we find the mean image for each category and evaluate the size of lossless JPG files, which shows the quantity of data in an image. Our concept is that a category with diverse images will produce a more blurred average image, the most extreme being a gray image, while a category with little variety will result in a more structured, sharper average image. Thus, we anticipate observing a smaller JPG file size for the average image of a more diverse category. Fig. 5 compares the image diversity in four randomly chosen categories in Caltech101 [8] and the mammal sub-tree of ImageNet.","In an attempt to quantify the complexity of image diversity, we determine the typical image for each grouping and assess the lossless JPG file size, which represents the information content in an image. Our thinking is that a grouping containing varied images will yield a more fuzzy average image, the most extreme being a gray image, while a grouping with little variety will produce a more structured, sharper average image. Therefore, we expect to see a smaller JPG file size for the average image of a more diverse grouping. Fig. 5 contrasts the image diversity in four randomly selected groupings in Caltech101 [8] and the mammal sub-tree of ImageNet.  ","To tackle the tricky issue of measuring image heterogeneity, we calculate the representative image for each category and evaluate the lossless JPG file dimension, which indicates the data amount in an image. Our rationale is that a category with heterogeneous images will generate a more blurred average image, the most extreme being a gray image, whereas a category with little heterogeneity will produce a more structured, sharper average image. Hence, we predict observing a smaller JPG file dimension for the average image of a more heterogeneous category. Fig. 5 compares the image heterogeneity in four randomly chosen categories in Caltech101 [8] and the mammal sub-branch of ImageNet.",A,1
1007,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"At test time, if the model outputs a string corresponding to a number between 1 and 5, we convert it to a floating-point value; otherwise, we treat the model’s prediction as incorrect. This effectively recasts the STS-B regression problem as a 21-class classification problem. Separately, we also convert the Winograd tasks (WNLI from GLUE, WSC from SuperGLUE, and the DPR data set we add to SuperGLUE) into a simpler format that is more amenable to the text-to-text framework. Examples from the Winograd tasks consist of a text passage containing an ambiguous pronoun that could refer to more than one of the noun phrases in the passage. ","During evaluation, if the model generates a string matching a number from 1 to 5, we turn it into a float; else, we see the prediction as wrong. This essentially changes the STS-B regression issue into a 21-class classification one. Also, we convert the Winograd tasks (WNLI from GLUE, WSC from SuperGLUE, and the DPR dataset we append to SuperGLUE) into a simpler form more suitable for the text-to-text framework. Instances from the Winograd tasks have a text excerpt with an ambiguous pronoun that could refer to multiple noun phrases in the excerpt.","When testing, if the model outputs a string representing a number between 1 and 5, we transform it into a floating-point value; otherwise, we consider the model's guess incorrect. This effectively alters the STS-B regression problem into a 21-class classification problem. Independently, we also change the Winograd tasks (WNLI from GLUE, WSC from SuperGLUE, and the DPR data set we add to SuperGLUE) into a more straightforward format more compatible with the text-to-text framework. Examples from the Winograd tasks contain a text passage with an ambiguous pronoun that could refer to more than one noun phrase in the passage.","During evaluation, if the model generates a string matching a number from 1 to 5, we convert it to a float; otherwise, we deem the prediction false. This effectively turns the STS-B regression issue into a 21-class classification one. Separately, we also modify the Winograd tasks (WNLI from GLUE, WSC from SuperGLUE, and the DPR dataset we append to SuperGLUE) into a simpler form more amenable to the text-to-text system. Instances from the Winograd tasks have a text excerpt containing an ambiguous pronoun that could refer to multiple noun phrases in the excerpt.",A,1
932,Deep Residual Learning for Image Recognition,"Then we use a stack of 6n layers with 3×3 convolutions on the feature maps of sizes {32, 16, 8} respectively, with 2n layers for each feature map size. The numbers of filters are {16, 32, 64} respectively. The subsampling is performed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. There are totally 6n+2 stacked weighted layers. The following table summarizes the architecture: output map size 32×32 16×16 8×8 # layers 1+2n 2n 2n # filters 16 32 64 When shortcut connections are used, they are connected to the pairs of 3×3 layers (totally 3n shortcuts).","We construct a stack of 6n layers containing 3x3 convolutions on feature maps of sizes {32, 16, 8} pixels, with 2n layers for each feature map size. The filter numbers are {16, 32, 64}. Subsampling happens via strided convolutions with stride 2. The network finishes with global average pooling, a 10-way fully-connected layer, and softmax. There are 6n+2 total weighted layers stacked. The architecture is summarized as: output size 32x32 16x16 8x8 # layers 1+2n 2n 2n # filters 16 32 64 Shortcuts connect pairs of 3x3 layers, making 3n shortcuts total.","We use a stack of 6n layers, with 3x3 convolutions on feature maps of sizes {32, 16, 8} pixels, using 2n layers for each map size. The filter numbers are {16, 32, 64}. We subsample by convolving with stride 2. The network has global average pooling, a 10-way fully-connected layer, and softmax. In total there are 6n+2 stacked weighted layers. The design is: output dimensions 32x32 16x16 8x8 layer count 1+2n 2n 2n filter count 16 32 64 Shortcuts connect 3x3 layer pairs, making 3n shortcuts. ","We build a stack of 6n layers, applying 3x3 convolutions to feature maps of sizes {32, 16, 8} pixels, utilizing 2n layers per map size. The filter counts are {16, 32, 64}. Downsampling is done by striding convolutions by 2. Finally there is global average pooling, a 10-way fully-connected layer, and softmax. Total stacked weighted layers is 6n+2. The layout is: output size 32x32 16x16 8x8 layer number 1+2n 2n 2n filter number 16 32 64 Shortcut connections link pairs of 3x3 layers, forming 3n shortcuts.",A,1
1164,ImageNet Classification with Deep Convolutional Neural Networks,"To learn about thousands of objects from millions of images, we need a model with a large learning capacity. However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have. Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies).","To become knowledgeable about countless objects from an enormous number of images, our model needs to have a substantial ability to learn. However, the extremely intricate nature of recognizing objects means that even a massive dataset like ImageNet cannot fully define this task. Thus, our model should also contain extensive prior understanding to make up for all the data we lack. Convolutional neural networks (CNNs) represent one such type of model [16, 11, 13, 18, 15, 22, 26]. Their learning ability can be adjusted by changing their depth and width, and they also make robust and largely accurate assumptions about the essence of images (namely, consistency of statistics and locality of pixel relationships).","To gain familiarity with a great many objects from millions upon millions of images, our system requires expansive learning potential. However, the tremendously complex essence of object identification means that not even a gargantuan dataset such as ImageNet can fully delineate this challenge. Therefore, our system should also hold abundant previous knowledge to compensate for all the data we do not possess. Convolutional neural networks (CNNs) constitute one such class of systems [16, 11, 13, 18, 15, 22, 26]. Their learning potential can be controlled by modifying their depth and breadth, and they also make sturdy, generally correct assumptions regarding the nature of images (namely, uniformity of statistics and locality of pixel connections).  ","To become well-versed about uncountable objects from countless images, our model necessitates far-reaching learning capacity. However, the exceptionally intricate character of object recognition signifies that even a massive dataset such as ImageNet cannot completely characterize this task. Accordingly, our model should also contain extensive prior comprehension to offset all the data we lack. Convolutional neural networks (CNNs) represent one such model type [16, 11, 13, 18, 15, 22, 26]. Their learning capacity is adjustable by varying their depth and width, and they also make robust, largely precise assumptions regarding the essence of images (namely, consistency of statistics and locality of pixel linkages).",A,1
1073,GloVe_Global Vectors for Word Representation,"Matrix factorization methods for generating low-dimensional word representations have roots stretching as far back as LSA. These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus. The particular type of information captured by such matrices varies by application. In LSA, the matrices are of “term-document” type, i.e., the rows correspond to words or terms, and the columns correspond to different documents in the corpus. In contrast, the Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996), for example, utilizes matrices of “term-term” type, i.e., the rows and columns correspond to words and the entries correspond to the number of times a given word occurs in the context of another given word.","Methods of generating compact semantic representations of words by factorizing large matrices have origins dating back to latent semantic analysis (LSA). These techniques use low-rank approximations to decompose sizable matrices that encode statistical patterns from a text corpus. The precise kind of data encoded in the matrices varies across applications. In LSA, the matrices have a ""term-document"" structure, meaning the rows represent words or terms, and the columns represent individual documents in the corpus. By contrast, Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996) employs ""term-term"" matrices, where both rows and columns correspond to words, and entries record how often a given word appears near another given word.","Techniques for producing low-dimensional semantic word vectors by decomposing large matrices trace their history to latent semantic analysis (LSA). These approaches leverage low-rank approximations to factor large matrices capturing statistical information from a text corpus. The specific type of data captured in the matrices changes based on the application. With LSA, the matrices follow a ""term-document"" format, with rows as words or terms, and columns as documents from the corpus. Conversely, Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996) uses ""term-term"" matrices, where rows and columns are words, and entries signify how often one word occurs near another. ","Methods of constructing compact semantic word embeddings via matrix factorization have their origins in latent semantic analysis (LSA). Such techniques employ low-rank approximations to decompose substantial matrices encoding statistical patterns from a text corpus. The particular kind of information encoded in these matrices varies by use case. In LSA, a ""term-document"" matrix structure is used, with word/terms as rows and individual corpus documents as columns. In contrast, Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996) adopts a ""term-term"" matrix format, where both rows and columns represent words, and entries capture co-occurrence statistics of word pairs.",A,1
919,Deep Residual Learning for Image Recognition,"Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts. plain ResNet 18 layers 27.94 27.88 34 layers 28.54 25.03 Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation. Here the ResNets have no extra parameter compared to their plain counterparts.","Slim lines show the training error, and thick lines show the validation error of the center crops. On the left: basic networks with 18 and 34 layers. On the right: ResNets with 18 and 34 layers. In this graph, the residual networks don't have any extra parameters compared to their basic versions. Basic ResNet 18 layers 27.94 27.88 34 layers 28.54 25.03 Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation set. Here the ResNets don't have any extra parameters compared to their basic versions.","Thin lines indicate training mistake, and bold lines indicate validation mistake of the center crops. On the left: plain networks of 18 and 34 tiers. On the right: ResNets of 18 and 34 tiers. In this chart, the residual networks contain no extra parameter compared to their plain counterparts. Plain ResNet 18 tiers 27.94 27.88 34 tiers 28.54 25.03 Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation. Here the ResNets contain no extra parameter compared to their plain counterparts. ","Narrow lines show the training inaccuracies, and thick lines show the validation inaccuracies of the center crops. On the left: simple networks with 18 and 34 levels. On the right: ResNets with 18 and 34 levels. In this diagram, the residual networks have no additional parameters compared to their simple versions. Simple ResNet 18 levels 27.94 27.88 34 levels 28.54 25.03 Table 2. Top-1 inaccuracies (%, 10-crop testing) on ImageNet validation. Here the ResNets have no additional parameters compared to their simple versions.",A,1
1127,Going deeper with convolutions,"Our networks were trained using the DistBelief [4] distributed machine learning system using modest amount of model and data-parallelism. Although we used CPU based implementation only, a rough estimate suggests that the GoogLeNet network could be trained to convergence using few high-end GPUs within a week, the main limitation being the memory usage. Our training used asynchronous stochastic gradient descent with 0.9 momentum [17], fixed learning rate schedule (decreasing the learning rate by 4% every 8 epochs).","Our neural networks were developed using the DistBelief distributed machine learning framework with a small degree of model and data parallelism. We utilized only CPU implementations, but projections indicate the GoogLeNet architecture could reach convergence in around 7 days using several high-performance GPUs, with memory consumption being the primary constraint. Training leveraged asynchronous stochastic gradient descent with momentum of 0.9, and a fixed learning rate schedule (reducing the rate by 4% every 8 epochs).","Our models were built leveraging the DistBelief distributed learning system applying moderate model and data parallelism. We used CPU-only implementations, however estimates show the GoogLeNet model could fully train in about a week using a few top-tier GPUs, with memory usage being the main limitation. Training employed asynchronous stochastic gradient descent with 0.9 momentum, and a static learning rate plan (lowering the rate by 4% every 8 epochs).  ","Our neural networks were constructed using the DistBelief distributed machine learning platform applying limited model and data parallelism. Although we utilized CPU-based implementations exclusively, projections indicate the GoogLeNet architecture could converge within a week using several high-end GPUs, with memory demands being the primary constraint. Training made use of asynchronous stochastic gradient descent with momentum of 0.9, and a fixed learning rate agenda (reducing the rate by 4% every 8 epochs).",A,1
1511,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results. In Table 5, we present results of multiple settings, including single-task and multi-task, as well as single models and ensembles. In the multi-task setting, we jointly train an XLNet on the four largest datasets—MNLI, SST-2, QNLI, and QQP—and finetune the network on the other datasets. Only single-task training is employed for the four large datasets.","The GLUE benchmark [34] is a set of 9 tasks for evaluating natural language understanding. The test labels are not publicly available, so researchers must submit predictions to the evaluation server to get test results. Table 5 shows the performance of various settings on GLUE, including single-task and multi-task learning, as well as individual models and ensembles. For multi-task learning, we train an XLNet jointly on the 4 largest datasets - MNLI, SST-2, QNLI, and QQP - then fine-tune on the other datasets. We only use single-task learning for the 4 big datasets.","The GLUE collection [34] contains 9 natural language understanding tasks. The test set labels are kept private, so practitioners need to upload their predictions to the evaluation server to see test results. Table 5 presents the performance of different approaches on GLUE, including training on individual tasks vs multiple tasks, and single models vs ensembles. For multi-task learning, we train one XLNet model together on the 4 biggest datasets - MNLI, SST-2, QNLI, and QQP - and then fine-tune on the other datasets. We only train separately on each task for the 4 large datasets.  ","GLUE [34] is a benchmark with 9 natural language understanding tasks. The test labels are not released publicly, so teams must submit predictions to the evaluation server to get test results. Table 5 shows the performance of various methods on GLUE, including single-task and multi-task training, and individual models versus ensembles. For multi-task learning, we jointly train one XLNet on the 4 largest datasets - MNLI, SST-2, QNLI, and QQP - then fine-tune on the other datasets. We use single-task training exclusively for the 4 big datasets.",A,1
1500,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Specifically, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP], where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although we follow the two-segment data format, XLNet-Large does not use the objective of next sentence prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4). Architecturally, different from BERT that adds an absolute segment embedding to the word embedding at each position, we extend the idea of relative encodings from Transformer-XL to also encode the segments.","In particular, we feed the same input to our model as BERT does: [CLS, A, SEP, B, SEP]. The tokens ""SEP"" and ""CLS"" are special symbols, while ""A"" and ""B"" represent the two segments. Despite using the two-segment format, XLNet-Large does not utilize next sentence prediction [10] since it did not consistently improve performance in our ablation experiments (refer to Section 3.4). Unlike BERT which adds an absolute segment embedding to each word embedding, we build on Transformer-XL's relative encodings to also encode the segments.","Specifically, our model takes as input: [CLS, A, SEP, B, SEP], identical to BERT. ""SEP"" and ""CLS"" are special tokens, and ""A"" and ""B"" are two segments. Although we use the two-segment setup, XLNet-Large skips next sentence prediction [10] because it didn't consistently boost results in our ablation tests (see Section 3.4). Architecturally, rather than BERT's absolute segment embedding added to each word embedding, we extend Transformer-XL's relative encodings to also encode segments.","In particular, we input [CLS, A, SEP, B, SEP] into our model, the same as BERT. ""SEP"" and ""CLS"" are special symbols, while ""A"" and ""B"" represent two segments. Despite employing the two-segment format, XLNet-Large omits next sentence prediction [10] since it didn't reliably improve performance in our ablation studies (refer to Section 3.4). Unlike BERT's absolute segment embedding added to each word embedding, we build on Transformer-XL's relative encodings to also encode the segments.",A,1
1479,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU. While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU.","Training a neural network using multiple graphics processing units (GPUs) takes advantage of data parallelism. The training images are split into smaller batches, with each GPU processing one batch at the same time. After the gradients are calculated for each GPU batch, they are combined to get the overall gradient for the full batch. The gradient calculations happen at the same time across all GPUs, so the result is the same as training on just one GPU. While more complex methods to speed up neural network training have been developed (Krizhevsky, 2014), which use model and data parallelism for different layers, our simpler approach already provides 3.75 times faster training on a standard 4-GPU system compared to using one GPU.","Using many GPUs together for neural network training makes use of data parallelism. The full set of training images is divided into smaller batches, with each GPU handling one batch simultaneously. Once the gradients are found for each GPU's batch, they are averaged together to get the gradient for the entire batch. The gradient math happens at the same time on all GPUs, so it's the same as training on a single GPU. Though more intricate techniques to accelerate neural network training have been created recently (Krizhevsky, 2014), which utilize model and data parallelism for different layers, our more straightforward method already speeds up training by 3.75 times on a typical 4-GPU system, compared to a single GPU.  ","Training neural networks with multiple GPUs leverages data parallelism. The full batch of training images is split up, with each GPU processing a smaller batch at the same time. After the gradients are calculated for each GPU's batch, they are combined to determine the gradient for the whole batch. The gradient calculations occur simultaneously on all GPUs, giving the same result as training on one GPU. While more advanced approaches to speeding up neural network training have been developed recently (Krizhevsky, 2014), using model and data parallelism for different layers, our simpler technique already achieves 3.75x faster training on a standard 4-GPU setup compared to a single GPU.",A,1
828,Bag of Tricks for Efficient Text Classification,"We also report the training time and test time. Test time is reported for a single thread, while training uses 20 threads for both models. and 200. Both models achieve a similar performance with a small hidden layer, but adding bigrams gives us a significant boost in accuracy. At test time, Tagspace needs to compute the scores for all the classes which makes it relatively slow, while our fast inference gives a significant speed-up when the number of classes is large (more than 300K here). Overall, we are more than an order of magnitude faster to obtain model with a better quality.","Additionally, we document the durations for training and testing. Testing time is for a single processor, while training utilizes 20 processors for both models. The two models attain comparable performance with a small concealed layer, but appending bigrams gives a substantial improvement in precision. During testing, Tagspace has to compute the valuations for all classes which makes it relatively slow, while our rapid deduction provides a major acceleration when the quantity of classes is large (over 300K here). On the whole, we are more than ten times quicker to get a model with superior quality.","Furthermore, we chronicle the periods for developing and assessing. Exam duration is for a single core, whereas preparation employs 20 cores for the two archetypes. The brace of paradigms gain analogous competence with a slight veiled stratum, however annexing bigrams gives a significant boost in correctness. At assay juncture, Tagspace must compute the valuations for all ranks which renders it relatively leaden, while our swift inference furnishes a foremost velocity when the figure of ranks is vast (above 300K here). By and large, we are more than tenfold faster to gain a prototype with superior caliber. ","Additionally, we document the timespans for instructing and evaluating. Examination interval is for a single processor, while education uses 20 processors for both examples. The two examples attain analogous capability with a small obscured layer, but attaching bigrams provides a significant improvement in precision. At test point, Tagspace must compute the valuations for all classes which makes it relatively slow, while our rapid deduction gives a major quickening when the amount of classes is huge (over 300K here). On the whole, we are more than tenfold quicker to obtain a model with superior quality.",A,1
1488,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"In comparison, AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. A notable example is BERT [10], which has been the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize bidirectional contexts for reconstruction.","In contrast, AE based pretraining does not explicitly model the data distribution but instead tries to reconstruct the original data from altered input. A prominent instance is BERT [10], which has been the top performing pretraining method. With the input token sequence, some tokens are swapped with a special symbol [MASK], and the model is optimized to predict the original tokens from the modified version. As density modeling is not part of the goal, BERT can leverage bidirectional contexts for reconstruction.","On the other hand, AE based pretraining does not directly estimate densities but rather aims to reproduce the original data from distorted input. A well-known example is BERT [10], which has been the state-of-the-art pretraining technique. Given the input token sequence, a portion of tokens are substituted with a special symbol [MASK], and the model is trained to restore the original tokens from the altered version. Since density estimation is not an objective, BERT can use bidirectional contexts for reconstruction.","In contrast, AE based pretraining does not explicitly learn the data distribution but instead reconstructs the original data from corrupted input. A prominent case is BERT [10], which has been the top performing pretraining approach. In the input token sequence, some tokens are replaced with a special symbol [MASK], and the model is optimized to predict the original tokens from the modified version. As density modeling is not a goal, BERT can leverage bidirectional contexts for reconstruction.",A,1
895,Deep Residual Learning for Image Recognition,"When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as reported in [11, 42] and thoroughly verified by our experiments. Fig. 1 shows a typical example. The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize.","As more complex neural networks start to converge, an issue emerges: as network depth increases, performance plateaus (which is predictable) and then worsens quickly. Surprisingly, this decline is not due to overfitting, and appending more layers to an adequately deep model raises training error, as described in [11, 42] and confirmed by our tests. Fig. 1 displays a typical case. The degradation (of training accuracy) implies that not all systems are equally simple to enhance.","When more sophisticated neural networks begin to converge, a problem surfaces: as the network gets deeper, accuracy stabilizes (which is foreseeable) then rapidly deteriorates. Counterintuitively, this decrease is not from overfitting, and attaching extra layers to a sufficiently deep model increases training mistake, as noted in [11, 42] and thoroughly corroborated by our trials. Fig. 1 exhibits a typical illustration. The degradation (of training precision) signifies that not all systems are similarly straightforward to optimize.","As more elaborate neural networks start converging, a drawback materializes: as network depth expands, performance saturates (which is expected) then quickly worsens. Strangely, this decline is not due to overfitting, and adding more layers to an adequately deep model raises training error, as documented in [11, 42] and thoroughly confirmed by our experiments. Fig. 1 displays a typical case. The degradation (of training accuracy) indicates that not all systems are equally easy to enhance.",A,1
1098,Going deeper with convolutions,"We propose a deep convolutional neural network architecture codenamed Inception, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing.","We put forward a deep convolutional neural network design called Inception, which set a new benchmark for categorization and identification in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The primary characteristic of this design is the enhanced use of the computing capabilities inside the network. This was realized through a meticulously developed architecture that enables increasing the depth and breadth of the network while maintaining a constant computational budget. To enhance quality, the architectural choices were grounded in the Hebbian theory and the concept of multi-scale processing.","We present a deep convolutional neural network model named Inception, which established the new state-of-the-art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge in 2014 (ILSVRC14). The principal hallmark of this model is the improved harnessing of the computing resources within the network. This was enabled by a thoughtfully engineered architecture that permits expanding the depth and width of the network while keeping the computational cost fixed. To optimize performance, the architectural decisions were based on the Hebbian principle and the notion of multi-scale processing.  ","We introduce a deep convolutional neural network design called Inception, which set the new benchmark for categorization and identification in the ImageNet Large-Scale Visual Recognition Challenge in 2014 (ILSVRC14). The major characteristic of this design is the enhanced utilization of the computing capabilities internal to the network. This was realized through a carefully conceived architecture that provides for increasing the depth and breadth of the network while maintaining the same computational budget. To enhance results, the architectural choices were founded on the Hebbian theory and the concept of multi-scale processing.",A,1
843,BERT,"Task #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-toright and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context. ","The first assignment: Masked language model. It makes sense to think that a deep two-way model has more capability than either a left-to-right model or a superficial joining of a left-to-right and a right-to-left model. However, standard conditional language models can only be trained left-to-right or right-to-left, since two-way conditioning would let each word indirectly ""see itself"", and the model could easily predict the target word in a multi-layered situation.","Task number one: Masked language modeling. Intuitively, it seems reasonable to assume that a deep bidirectional model has more power than either a left-to-right model or the shallow combination of a left-to-right and a right-to-left model. But typical conditional language models can only be trained left-to-right or right-to-left, because bidirectional conditioning would allow each word to indirectly ""view itself"", and the model could simply predict the target word in a multi-layer context.  ","The first task: Masked language model. It makes sense to think that a deep two-way model is more powerful than either a left-to-right model or a superficial combination of a left-to-right and a right-to-left model. However, standard conditional language models can only be trained left-to-right or right-to-left, since two-way conditioning would enable each word to indirectly ""see itself"", and the model could easily predict the target word in a multi-layer situation.",A,1
998,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"For simplicity, when fine-tuning we treat all of the tasks in the GLUE benchmark (and similarly for SuperGLUE) as a single task by concatenating all of the constituent data sets. As suggested by Kocijan et al. (2019) we also include the Definite Pronoun Resolution (DPR) data set (Rahman and Ng, 2012) in the combined SuperGLUE task. The CNN/Daily Mail (Hermann et al., 2015) data set was introduced as a questionanswering task but was adapted for text summarization by Nallapati et al. (2016); we use the non-anonymized version from See et al. (2017) as an abstractive summarization task. SQuAD (Rajpurkar et al., 2016) is a common question-answering benchmark. ","For simplicity, when fine-tuning we treat all the tasks in the GLUE benchmark (and similarly for SuperGLUE) as one task by joining together all the different data sets. As proposed by Kocijan et al. (2019) we also add the Definite Pronoun Resolution (DPR) data set (Rahman and Ng, 2012) to the combined SuperGLUE task. The CNN/Daily Mail (Hermann et al., 2015) data set was originally introduced as a question-answering task but was adapted for text summarization by Nallapati et al. (2016); we use the non-anonymized version from See et al. (2017) as an abstractive summarization task. SQuAD (Rajpurkar et al., 2016) is a well-known question-answering benchmark.","To simplify, when fine-tuning we treat all the tasks in the GLUE benchmark (and similarly for SuperGLUE) as one task by concatenating all the different data sets together. As suggested by Kocijan et al. (2019) we also incorporate the Definite Pronoun Resolution (DPR) data set (Rahman and Ng, 2012) into the combined SuperGLUE task. The CNN/Daily Mail (Hermann et al., 2015) data set was first introduced as a question-answering task but was adapted for text summarization by Nallapati et al. (2016); we use the non-anonymized version from See et al. (2017) as an abstractive summarization task. SQuAD (Rajpurkar et al., 2016) is a widely used question-answering benchmark.  ","For simplicity, when fine-tuning we treat all the tasks in the GLUE benchmark (and similarly for SuperGLUE) as one task by merging all the different data sets together. As recommended by Kocijan et al. (2019) we also include the Definite Pronoun Resolution (DPR) data set (Rahman and Ng, 2012) in the combined SuperGLUE task. The CNN/Daily Mail (Hermann et al., 2015) data set was first presented as a question-answering task but was modified for text summarization by Nallapati et al. (2016); we use the non-anonymized version from See et al. (2017) as an abstractive summarization task. SQuAD (Rajpurkar et al., 2016) is a common question-answering benchmark dataset.",A,1
1154,ImageNet A Large_Scale Hierarchical Image Database,"Compared to other available datasets, ImageNet provides image data in a densely populated hierarchical structure. Many possible algorithms could be applied to exploit a hierarchical data structure (e.g. [16, 17, 28, 18]). In this experiment, we choose to illustrate the usefulness of the ImageNet hierarchy by a simple object classification method which we call the “tree-max classifier”. Imagine you have a classifier at each synset node of the tree and you want to decide whether an image contains an object of that synset or not.","In contrast to other existing image datasets, ImageNet gives image information organized in a very detailed hierarchical way. There are many potential algorithms that could make use of this hierarchical organization of data (for instance [16, 17, 28, 18]). For this test, we decided to show the value of the ImageNet hierarchy using a straightforward object classification approach we refer to as the ""tree-max classifier"". Picture having a classifier at every node in the tree to determine if an image has an object of that node or not.","Compared with other available image datasets, ImageNet provides image data structured in a very dense hierarchical way. Numerous possible algorithms could leverage this hierarchical data structure (such as [16, 17, 28, 18]). For this experiment, we opted to demonstrate the usefulness of the ImageNet hierarchy through a simple object classification technique we call the ""tree-max classifier"". Envision having a classifier at each node in the hierarchy to classify if an image contains an object represented by that node. ","In comparison to other existing image datasets, ImageNet gives image data organized in a very populated hierarchical structure. There are many conceivable algorithms that could utilize this hierarchical data organization (for example [16, 17, 28, 18]). For this experiment, we chose to showcase the value of the ImageNet hierarchy by using a straightforward object classification approach we refer to as the ""tree-max classifier"". Picture having a classifier at every node in the hierarchy to determine if an image has an object represented by that node.",A,1
1381,Transformer-XL,"In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x<t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token. In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation. Given infinite memory and computation, a simple solution would be to process the entire context sequence using an unconditional Transformer decoder, similar to a feed-forward neural network.","In this work, we adhere to the conventional neural technique for modeling the conditional likelihood. Namely, a trainable neural network is utilized to encode the context x<t into a static sized latent state, which is multiplied with the word embeddings to get the logits. The logits are then provided to the Softmax function, yielding a categorical probability allocation over the next token. To be able to apply Transformer or self-attention to language modeling, the key issue is how to teach a Transformer to competently encode an arbitrarily long context into a fixed size depiction. Provided unrestricted memory and calculation, a straightforward solution would be to process the whole context sequence utilizing an unconditional Transformer decoder, akin to a feed-forward neural network.","In this work, we follow the standard neural methodology for modeling the conditional probability. In particular, a trainable neural network is employed to encode the context x<t into a constant size hidden representation, which is multiplied with the word vectors to obtain the logits. The logits are then input to the Softmax function, producing a categorical probability distribution over the next token. To apply Transformer or self-attention to language modeling, the central challenge is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation. With unlimited memory and computation, a simple solution would be to process the entire context sequence using an unconditional Transformer decoder, similar to a feedforward neural network.","In this work, we adhere to the conventional neural approach for modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x<t into a fixed dimension latent state, which is multiplied with the word embeddings to get the logits. The logits are then passed to the Softmax function, yielding a categorical probability distribution over the next token. To apply Transformer or self-attention to language modeling, the core problem is how to train a Transformer to efficiently encode an arbitrarily long context into a fixed size representation. Given infinite memory and computation, a straightforward solution would be to process the whole context sequence using an unconditional Transformer decoder, akin to a feedforward neural network.",A,1
1354,Sequence to Sequence Learning with Neural Networks,"Second, we found that deep LSTMs significantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found it extremely valuable to reverse the order of the words of the input sentence. So for example, instead of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ, where α, β, γ is the translation of a, b, c. This way, a is in close proximity to α, b is fairly close to β, and so on, a fact that makes it easy for SGD to “establish communication” between the input and the output. We found this simple data transformation to greatly improve the performance of the LSTM.","Secondly, we discovered that deep LSTMs significantly surpassed shallow LSTMs, so we opted for an LSTM with four tiers. Thirdly, we found it tremendously beneficial to invert the order of the words in the input sentence. Therefore instead of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is tasked with mapping c, b, a to α, β, γ, where α, β, γ is the translation of a, b, c. This way, a is close to α, b is fairly near to β, and so forth, a fact that makes it easy for stochastic gradient descent to ""build communication"" between the input and output. We found this simple data change greatly improved the LSTM's performance.","Next, we determined that deep LSTMs substantially outstripped shallow LSTMs, so we went with a 4-layer LSTM. Also, we realized it was extremely valuable to flip the order of the words in the input sentence. So rather than mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ, where α, β, γ is the translation of a, b, c. This way, a is near α, b is fairly close to β, and so on, which makes it easy for stochastic gradient descent to ""make connections"" between input and output. We found this simple data alteration greatly boosted the LSTM's capabilities.  ","Secondly, we ascertained that deep LSTMs significantly exceeded shallow LSTMs in performance, prompting our selection of a 4-tier LSTM architecture. Additionally, we determined that reversing the sequence of words in the input sentence was tremendously advantageous. Rather than mapping the sentence a, b, c to α, β, γ, the LSTM instead maps c, b, a to α, β, γ, where α, β, γ represents the translation of a, b, c. This juxtaposition of input and output facilitates stochastic gradient descent in forging representational links between them. Applying this basic data transformation substantially enhanced the capabilities of the LSTM.",A,1
1087,GloVe_Global Vectors for Word Representation,"Because all unsupervised methods for learning word vectors are ultimately based on the occurrence statistics of a corpus, there should be commonalities between the models. Nevertheless, certain models remain somewhat opaque in this regard, particularly the recent window-based methods like skip-gram and ivLBL. Therefore, in this subsection we show how these models are related to our proposed model, as defined in Eqn. (8). The starting point for the skip-gram or ivLBL methods is a model Qi j for the probability that word j appears in the context of word i.","Since all unsupervised techniques for learning word vectors stem from the occurrence data of a text collection, there should be similarities between the models. However, some models stay fairly unclear about this relationship, especially the latest window-based approaches such as skip-gram and ivLBL. Thus, in this part we illustrate how these models connect to our suggested model, as characterized in Eqn. (8). The basis for the skip-gram or ivLBL approaches is a model Qi j for the likelihood that word j shows up near word i.","Given that all unsupervised procedures for acquiring word vectors originate from the statistical patterns in a dataset, commonalities between the models should exist. But certain models continue to be quite vague about this connection, notably the recent context window methods such as skip-gram and ivLBL. Therefore, here we elucidate how these models relate to our proposed model, as formulated in Eqn. (8). The starting point for the skip-gram or ivLBL techniques is a model Qi j for the probability of word j appearing close to word i.","Since all unsupervised learning algorithms for word vectors stem from the occurrence statistics present in a text corpus, similarities between the models should arise. However, some models remain somewhat unclear about this relationship, especially the latest context window approaches like skip-gram and ivLBL. As such, we demonstrate in this section how these models link to our suggested model, as defined in Eqn. (8). The basis for the skip-gram or ivLBL methods is a model Qi j representing the probability of word j occurring in proximity to word i.",A,1
811,Attention is All You Need,"Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.","The data presented in Table 4 indicates that without any particular adjustment for the task, our model has unexpectedly strong performance, producing superior results to all other models reported except the Recurrent Neural Network Grammar [8]. Unlike RNN sequence-to-sequence models [37], the Transformer surpasses the BerkeleyParser [29] even when trained exclusively on the 40K sentence WSJ training set.","Our findings shown in Table 4 demonstrate that despite no task-specific fine-tuning, our system achieves remarkably good performance, generating better outcomes than all previously documented models excluding the Recurrent Neural Network Grammar [8]. In contrast with RNN sequence-to-sequence architectures [37], the Transformer beats the BerkeleyParser [29] even with training constrained to the 40K sentence WSJ training corpus.  ","The results in Table 4 exhibit that without any task-specific adaptation, our system attains unexpectedly robust performance, yielding superior results compared to all other formerly reported models besides the Recurrent Neural Network Grammar [8]. Unlike RNN sequence-to-sequence frameworks [37], the Transformer outperforms the BerkeleyParser [29] even when limited to training on the 40K sentence WSJ training set.",A,1
1078,GloVe_Global Vectors for Word Representation,"Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors. Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the entire corpus, which fails to take advantage of the vast amount of repetition in the data.","By testing on a word analogy challenge, these models showed they can learn language patterns as linear associations between the word vectors. In contrast to the matrix factorization techniques, the shallow window-based methods have the downside that they do not work directly with the co-occurrence data of the corpus. Rather, these models look through context windows across the whole corpus, which does not utilize the huge amount of repetition in the data.","Through assessment on a word analogy exercise, these models exhibited the ability to acquire linguistic patterns as linear links between the word vectors. Dissimilar to the matrix factorization approaches, the superficial window-based methods suffer from the weakness that they do not leverage the co-occurrence statistics of the corpus directly. Instead, these models scan context windows over the full corpus, which fails to capitalize on the vast repetition present in the data.  ","By evaluating on a word analogy evaluation, these models proved their capacity to learn language patterns as linear connections between the word vectors. Contrary to the matrix factorization techniques, the shallow window-based methods have the disadvantage that they do not harness the co-occurrence frequencies of the corpus straightaway. Rather, these models traverse context windows throughout the complete corpus, which neglects to exploit the tremendous repetition existent in the data.",A,1
1125,Going deeper with convolutions,"This enables adapting and fine-tuning our networks for other label sets easily, but it is mostly convenience and we do not expect it to have a major effect. It was found that a move from fully connected layers to average pooling improved the top-1 accuracy by about 0.6%, however the use of dropout remained essential even after removing the fully connected layers. Given the relatively large depth of the network, the ability to propagate gradients back through all the layers in an effective manner was a concern.","This makes it easy to adjust and refine our networks for other label groups without much effort, but we don't think it will have a big impact. We found that switching from fully connected layers to average pooling increased top-1 accuracy by around 0.6%. However, dropout was still crucial even after getting rid of the fully connected layers. With the network being quite deep, being able to pass gradients backward through all layers efficiently was a worry.","This allows us to simply tune and optimize our networks for other label sets, though it likely won't make a major difference. Replacing fully connected layers with average pooling was found to boost top-1 accuracy by 0.6% or so, but dropout remained key even minus the fully connected layers. Given how deep the network is, being able to propagate gradients back through all layers effectively was a concern. ","This lets us easily adapt and fine-tune our networks for other label groups, but we don't expect a big effect. We found top-1 accuracy rose about 0.6% from switching fully connected layers to average pooling, yet dropout was still vital without the fully connected layers. With the network's large depth, propagating gradients back through all layers efficiently was worrisome.",A,1
1243,Language Models are Unsupervised Multitask Learners,"We trained and benchmarked four LMs with approximately log-uniformly spaced sizes. The architectures are summarized in Table 2. The smallest model is equivalent to the original GPT, and the second smallest equivalent to the largest model from BERT (Devlin et al., 2018). Our largest model, which we call GPT-2, has over an order of magnitude more parameters than GPT. The learning rate of each model was manually tuned for the best perplexity on a 5% held-out sample of WebText. All models still underfit WebText and held-out perplexity has as of yet improved given more training time.","We educated and evaluated four language models with roughly log-uniformly distributed magnitudes. The designs are outlined in Table 2. The smallest model equals the first GPT, and the second smallest equals the biggest model from BERT (Devlin et al., 2018). Our most substantial model, which we term GPT-2, has over ten times more parameters than GPT. The learning pace of each model was manually adapted for the best perplexity on a 5% held-out exemplar of WebText. All models still inadequately fit WebText and held-out perplexity has up to now enhanced given more training time.","We trained and benchmarked four natural language processing models with approximately logarithmically evenly spaced sizes. The architectures are summarized in Table 2. The most diminutive model is the same as the original GPT, and the second smallest the same as the biggest model from BERT (Devlin et al., 2018). Our largest model, which we designate GPT-2, has over ten times more parameters than GPT. The learning velocity of each model was manually calibrated for the optimal perplexity on a 5% retained sample of WebText. All models still insufficiently fit WebText and retained perplexity has so far gotten better given additional training time.  ","We educated and evaluated four natural language models with roughly logarithmically equally allocated magnitudes. The designs are outlined in Table 2. The most minute model equals the inaugural GPT, and the second smallest equals the most substantial model from BERT (Devlin et al., 2018). Our most sizable model, which we entitle GPT-2, has over an order of magnitude more parameters than GPT. The learning pace of each model was manually tuned for the best perplexity on a 5% withheld exemplar of WebText. All models still inadequately accommodate WebText and withheld perplexity has hitherto enhanced given supplementary training time.",A,1
1133,ImageNet A Large_Scale Hierarchical Image Database,"ImageNet, therefore, will offer tens of millions of cleanly sorted images. In this paper, we report the current version of ImageNet, consisting of 12 “subtrees”: mammal, bird, fish, reptile, amphibian, vehicle, furniture, musical instrument, geological formation, tool, flower, fruit. These subtrees contain 5247 synsets and 3.2 million images. Fig. 1 shows a snapshot of two branches of the mammal and vehicle subtrees The rest of the paper is organized as follows: We first show that ImageNet is a large-scale, accurate and diverse image database (Section 2).","ImageNet provides many millions of well-organized images. This report describes the current form of ImageNet, with 12 categories: mammals, birds, fish, reptiles, amphibians, vehicles, furniture, musical instruments, geological formations, tools, flowers, and fruits. These categories have 5247 concepts and 3.2 million images. Figure 1 displays parts of the mammal and vehicle categories. The rest of the report covers how ImageNet is a big, precise, and varied image database (Section 2).","ImageNet offers a great number of neatly classified images. This article presents the existing version of ImageNet, made up of 12 ""subcategories"": mammals, birds, fish, reptiles, amphibians, vehicles, furniture, musical instruments, geological formations, tools, flowers, fruits. These subcategories contain 5247 concepts and 3.2 million images. Figure 1 shows two branches of the mammal and vehicle subcategories. The rest of the article is structured as follows: We first demonstrate that ImageNet is a large-scale, accurate and diverse image database (Section 2).","ImageNet provides many millions of well-organized images. This paper describes the current iteration of ImageNet, comprising 12 ""subtrees"": mammals, birds, fish, reptiles, amphibians, vehicles, furniture, musical instruments, geological formations, tools, flowers, fruits. These subtrees contain 5247 concepts and 3.2 million images. Figure 1 displays a sample of two branches of the mammal and vehicle subtrees. The remainder of the paper is structured thus: We first illustrate that ImageNet is a large-scale, precise, and varied image database (Section 2).",A,1
1274,Neural Machine Translation by Jointly Learning To Align and Translate,"The conventional approach to neural machine translation, called an encoder–decoder approach, encodes a whole input sentence into a fixed-length vector from which a translation will be decoded. We conjectured that the use of a fixed-length context vector is problematic for translating long sentences, based on a recent empirical study reported by Cho et al. (2014b) and Pouget-Abadie et al. (2014). In this paper, we proposed a novel architecture that addresses this issue. We extended the basic encoder–decoder by letting a model (soft-)search for a set of input words, or their annotations computed by an encoder, when generating each target word.","The standard way of doing neural machine translation, known as encoder-decoder, turns an entire input sentence into a vector of fixed length that a translation will be created from. We hypothesized that using a fixed-length context vector causes issues for translating long sentences, based on a recent study by Cho et al. (2014b) and Pouget-Abadie et al. (2014). In this paper, we suggested a new architecture that solves this problem. We expanded the basic encoder-decoder by letting a model (softly) look for a set of input words, or their encodings generated by an encoder, when producing each target word.","The typical approach to neural machine translation, called encoder-decoder, encodes an entire input sentence into a static-length vector that a translation will be generated from. We theorized that utilizing a static-length context vector is problematic for translating lengthy sentences, based on a recent empirical study by Cho et al. (2014b) and Pouget-Abadie et al. (2014). In this paper, we proposed an innovative architecture that resolves this issue. We extended the fundamental encoder-decoder by enabling a model to (softly) search for a set of input words, or their representations computed by an encoder, when creating each target word.","The conventional technique for neural machine translation, known as encoder-decoder, transforms an entire input sentence into a fixed-size vector that a translation will be derived from. We conjectured that employing a fixed-size context vector causes difficulties for translating long sentences, based on a recent empirical study by Cho et al. (2014b) and Pouget-Abadie et al. (2014). In this paper, we put forth a novel architecture that addresses this problem. We augmented the basic encoder-decoder by allowing a model to (softly) look for a set of input words, or their encodings generated by an encoder, when producing each target word.",A,1
897,Deep Residual Learning for Image Recognition,"We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. The formulation of F(x) +x can be realized by feedforward neural networks with “shortcut connections” (Fig. 2). Shortcut connections [2, 34, 49] are those skipping one or more layers. In our case, the shortcut connections simply perform identity mapping, and their outputs are added to the outputs of the stacked layers (Fig. 2).","Our theory is that optimizing the residual function is simpler than optimizing the original unmodified function. In an extreme case, if the identity function was optimal, it would be easier to make the residual zero than to fit an identity function using nonlinear layers. The F(x)+x formulation can be implemented in neural networks by adding ""shortcut connections"" that skip one or more layers. Here, the shortcuts just do an identity mapping, and their outputs are summed with the outputs of the stacked layers.","We propose that tuning the residual mapping is less difficult than tuning the original unreferenced mapping. As an extreme example, if the identity map was ideal, it would be simpler to reduce the residual to nil rather than fit an identity map using a stack of nonlinear layers. The F(x)+x formulation can be created with feedforward neural networks containing ""shortcut links"" that miss one or more layers. In our case, the shortcuts just execute identity mapping, and their outputs are combined with the outputs of the stacked layers.  ","Our conjecture is that adjusting the residual function is easier than adjusting the original raw function. In the extreme scenario where the identity function is optimal, it would be less complex to make the residual zero than to model an identity function with nonlinear layers. The F(x)+x structure can be implemented in neural networks through ""shortcut connections"" that bypass one or more layers. Here, the shortcuts simply perform identity mapping, and their outputs are added to the outputs of the stacked layers.",A,1
869,Deep contextualized word representations,"In this paper, we take full advantage of access to plentiful monolingual data, and train our biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014). We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2- layer LSTM encoder are better at predicting POS tags then second layer. ","In this document, we make the most of access to abundant single-language information, and teach our biLM on a collection containing around 30 million sentences (Chelba et al., 2014). We also generalize these methodologies to profound contextual representations, which we demonstrate work admirably over an extensive scope of different NLP errands. For instance, presenting multi-task syntactic supervision (e.g., part-of-discourse labels) at the lower levels of a profound LSTM can improve by and large execution of higher level undertakings like reliance parsing (Hashimoto et al., 2017) or CCG super labeling (Søgaard and Goldberg, 2016). In a RNN-based encoder-decoder machine interpretation framework, Belinkov et al. (2017) demonstrated that the representations learned at the initial layer in a 2-layer LSTM encoder are better at anticipating POS labels then the second layer.","In this article, we capitalize on access to abundant single-language data, and educate our biLM on a corpus containing approximately 30 million sentences (Chelba et al., 2014). We additionally generalize these techniques to profound contextual representations, which we exhibit work admirably over a wide scope of various NLP errands. For instance, presenting multi-task syntactic supervision (e.g., part-of-discourse labels) at the lower levels of a profound LSTM can improve generally execution of higher level undertakings like reliance parsing (Hashimoto et al., 2017) or CCG super marking (Søgaard and Goldberg, 2016). In a RNN-based encoder-decoder machine interpretation framework, Belinkov et al. (2017) showed that the representations learned at the initial layer in a 2-layer LSTM encoder are better at anticipating POS labels then the second layer.","In this paper, we take full advantage of access to abundant single-language data, and train our biLM on a corpus containing around 30 million sentences (Chelba et al., 2014). We also generalize these techniques to deep contextual representations, which we demonstrate work well across a wide range of diverse NLP tasks. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks like dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016). In a RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2-layer LSTM encoder are better at predicting POS tags than the second layer.",A,1
838,BERT,"ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al.","ELMo and its forerunner (Peters et al., 2017, 2018a) generalize conventional word embedding research along a different axis. They derive context-sensitive characteristics from a left-to-right and a right-to-left language archetype. The contextual illustration of each token is the fusion of the left-to-right and right-to-left portrayals. When combining contextual word embeddings with existing task-explicit architectures, ELMo propels the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question solving (Rajpurkar et al., 2016), sentiment examination (Socher et al., 2013), and named entity identification (Tjong Kim Sang and De Meulder, 2003). Melamud et al.","ELMo and its precursor (Peters et al., 2017, 2018a) broaden traditional word embedding work in a different way. They extract context-sensitive features from a left-to-right and a right-to-left linguistic model. The contextual representation of each token is the combination of the left-to-right and right-to-left representations. When integrating contextual word embeddings into existing task-focused architectures, ELMo improves the state-of-the-art for several major NLP benchmarks (Peters et al., 2018a) including question replying (Rajpurkar et al., 2016), sentiment review (Socher et al., 2013), and named entity spotting (Tjong Kim Sang and De Meulder, 2003). Melamud et al.  ","ELMo and its predecessor (Peters et al., 2017, 2018a) expand conventional word embedding research in a different dimension. They derive context-sensitive traits from a left-to-right and a right-to-left language prototype. The contextual depiction of each token is the union of the left-to-right and right-to-left portrayals. When combining contextual word embeddings with current task-explicit architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al.",A,1
1230,Language Models are Unsupervised Multitask Learners,"Learning to perform a single task can be expressed in a probabilistic framework as estimating a conditional distribution p(output|input). Since a general system should be able to perform many different tasks, even for the same input, it should condition not only on the input but also on the task to be performed. That is, it should model p(output|input, task). This has been variously formalized in multitask and meta-learning settings. Task conditioning is often implemented at an architectural level, such as the task specific encoders and decoders in (Kaiser et al., 2017) or at an algorithmic level such as the inner and outer loop optimization framework of MAML (Finn et al., 2017).","Acquiring the skills to carry out a solitary chore can be depicted using probability theory as approximating a conditional distribution p(result|data). Since a universal structure ought to be capable of executing numerous distinct chores, even for the same input, it should be conditional not solely on the input but also on the chore to be executed. That is, it should exemplify p(result|data, chore). This has been articulated in various ways in multitask and meta-learning contexts. Chore conditioning is frequently actualized at an architectural level, like the chore particular encoders and decoders in (Kaiser et al., 2017) or at an algorithmic level like the inner and outer loop enhancement structure of MAML (Finn et al., 2017).","Learning how to complete a single job can be modeled mathematically as estimating a conditional probability p(output|input). Because a general-purpose system should be able to perform many different jobs, even for the same input, it should take into account not just the input but also the specific job to be done. In other words, it should model p(output|input, job). This idea has been formalized in various ways in multi-task and meta-learning settings. Conditioning on the job is often implemented architecturally, as with the job-specific encoders and decoders in (Kaiser et al., 2017), or algorithmically, as with the inner and outer loop optimization approach of MAML (Finn et al., 2017).","Grasping how to execute a solitary assignment can be depicted probabilistically as approximating a conditional distribution p(consequence|information). Since an all-purpose structure ought to have the capacity to play out various distinctive assignments, even for a similar information, it ought to condition not just on the information yet in addition on the assignment to be performed. That is, it ought to demonstrate p(consequence|information, assignment). This has been formalized in various ways in multitask and meta-learning situations. Task conditioning is frequently executed designally, for example, the assignment particular encoders and decoders in (Kaiser et al., 2017) or algorithmically, for example, the internal and outer circle enhancement structure of MAML (Finn et al., 2017).",A,1
1484,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"Finally, we compare our results with the state of the art in Table 7. In the classification task of ILSVRC-2014 challenge (Russakovsky et al., 2014), our “VGG” team secured the 2nd place with 7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models. As can be seen from Table 7, our very deep ConvNets significantly outperform the previous generation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competitions. Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it.","In conclusion, we make a comparison of our findings with the current state of the art in Table 7. In the image classification challenge of the ILSVRC-2014 competition (Russakovsky et al., 2014), our ""VGG"" team got 2nd place with a 7.3% test error rate using an ensemble of 7 models. After submitting, we reduced the error rate to 6.8% using just 2 models. As evident in Table 7, our very deep Convolutional Networks significantly surpass the previous generation of models, which had the top results in the ILSVRC-2012 and ILSVRC-2013 contests. Our result is also competitive with the classification winner (GoogLeNet with 6.7% error) and substantially exceeds the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with external training data and 11.7% without it.","Finally, we make a comparison between our results and the current state of the art in Table 7. In the image classification task of the ILSVRC-2014 challenge (Russakovsky et al., 2014), our ""VGG"" team came in 2nd place with a 7.3% test error rate using an ensemble of 7 models. After submission, we lowered the error rate to 6.8% using just 2 models. As shown in Table 7, our very deep Convolutional Networks greatly surpass the previous generation of models, which had the best results in the ILSVRC-2012 and ILSVRC-2013 contests. Our result is also competitive with the image classification winner (GoogLeNet with 6.7% error) and substantially beats the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it.","In closing, we make a comparison of our results with the current state of the art in Table 7. In the image classification task of the ILSVRC-2014 challenge (Russakovsky et al., 2014), our ""VGG"" team took 2nd place with a 7.3% test error rate using an ensemble of 7 models. After submission, we reduced the error rate to 6.8% using just 2 models. As evident in Table 7, our very deep Convolutional Networks greatly exceed the previous generation of models, which had the top results in the ILSVRC-2012 and ILSVRC-2013 contests. Our result is also competitive with the classification winner (GoogLeNet with 6.7% error) and substantially beats the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with external training data and 11.7% without it.",A,1
1541,"You Only Look Once_Unified, Real-Time Object Detection","Fast and Faster R-CNN focus on speeding up the R-CNN framework by sharing computation and using neural networks to propose regions instead of Selective Search [14] [28]. While they offer speed and accuracy improvements over R-CNN, both still fall short of real-time performance. Many research efforts focus on speeding up the DPM pipeline [31] [38] [5]. They speed up HOG computation, use cascades, and push computation to GPUs. However, only 30Hz DPM [31] actually runs in real-time. Instead of trying to optimize individual components of a large detection pipeline, YOLO throws out the pipeline entirely and is fast by design.","The R-CNN models Fast R-CNN and Faster R-CNN aim to accelerate the R-CNN framework by sharing computations and utilizing neural networks rather than Selective Search to propose regions [14] [28]. Although they enhance speed and accuracy over R-CNN, both still fall short of real-time performance. Much research strives to expedite the DPM pipeline [31] [38] [5] by speeding up HOG calculation, employing cascades, and shifting computation to GPUs. However, only 30Hz DPM [31] truly operates in real-time. Rather than attempting to optimize individual parts of a large detection pipeline, YOLO discards the entire pipeline and is designed to be fast.","Fast R-CNN and Faster R-CNN focus on increasing the speed of R-CNN by sharing computations and using neural networks instead of Selective Search to propose regions [14] [28]. While they improve speed and accuracy compared to R-CNN, they still do not achieve real-time performance. Many efforts aim to accelerate the DPM pipeline [31] [38] [5] by speeding up HOG, using cascades, and moving computation to GPUs. But only 30Hz DPM [31] actually runs in real-time. Instead of optimizing individual components of a complex detection pipeline, YOLO abandons the entire pipeline and is inherently fast. ","Fast R-CNN and Faster R-CNN concentrate on boosting the speed of R-CNN through shared computation and neural network region proposals instead of Selective Search [14] [28]. Although they enhance speed and accuracy over R-CNN, they still fall short of real-time capability. Numerous works focus on expediting the DPM pipeline [31] [38] [5] via faster HOG, cascades, and GPU computation. However, only 30Hz DPM [31] truly achieves real-time operation. Rather than optimizing individual parts of a large detection pipeline, YOLO eschews the entire pipeline and is designed for speed.",A,1
985,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"We empirically explore these architectural variants in Section 3.2. Overall, our encoder-decoder Transformer implementation closely follows its originallyproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent. ","We experimentally investigate these architectural variations in Section 3.2. In general, our encoder-decoder Transformer realization closely adheres to its originally suggested form (Vaswani et al., 2017). Initially, an input sequence of tokens is converted to a sequence of embeddings, which is then inputted into the encoder. The encoder is comprised of a pile of ""blocks"", each containing two subparts: a self-attention layer followed by a small feedforward network. Layer normalization (Ba et al., 2016) is applied to the input of each subpart.","We empirically analyze these architectural options in Section 3.2. On the whole, our encoder-decoder Transformer execution closely matches its originally described structure (Vaswani et al., 2017). To start, an input series of tokens is transformed into a series of embeddings, which is then fed into the encoder. The encoder consists of a stack of ""blocks"", each having two components: a self-attention layer followed by a small feedforward network. Layer normalization (Ba et al., 2016) is used on the input of each component.  ","We experimentally examine these architectural configurations in Section 3.2. In summary, our encoder-decoder Transformer implementation closely adheres to its originally presented form (Vaswani et al., 2017). First off, an input sequence of tokens is converted into a sequence of embeddings, which is then entered into the encoder. The encoder is made up of a pile of ""blocks"", each containing two pieces: a self-attention layer followed by a small feedforward network. Layer normalization (Ba et al., 2016) is applied to the input of each piece.",A,1
1321,Sentence Embeddings using Siamese BERT-Networks,"These two options are also provided by the popular bert-as-a-service-repository3 . Up to our knowledge, there is so far no evaluation if these methods lead to useful sentence embeddings. Sentence embeddings are a well studied area with dozens of proposed methods. Skip-Thought (Kiros et al., 2015) trains an encoder-decoder architecture to predict the surrounding sentences. InferSent (Conneau et al., 2017) uses labeled data of the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the MultiGenre NLI dataset (Williams et al., 2018) to train a siamese BiLSTM network with max-pooling over the output.","These two choices are also given by the well-known bert-as-a-service-repository as well. As far as we know, there has not been any assessment done yet on whether these approaches generate useful sentence representations. Sentence representations are a thoroughly researched area with many proposed techniques. Skip-Thought (Kiros et al., 2015) teaches an encoder-decoder model to predict the nearby sentences. InferSent (Conneau et al., 2017) utilizes labeled data from the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the MultiGenre NLI dataset (Williams et al., 2018) to educate a siamese BiLSTM network with max-pooling over the output.","These two alternatives are also provided by the popular bert-as-a-service-repository too. To our understanding, there has not been any evaluation yet on if these ways lead to beneficial sentence vectors. Sentence vectors are a well studied field with many suggested approaches. Skip-Thought (Kiros et al., 2015) develops an encoder-decoder structure to foresee the surrounding sentences. InferSent (Conneau et al., 2017) employs labeled information from the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the MultiGenre NLI dataset (Williams et al., 2018) to develop a siamese BiLSTM network with max-pooling over the output.  ","These two options are also given by the well-known bert-as-a-service-repository as well. As far as we are aware, there is still no assessment if these techniques result in useful sentence embeddings. Sentence embeddings are an extensively studied area with many proposed methods. Skip-Thought (Kiros et al., 2015) builds an encoder-decoder model to predict the adjacent sentences. InferSent (Conneau et al., 2017) harnesses labeled data from the Stanford Natural Language Inference dataset (Bowman et al., 2015) and the MultiGenre NLI dataset (Williams et al., 2018) to construct a siamese BiLSTM network with max-pooling over the output.",A,1
1111,Going deeper with convolutions,"Also, non-uniform sparse models require more sophisticated engineering and computing infrastructure. Most current vision oriented machine learning systems utilize sparsity in the spatial domain just by the virtue of employing convolutions. However, convolutions are implemented as collections of dense connections to the patches in the earlier layer. ConvNets have traditionally used random and sparse connection tables in the feature dimensions since [11] in order to break the symmetry and improve learning, the trend changed back to full connections with [9] in order to better optimize parallel computing. The uniformity of the structure and a large number of filters and greater batch size allow for utilizing efficient dense computation.","Furthermore, irregular sparse models need more complex engineering and computing infrastructure. Most current vision focused machine learning systems use sparsity in the spatial domain simply by using convolutions. However, convolutions are actualized as groups of dense connections to the patches in the prior layer. ConvNets have historically utilized arbitrary and sparse connection tables in the feature dimensions since [11] to break the symmetry and get better learning, but the trend reverted back to full connections with [9] to better enhance parallel computing. The consistency of the structure and a large number of filters and greater batch size enable utilizing efficient dense computation.","In addition, non-consistent sparse models call for more sophisticated engineering and computing facilities. The majority of current vision oriented machine learning frameworks employ sparseness in the spatial domain just by employing convolutions. Though, convolutions are implemented as sets of dense links to the patches in the earlier layer. ConvNets have customarily utilized random and sparse connection tables in the feature dimensions since [11] in order to break the symmetry and enhance learning, however the trend changed back to full connections with [9] in order to better optimize parallel computing. The uniformity of the structure and a substantial number of filters and greater batch size permit utilizing efficient dense computation.","Moreover, irregular sparse models require more complex engineering and computing infrastructure. Most current vision focused machine learning systems use sparseness in the spatial domain simply by utilizing convolutions. However, convolutions are materialized as collections of dense connections to the patches in the prior layer. ConvNets have historically employed arbitrary and sparse connection tables in the feature dimensions since [11] in order to break the symmetry and ameliorate learning, nevertheless the trend reverted back to full connections with [9] in order to better enhance parallel computing. The consistency of the structure and a significant number of filters and greater batch size enable leveraging efficient dense computation.",A,1
1331,Sentence Embeddings using Siamese BERT-Networks,"BERT is able to use attention to compare directly both sentences (e.g. word-by-word comparison), while SBERT must map individual sentences from an unseen topic to a vector space such that arguments with similar claims and reasons are close. This is a much more challenging task, which appears to require more than just two topics for training to work on-par with BERT.","BERT can utilize attention to directly contrast both sentences (for instance, contrasting each word), whereas SBERT has to change individual sentences from an unknown subject into a vector space so that contentions with comparable claims and rationales are near each other. This is a considerably more troublesome errand, which seems to require more than only two subjects for preparing to work similarly too BERT.","BERT can use attention to straightforwardly analyze both sentences word for word, while SBERT needs to change singular sentences from an obscure point into a vector space with the goal that conflicts with comparable cases and legitimizations are close. This is a significantly more troublesome assignment, which seems to require more than two subjects for preparing to perform similarly to BERT. ","BERT can straightforwardly look at both sentences word for word using attention, though SBERT needs to change singular sentences from a dark point into a vector space so debates with comparable cases and legitimizations are near one another. This is a significantly more troublesome task, which appears to require over two subjects for preparing to act comparably to BERT.",A,1
1116,Going deeper with convolutions,"We assume that each unit from the earlier layer corresponds to some region of the input image and these units are grouped into filter banks. In the lower layers (the ones close to the input) correlated units would concentrate in local regions. This means, we would end up with a lot of clusters concentrated in a single region and they can be covered by a layer of 1×1 convolutions in the next layer, as suggested in [12]. It also means that the suggested architecture is a combination of all those layers with their output filter banks concatenated into a single output vector forming the input of the next stage.","We suppose that each component from the previous layer matches to a particular area of the input image, and these components are assembled into filter banks. In the lower layers (the ones near the input), correlated components would focus in local zones. This signifies we would finish with numerous clusters focused in a single area that can be enclosed by a layer of 1×1 convolutions in the next layer, as proposed in [12]. It also denotes that the recommended architecture is a fusion of all those layers with their output filter banks joined into a single output vector constituting the input of the next phase.","We think that every unit from the prior stratum relates to a region of the input image, and these units are categorized into filter banks. In the lower strata (the ones adjacent to the input), correlated units would concentrate in local areas. This entails we would end up with many clusters focused in a single region that can be covered by a layer of 1×1 convolutions in the next stratum, as indicated in [12]. It also means the suggested architecture is a combination of all those strata with their output filter banks merged into a single output vector forming the input of the next stage.  ","We posit that each constituent from the earlier echelon corresponds to some precinct of the input image, and these constituents are assembled into filter banks. In the lower echelons (the ones proximate to the input), correlated constituents would amass in local precincts. This signifies we would conclude with copious clusters concentrated in a single precinct that can be encompassed by a layer of 1×1 convolutions in the next echelon, as proposed in [12]. It also denotes that the suggested architecture is an amalgamation of all those echelons with their output filter banks coalesced into a single output vector constituting the input of the next phase.",A,1
926,Deep Residual Learning for Image Recognition,"We argue that this is because the zero-padded dimensions in A indeed have no residual learning. C is marginally better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts. But the small differences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem. So we do not use option C in the rest of this paper, to reduce memory/time complexity and model sizes. Identity shortcuts are particularly important for not increasing the complexity of the bottleneck architectures that are introduced below.","We contend that this is due to the zero-padded dimensions in A truly not having any residual learning. C is slightly superior to B, and we credit this to the extra parameters introduced by numerous (thirteen) projection shortcuts. However, the small variances between A/B/C signify that projection shortcuts are not imperative for tackling the degradation issue. Thus, we do not utilize option C for the remainder of this paper, to decrease memory/time intricacy and model magnitudes. Identity shortcuts are especially vital for not amplifying the complexity of the bottleneck architectures presented below.","Our position is that the reason for this is that the zero-padded dimensions in A genuinely have no leftover learning. C is marginally preferable to B, and we attribute this to the additional parameters created by many (thirteen) projection shortcuts. But the minor differences between A/B/C indicate that projection shortcuts are not essential for addressing the degradation dilemma. Therefore, we do not employ option C for the rest of this paper, to reduce memory/time complexity and model sizes. Identity shortcuts are particularly important for not increasing the intricacy of the bottleneck architectures introduced later.  ","Our stance is that the rationale for this is that the zero-padded dimensions in A truly possess no residual learning. C is slightly better than B, and we ascribe this to the extra parameters generated by numerous (thirteen) projection shortcuts. However, the small variances between A/B/C signify that projection shortcuts are not imperative for tackling the degradation predicament. As such, we do not use option C for the remainder of this paper, to decrease memory/time complexity and model magnitudes. Identity shortcuts are especially critical for not amplifying the complexity of the bottleneck architectures presented subsequently.",A,1
1089,GloVe_Global Vectors for Word Representation,"One could interpret this objective as a “global skip-gram” model, and it might be interesting to investigate further. On the other hand, Eqn. (13) exhibits a number of undesirable properties that ought to be addressed before adopting it as a model for learning word vectors. To begin, cross entropy error is just one among many possible distance measures between probability distributions, and it has the unfortunate property that distributions with long tails are often modeled poorly with too much weight given to the unlikely events. Furthermore, for the measure to be bounded it requires that the model distribution Q be properly normalized.","This goal could be seen as a ""universal skip-gram"" model, and it may be worthwhile to explore it more. However, Equation 13 shows several problematic characteristics that should be dealt with before using it to learn word vectors. To start, cross entropy error is only one of many potential distance metrics between probability distributions, and it unfortunately tends to model distributions with long tails poorly by giving too much importance to rare events. Also, for the measure to have an upper bound, it necessitates that the model distribution Q be correctly normalized.","One interpretation of this aim is a ""broad skip-gram"" model, and further investigation into it could be interesting. But Equation 13 has a number of unfavorable properties that need addressing before adopting it to learn word vectors. For one, cross entropy error is just one of multiple possible distance measures between probability distributions, and it has the bad tendency of poorly modeling distributions with long tails by assigning too much weight to unlikely events. In addition, for the measure to have a finite bound, it requires the model distribution Q to be properly normalized.","You could view this goal as a ""universal skip-gram"" model, and exploring it further might be valuable. However, Equation 13 displays several undesirable characteristics that should be resolved before using it as a model to learn word vectors. To start, cross entropy error is only one of many potential distance metrics between probability distributions, and unfortunately it tends to poorly model distributions with long tails by assigning too much significance to rare events. Also, for the measure to have a finite upper limit, it necessitates that the model distribution Q be appropriately normalized.",A,1
1077,GloVe_Global Vectors for Word Representation,"Recently, the importance of the full neural network structure for learning useful word representations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric. In the skip-gram and ivLBL models, the objective is to predict a word’s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context.","The significance of the complete neural network design for acquiring beneficial word representations was recently questioned. The skip-gram and continuous bag-of-words (CBOW) architectures from Mikolov et al. (2013a) put forth a basic single-layer structure founded on the inner product of two word vectors. Mnih and Kavukcuoglu (2013) also presented strongly linked vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric. In the skip-gram and ivLBL models, the goal is to predict a word's context given the word itself, while the goal in the CBOW and vLBL models is to predict a word given its context.","Recently, doubts have emerged about how important the full neural network architecture is for learning useful word representations. The skip-gram and continuous bag-of-words (CBOW) models proposed by Mikolov et al. (2013a) have a simple single-layer design based on the inner product of two word vectors. Related vector log-bilinear models called vLBL and ivLBL were also proposed by Mnih and Kavukcuoglu (2013), and Levy et al. (2014) proposed explicit word embeddings using a PPMI metric. The objective in the skip-gram and ivLBL models is to predict a word's context from the word itself, while the objective in the CBOW and vLBL models is to predict a word from its context.","The necessity of the complete neural network structure for acquiring beneficial word representations has recently come into question. The skip-gram and continuous bag-of-words (CBOW) models from Mikolov et al. (2013a) employ a basic single-layer architecture utilizing the inner product of two word vectors. Highly related vector log-bilinear models called vLBL and ivLBL were also put forth by Mnih and Kavukcuoglu (2013), and explicit word embeddings based on a PPMI metric were proposed by Levy et al. (2014). The skip-gram and ivLBL models aim to predict a word's context from the word itself, while the CBOW and vLBL models aim to predict a word from its context.",A,1
805,Attention is All You Need,"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).","Our models were educated on a single computer with 8 NVIDIA P100 graphics processing units. Using the parameter settings described in the paper, each iteration of training took around 0.4 seconds for our foundational models. We trained the foundational models for 100,000 iterations total, which took 12 hours. For our large models (detailed on the last line of table 3), each iteration took 1.0 seconds. We trained the large models for 300,000 iterations (3.5 days).","We taught our algorithms on a single device equipped with 8 NVIDIA P100 GPUs. With the hyperparameters outlined in the document, every round of learning required about 0.4 seconds for our baseline models. The baseline models were educated over 100,000 rounds or 12 hours total. For our expansive models (depicted on the final row of table 3), each round lasted 1.0 seconds. We trained the expansive models for 300,000 rounds (3.5 days).  ","Our algorithms were developed on a single processor containing 8 NVIDIA P100 graphics chips. Utilizing the settings described in the article, each training pass took around 0.4 seconds for our elementary models. The elementary models were trained for 100,000 passes or 12 hours altogether. For our large-scale models (portrayed on the ending line of table 3), each pass took 1.0 seconds. We trained the large-scale models for 300,000 passes (3.5 days).",A,1
1345,Sequence to Sequence Learning with Neural Networks,"In this paper, we show that a straightforward application of the Long Short-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems. The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed dimensional vector representation, and then to use another LSTM to extract the output sequence from that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model [28, 23, 30] except that it is conditioned on the input sequence.","This paper demonstrates that a simple implementation of the Long Short-Term Memory (LSTM) model [16] is capable of handling general sequence-to-sequence tasks. The approach utilizes one LSTM to process the input sequence incrementally to generate a large fixed-size vector representation. Then a second LSTM is used to decode the output sequence from that vector (fig. 1). The second LSTM is basically a recurrent neural network language model [28, 23, 30] that is conditioned on the input sequence.","In this work, we establish that directly applying the Long Short-Term Memory (LSTM) framework [16] enables solving generic sequence-to-sequence problems. The methodology employs one LSTM for reading in the input sequence step-by-step to create a high-dimensional fixed-size vector embedding. Afterwards, another LSTM is leveraged for producing the output sequence from said vector (fig. 1). This second LSTM resembles a recurrent neural network language model [28, 23, 30] apart from being dependent on the input sequence. ","This paper exhibits that a simple Long Short-Term Memory (LSTM) architecture [16] suffices to tackle general sequence-to-sequence tasks. The technique feeds an input sequence into one LSTM incrementally to form a large static vector representation. Then a second LSTM takes this vector and decodes an output sequence (fig. 1). This second LSTM resembles a recurrent neural network language model [28, 23, 30] conditioned on the input sequence.",A,1
927,Deep Residual Learning for Image Recognition,"Next we describe our deeper nets for ImageNet. Because of concerns on the training time that we can afford, we modify the building block as a bottleneck design4 . For each residual function F, we use a stack of 3 layers instead of 2 (Fig. 5). The three layers are 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3×3 layer a bottleneck with smaller input/output dimensions. Fig. 5 shows an example, where both designs have similar time complexity.","In the following section we present the more complex neural networks we used for ImageNet. To address worries about the training time we could dedicate, we altered the architecture to a bottleneck design with 3 layers rather than 2 in each residual function F. The layers are 1x1, 3x3, and 1x1 convolutions, where the 1x1 layers decrease and then restore the dimensions, making the 3x3 layer a bottleneck with smaller input/output sizes. Figure 5 illustrates an example where both architectures have comparable time complexity.","Next we explain the more sophisticated neural networks we leveraged for ImageNet. Due to concerns about the training time available, we tweaked the building block to a bottleneck style with 3 layers instead of 2 in each residual function F. The layers are 1x1, 3x3, and 1x1 convolutions, where the 1x1 layers reduce and then expand the dimensions, causing the 3x3 layer to become a bottleneck with smaller input/output dimensions. Figure 5 provides an example, where both designs have similar time complexity.  ","In the following we describe the deeper neural networks we used for ImageNet. Because of worries about the training time we could spend, we altered the module to a bottleneck architecture with 3 layers rather than 2 in each residual function F. The layers are 1x1, 3x3, and 1x1 convolutions, where the 1x1 layers decrease and then increase the dimensions, making the 3x3 layer a bottleneck with smaller input/output sizes. Figure 5 shows an example, where both architectures have comparable time complexity.",A,1
1182,ImageNet Classification with Deep Convolutional Neural Networks,"Combining the predictions of many different models is a very successful way to reduce test errors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in backpropagation.","Putting together the forecasts from numerous separate models has proven very effective for decreasing errors on exams [1, 3], but this seems prohibitively costly for large neural networks that already require multiple days of training. Fortunately, there is a very efficient form of model combination that only increases training time by around a factor of two. This recently presented approach, termed ""dropout"" [10], works by randomly setting the output of each hidden neuron to zero with 0.5 probability. The neurons that are ""dropped out"" in this manner do not add to the forward pass and are not involved in backpropagation.","Integrating the projections from many distinct models has been shown to be a very useful way to lower mistakes on tests [1, 3], however this appears too expensive for big neural networks which already need several days to train. Luckily, there is a very cost-effective version of model integration that only raises training time by about twice as much. This newly introduced method, called ""dropout"" [10], functions by randomly setting the output of every hidden neuron to zero with a 0.5 probability. The neurons that are ""dropped out"" in this way do not contribute to the forward pass and are excluded from backpropagation.","Combining the anticipations from numerous different models has proven very effectual for reducing inaccuracies on evaluations [1, 3], nevertheless this seems too costly for large neural networks that already necessitate multiple days of training. Fortunately, there is a very efficient form of model amalgamation that only enlarges training time by around twice as much. This recently presented technique, termed ""dropout"" [10], operates by arbitrarily setting the output of each hidden neuron to zero with a 0.5 probability. The neurons that are ""dropped out"" in this fashion do not add to the forward pass and are precluded from backpropagation.",A,1
1232,Language Models are Unsupervised Multitask Learners,"Language modeling is also able to, in principle, learn the tasks of McCann et al. (2018) without the need for explicit supervision of which symbols are the outputs to be predicted. Since the supervised objective is the same as the unsupervised objective but only evaluated on a subset of the sequence, the global minimum of the unsupervised objective is also the global minimum of the supervised objective. In this slightly toy setting, the concerns with density estimation as a principled training objective discussed in (Sutskever et al., 2015) are side stepped. The problem instead becomes whether we are able to, in practice, optimize the unsupervised objective to convergence.","Language models can theoretically learn the tasks of McCann et al. (2018) without needing explicit guidance on which symbols to predict as outputs. Since the supervised and unsupervised goals are identical except the supervised evaluates a sequence subset, the unsupervised minimum is the supervised minimum too. In this somewhat simplistic setting, issues with density estimation as a principled training aim (Sutskever et al., 2015) are avoided. The issue becomes whether we can optimize the unsupervised aim fully in practice.","Language models are capable, in theory, of acquiring the tasks of McCann et al. (2018) with no explicit teaching of which symbols are the outputs to be foreseen. Because the supervised purpose equals the unsupervised purpose but is only appraised on a sequence portion, the global bottom of the unsupervised purpose is also the global bottom of the supervised purpose. In this somewhat basic setting, the concerns with density approximation as a principled training intention discussed in (Sutskever et al., 2015) are bypassed. The problem instead becomes whether we can, in practice, enhance the unsupervised intention to completion. ","Language models can learn the tasks of McCann et al. (2018) in principle without needing clear instruction on which symbols are the outputs to be predicted. Since the supervised goal matches the unsupervised goal but is only measured on part of the sequence, the overall minimum of the unsupervised goal also minimizes the supervised goal. In this somewhat simplified case, issues with density estimation as a sound training aim (Sutskever et al., 2015) are avoided. The question becomes whether we can fully optimize the unsupervised aim in practice.",A,1
1455,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale image and video recognition (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014) which has become possible due to the large public image repositories, such as ImageNet (Deng et al., 2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al., 2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2014), which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin et al., 2010) (the winner of ILSVRC-2011) to deep ConvNets (Krizhevsky et al., 2012) (the winner of ILSVRC-2012).","Recently, convolutional neural networks (ConvNets) have achieved great success in large-scale image and video recognition tasks (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014). This breakthrough has become feasible thanks to the availability of large public image datasets like ImageNet (Deng et al., 2009), and high-performance computing systems like GPUs or large distributed clusters (Dean et al., 2012). Specifically, the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2014) has played an important role in advancing deep visual recognition architectures. ILSVRC has served as a testbed for several generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin et al., 2010) (the ILSVRC-2011 winner) to deep ConvNets (Krizhevsky et al., 2012) (the ILSVRC-2012 winner).","In recent years, convolutional neural networks (ConvNets) have achieved tremendous success in large-scale image and video recognition tasks (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014). This has become feasible due to the availability of large public image datasets like ImageNet (Deng et al., 2009), and high-performance computing systems like GPUs or large distributed clusters (Dean et al., 2012). In particular, the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2014) has played a crucial role in advancing deep visual recognition architectures. ILSVRC has served as a testing ground for several generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin et al., 2010) (the ILSVRC-2011 winner) to deep ConvNets (Krizhevsky et al., 2012) (the ILSVRC-2012 winner).  ","In recent times, convolutional neural networks (ConvNets) have achieved immense success in large-scale image and video recognition tasks (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014). This has been made possible due to the availability of large public image datasets such as ImageNet (Deng et al., 2009), and high-performance computing systems like GPUs or large distributed clusters (Dean et al., 2012). Specifically, the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2014) has played a pivotal role in advancing deep visual recognition architectures. ILSVRC has served as an experimental platform for several generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin et al., 2010) (the ILSVRC-2011 winner) to deep ConvNets (Krizhevsky et al., 2012) (the ILSVRC-2012 winner).",A,1
870,Deep contextualized word representations,"Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision. Dai and Le (2015) and Ramachandran et al. (2017) pretrain encoder-decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision. ","In conclusion, the highest layer of an LSTM used for encoding word context (Melamud et al., 2016) has demonstrated an ability to learn representations of word sense. We illustrate that comparable signals are also created by the altered language model goal of our ELMo representations, and combining these different kinds of semi-supervision can be very advantageous for downstream task learning. Dai and Le (2015) and Ramachandran et al. (2017) pre-train encoder-decoder duos utilizing language models and sequence autoencoders followed by fine tuning with supervision particular to the task.","To summarize, the topmost layer of an LSTM utilized for encoding word context (Melamud et al., 2016) has shown an aptitude for learning representations of word sense. We exhibit that similar signals are also produced by the modified language modeling objective of our ELMo representations, and fusing these different semi-supervised techniques can be highly beneficial for learning downstream tasks. Dai and Le (2015) and Ramachandran et al. (2017) pre-train encoder-decoder pairs by using language models and sequence autoencoders then fine tune with task-specific supervision.","In closing, the highest layer of an LSTM used for encoding word context (Melamud et al., 2016) has demonstrated an ability to learn representations of word meaning. We show that comparable signals are also generated by the altered language model goal of our ELMo representations, and combining these different semi-supervised learnings can be very advantageous for learning downstream tasks. Dai and Le (2015) and Ramachandran et al. (2017) pre-train encoder-decoder teams using language models and sequence autoencoders then fine tune with supervision specific to the task.",A,1
848,BERT,"Table 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available, and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQuAD","The second table displays the highest ranked entries on the leaderboard as well as outcomes from the best previously published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top performing systems on the SQuAD leaderboard do not have current public descriptions of their systems available, and are permitted to utilize any public data when developing their systems. As a result, we implement minor data enhancement in our system by first tuning on TriviaQA (Joshi et al., 2017) before tuning on SQuAD.","Table number two exhibits the top ranked submissions on the leaderboard along with results from the most successful published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The highest performing entries on the SQuAD leaderboard do not have up-to-date public explanations of their systems accessible, and are allowed to employ any public information when constructing their systems. Therefore, we implement modest data expansion in our system by first adjusting on TriviaQA (Joshi et al., 2017) before adjusting on SQuAD.  ","The second table shows the highest scoring entries on the leaderboard and outcomes from the best previously released systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top performing submissions on the SQuAD leaderboard do not have current public descriptions of their systems available, and are permitted to use any public data when developing their systems. As a consequence, we implement minor data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) before fine-tuning on SQuAD.",A,1
1506,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"We perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. Specifically, there are three main aspects we hope to study: 1) The effectiveness of the permutation language modeling objective alone, especially compared to the denoising auto-encoding objective used by BERT. 2) The importance of using Transformer-XL as the backbone neural architecture. 3) The necessity of some implementation details including span-based prediction, the bidirectional input pipeline, and next-sentence prediction.","We conduct an analysis removing different components to comprehend the value of each design decision using four datasets with varying properties. In particular, there are three primary facets we aspire to examine: 1) The potency of the permutation language modeling goal by itself, especially compared to the denoising auto-encoding purpose utilized by BERT. 2) The significance of employing Transformer-XL as the backbone neural building block. 3) The need for some implementation information including span-based forecasting, the bidirectional input workflow, and next-sentence prediction.","We perform an exploratory analysis eliminating various elements to understand the importance of each design selection based on four datasets with diverse qualities. Specifically, there are three key aspects we hope to investigate: 1) The effectiveness of just the permutation language modeling aim, contrasted with the denoising auto-encoding goal used in BERT. 2) The importance of utilizing Transformer-XL as the backbone neural architecture. 3) The necessity of some implementation details like span-based projection, the bidirectional input system, and next-sentence prediction.  ","We do a study removing different parts to grasp the value of each design decision using four datasets with varying natures. In particular, there are three primary facets we want to examine: 1) The potency of solely the permutation language modeling purpose, especially compared to the denoising auto-encoding goal employed by BERT. 2) The significance of using Transformer-XL as the backbone neural structure. 3) The need for some implementation information including span-based forecasting, the bidirectional input workflow, and next-sentence prediction.",A,1
1255,Neural Machine Translation by Jointly Learning To Align and Translate,"Cho et al. (2014b) showed that indeed the performance of a basic encoder–decoder deteriorates rapidly as the length of an input sentence increases. In order to address this issue, we introduce an extension to the encoder–decoder model which learns to align and translate jointly. Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.","Cho and colleagues (2014b) demonstrated that the performance of a simple encoder-decoder model worsens quickly as the length of an input sentence grows. To address this problem, we present a modification to the encoder-decoder model that learns to align and translate together. Each time the proposed model produces a word in a translation, it performs a soft search to find positions in the source sentence where the most useful information is focused. The model then forecasts a target word based on the context vectors linked to those source positions and all previously generated target words.","The research by Cho et al. (2014b) showed that the capabilities of a basic encoder-decoder model deteriorate rapidly when the length of an input sentence is increased. To tackle this issue, we put forward an enhancement to the encoder-decoder model that concurrently learns how to align and translate. Whenever the suggested model outputs a word in a translation, it carries out a soft search to identify locations in a source sentence where the most relevant information is concentrated. The model then predicts a target word relying on the context vectors related to those source locations and all earlier produced target words.  ","The study by Cho and co-authors (2014b) demonstrated that the efficacy of a simple encoder-decoder architecture quickly declines as the length of an input sentence becomes longer. To address this problem, we introduce an augmentation to the encoder-decoder model that learns alignment and translation together. Each instance when the proposed model emits a word in a translation, it executes a soft search to pinpoint positions in a source sentence where the most pertinent information resides. The model then conjectures a target word based on the context vectors linked to those source positions and all previously spawned target words.",A,1
816,Bag of Tricks for Efficient Text Classification,"We evaluate the quality of our approach fastText1 on two different tasks, namely tag prediction and sentiment analysis. A simple and efficient baseline for sentence classification is to represent sentences as bag of words (BoW) and train a linear classifier, e.g., a logistic regression or an SVM (Joachims, 1998; Fan et al., 2008). However, linear classifiers do not share parameters among features and classes. This possibly limits their generalization in the context of large output space where some classes have very few examples.","We assess the performance of our fastText1 method on two different assignments - tag forecasting and sentiment review. A straightforward and capable foundation model for sentence sorting is to depict sentences as bag of words (BoW) and prepare a linear classifier, for example, a logistic regression or an SVM (Joachims, 1998; Fan et al., 2008). However, linear classifiers don't share parameters among features and classes. This possibly confines their generalization in the setting of large output space where some classes have very few examples.","We gauge the value of our fastText1 approach across two distinct tasks - tag prediction and sentiment analysis. A simple and effectual baseline technique for sentence classification is to model sentences as bag of words (BoW) and develop a linear classifier, like a logistic regression or SVM (Joachims, 1998; Fan et al., 2008). Though, linear classifiers don't share parameters between features and classes. This potentially limits their generalization in the context of a large output space where some classes have very few samples.  ","We measure the excellence of our fastText1 methodology on two separate jobs - tag forecasting and sentiment review. A basic and capable reference model for sentence sorting is to depict sentences as bag of words (BoW) and construct a linear classifier, such as a logistic regression or SVM (Joachims, 1998; Fan et al., 2008). However, linear classifiers don't share parameters among features and classes. This possibly constrains their generalization in the setting of a large output space where some classes have very sparse examples.",A,1
1053,Generative Adversarial Nets,"In predictability minimization, two networks’ outputs are compared, with one network trying to make the outputs similar and the other trying to make the outputs different. The output in question is a single scalar. In GANs, one network produces a rich, high dimensional vector that is used as the input to another network, and attempts to choose an input that the other network does not know how to process. 3) The specification of the learning process is different. Predictability minimization is described as an optimization problem with an objective function to be minimized, and learning approaches the minimum of the objective function.","In predictability minimization, two networks' outputs are analyzed, with one network attempting to make the outputs alike and the other trying to make the outputs distinct. The output being examined is a single scalar value. In GANs, one network generates a complex, high dimension vector that is utilized as the input to another network, and tries to select an input that the other network cannot handle. The definition of the learning procedure is different. Predictability minimization is depicted as an optimization issue with a goal function to be reduced, and learning moves toward the minimum of the goal function.","In predictability minimization, two networks have their outputs compared, with one network striving to make the outputs similar and the other attempting to make the outputs different. The output being looked at is a single scalar. In GANs, one network produces a rich, high dimensional vector which is fed into another network, and tries to pick an input that the other network is unable to process. The explanation of the learning algorithm is different. Predictability minimization is described as an optimization problem with an objective function to be decreased, and learning approaches the minimum of the objective function.","In predictability minimization, two networks have their outputs contrasted, with one network working to make the outputs alike and the other working to make the outputs distinct. The output in question is a single scalar value. In GANs, one network generates a complex, high dimension vector which is utilized as the input to another network, and attempts to select an input that the other network does not comprehend how to process. The characterization of the learning procedure is different. Predictability minimization is portrayed as an optimization issue with a goal function to be reduced, and learning moves toward the minimum of the goal function.",A,1
1466,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between). The incorporation of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the nonlinearity of the decision function without affecting the receptive fields of the conv. layers. Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectification function. It should be noted that 1×1 conv. layers have recently been utilised in the “Network in Network” architecture of Lin et al. (2014).","This can be viewed as enforcing a regularization on the 7 × 7 convolution filters, compelling them to have a decomposition through the 3 × 3 filters (with non-linearity added in between). The inclusion of 1 × 1 convolution layers (configuration C, Table 1) is a technique to boost the non-linearity of the decision function without changing the receptive fields of the convolution layers. Although in our case the 1 × 1 convolution is basically a linear projection onto the space of the same dimensionality (the input and output channel numbers are the same), extra non-linearity is introduced by the rectifier function. It should be noted that 1×1 convolution layers have recently been used in the ""Network in Network"" architecture of Lin et al. (2014).","This can be considered as imposing a constraint on the 7 × 7 convolutional filters, requiring them to have a factorization through the 3 × 3 filters (with nonlinearity inserted in between). The addition of 1 × 1 convolutional layers (configuration C, Table 1) is a way to increase the nonlinear nature of the decision function without modifying the receptive fields of the convolutional layers. Even though in our case the 1 × 1 convolution is fundamentally a linear projection onto the space of the same dimension (the input and output channel numbers are the same), supplementary nonlinearity is introduced by the rectifier function. It should be noted that 1×1 convolutional layers have recently been utilized in the ""Network in Network"" architecture of Lin et al. (2014).  ","This can be viewed as enforcing a structure on the 7 × 7 convolutional filters, necessitating them to have a decomposition via the 3 × 3 filters (with nonlinearity added between). The incorporation of 1 × 1 convolutional layers (configuration C, Table 1) is a technique to amplify the nonlinear nature of the decision function without altering the receptive fields of the convolutional layers. Despite the fact that in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimension (the input and output channel numbers are identical), extra nonlinearity is introduced by the rectifier function. It should be noted that 1×1 convolutional layers have recently been employed in the ""Network in Network"" architecture of Lin et al. (2014).",A,1
1330,Sentence Embeddings using Siamese BERT-Networks,"The similarity score is computed using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation r to make the results comparable to Misra et al. However, we showed (Reimers et al., 2016) that Pearson correlation has some serious drawbacks and should be avoided for comparing STS systems. The results are depicted in Table 3. Unsupervised methods like tf-idf, average GloVe embeddings or InferSent perform rather badly on this dataset with low scores.","The likeness rating is determined using cosine-similarity founded on the sentence representations. We also give the Pearson correlation r to make the outcomes comparable to Misra et al. Though, we demonstrated (Reimers et al., 2016) that Pearson correlation has some grave shortcomings and ought to be avoided for contrasting STS frameworks. The consequences are delineated in Table 3. Unsupervised techniques like tf-idf, normal GloVe embeddings or InferSent perform somewhat severely on this informational collection with low scores.","The similarity value is figured utilizing cosine-similarity dependent on the sentence implantings. We likewise give the Pearson relationship r to make the outcomes practically identical to Misra et al. In any case, we showed (Reimers et al., 2016) that Pearson relationship has some genuine disadvantages and ought to be kept away from for looking at STS frameworks. The outcomes are depicted in Table 3. Unsupervised strategies like tf-idf, normal GloVe embeddings or InferSent perform rather ineffectively on this dataset with low scores. ","The resemblance rating is ascertained using cosine-similarity founded on the sentence representations. We also make available the Pearson correlation r to make the results comparable to Misra et al. However, we exhibited (Reimers et al., 2016) that Pearson correlation has some serious shortcomings and should be avoided for comparing STS systems. The consequences are illustrated in Table 3. Unsupervised approaches like tf-idf, mean GloVe embeddings or InferSent perform quite poorly on this data set with low scores.",A,1
1093,GloVe_Global Vectors for Word Representation,"We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl . We tokenize and lowercase each corpus with the Stanford tokenizer, build a vocabulary of the 400,000 most frequent words , and then construct a matrix of cooccurrence counts X. In constructing X, we must choose how large the context window should be and whether to distinguish left context from right context.","We educated our system using 5 groups of texts of different sizes: a 2010 Wikipedia copy containing 1 billion words; a 2014 Wikipedia copy with 1.6 billion words; Gigaword 5 having 4.3 billion words; the mix of Gigaword5 + Wikipedia2014, containing 6 billion words; and 42 billion words of web data, from Common Crawl. We separate and convert to lowercase each group of texts using the Stanford tokenizer, build a vocabulary of the 400,000 most common words, and then make a matrix of co-occurrence counts X. In making X, we need to decide how big the context window should be and whether to differentiate left context from right context.","We trained our model using 5 collections of text of varying length: a 2010 Wikipedia extract with 1 billion terms; a 2014 Wikipedia extract with 1.6 billion terms; Gigaword 5 which has 4.3 billion terms; the blend of Gigaword5 + Wikipedia2014, containing 6 billion terms; and 42 billion terms of web content, from Common Crawl. We break down and change to lowercase each collection using the Stanford tokenizer, construct a lexicon of the 400,000 most frequent words, and then assemble a matrix of co-occurrence tallies X. In constructing X, we have to determine how large the context window should be and whether to separate left context from right context.","We developed our model utilizing 5 groups of texts of different sizes: a 2010 Wikipedia excerpt with 1 billion words; a 2014 Wikipedia excerpt with 1.6 billion words; Gigaword 5 containing 4.3 billion words; the fusion of Gigaword5 + Wikipedia2014, having 6 billion words; and 42 billion words of web material, from Common Crawl. We split and convert to lowercase each group using the Stanford tokenizer, create a vocabulary of the 400,000 most common words, and then generate a matrix of co-occurrence counts X. In creating X, we need to choose how big the context window should be and whether to differentiate left context from right context.",A,1
1385,Transformer-XL,"During evaluation, at each step, the vanilla model also consumes a segment of the same length as in training, but only makes one prediction at the last position. Then, at the next step, the segment is shifted to the right by only one position, and the new segment has to be processed all from scratch. As shown in Fig. 1b, this procedure ensures that each prediction utilizes the longest possible context exposed during training, and also relieves context fragmentation issue encountered in training. However, this evaluation procedure is extremely expensive. We will show that our proposed architecture is able to substantially improve the evaluation speed.","When assessing performance, the standard model ingests a section of identical size to that used during learning at each phase, but solely generates one forecast at the final spot. Subsequently, in the next phase, the section is moved to the right by just a single location, and the new section needs to be handled completely from the beginning. As depicted in Fig. 1b, this process guarantees that each prediction leverages the longest available context revealed during learning, and also alleviates context fragmentation difficulties experienced during training. However, this evaluation methodology is extremely costly. We will demonstrate that our suggested design can substantially enhance the evaluation velocity.","During testing, the vanilla model takes in a chunk of the same length as in training at each iteration, but only produces one prediction at the final position. At the next iteration, the chunk is shifted right by only one spot, and the new chunk must be processed from scratch. As shown in Fig. 1b, this approach ensures each prediction uses the longest context seen in training, and avoids context fragmentation issues faced in training. But this testing procedure is very expensive. We will show our proposed architecture can greatly improve the testing speed.","When measuring performance, the basic model consumes a segment of equal size to training at each step, but only generates one forecast at the final spot. Next, the segment moves right one place, and the new segment must be handled from the start. As in Fig. 1b, this verifies each prediction harnesses the maximum context from training, and prevents context fragmentation from training. However, this is very costly to evaluate. We will demonstrate our architecture can drastically improve evaluation pace.",A,1
969,"DistilBERT, a distilled version of BERT","However, as shown in the ablation study, leveraging the teacher’s knowledge with initialization and additional losses leads to substantial gains. Other compression techniques have been studied to compress large models. Recent developments in weights pruning reveal that it is possible to remove some heads in the self-attention at test time without significantly degrading the performance Michel et al. [2019]. Some layers can be reduced to one head.","Nevertheless, the ablation analysis demonstrated that taking advantage of the teacher's expertise through initialization and supplementary losses results in considerable improvements. Other methods of compression have been explored to shrink large models. Recent advancements in weights pruning indicate that removing certain heads in the self-attention during testing does not notably worsen performance, according to Michel et al. [2019]. Some layers can be decreased to a single head.","However, as exhibited in the ablation research, leveraging the teacher's knowledge through initialization and extra losses leads to major gains. Additional compression techniques have been analyzed to compress substantial models. Current developments in weights pruning show that it is feasible to eliminate some heads in the self-attention when testing without significantly diminishing the performance, as per Michel et al. [2019]. Certain layers can be reduced to one head. ","Though, as evidenced in the ablation study, taking advantage of the teacher's expertise via initialization and supplementary losses yields significant improvements. Other compression methods have been investigated to shrink large models. The latest advancements in weights pruning indicate that removing select heads in the self-attention during evaluation does not markedly degrade performance, as stated by Michel et al. [2019]. Specific layers can be decreased to a single head.",A,1
1368,Sequence to Sequence Learning with Neural Networks,"Similarly to this work, Cho et al. [5] used an LSTM-like RNN architecture to map sentences into vectors and back, although their primary focus was on integrating their neural network into an SMT system. Bahdanau et al. [2] also attempted direct translations with a neural network that used an attention mechanism to overcome the poor performance on long sentences experienced by Cho et al. [5] and achieved encouraging results. Likewise, Pouget-Abadie et al. [26] attempted to address the memory problem of Cho et al. [5] by translating pieces of the source sentence in way that produces smooth translations, which is similar to a phrase-based approach.","In the same way, Cho and colleagues used a recurrent neural network resembling an LSTM to encode sentences into vectors and decode them back, even though their main goal was to integrate their neural network into a statistical machine translation system. Bahdanau and others also tried direct translation with a neural network having an attention mechanism to get around the poor performance on long sentences seen by Cho et al. and got promising outcomes. Similarly, Pouget-Abadie et al. tried to address the memory issues of Cho et al. by translating chunks of the source sentence to generate fluid translations, akin to a phrase-based method.","Analogously, Cho and co-authors employed an LSTM-style recurrent neural network to map sentences to vectors and back again, despite their primary objective being incorporation of their neural network into a statistical MT system. Bahdanau and colleagues likewise attempted direct translation using a neural network with an attention mechanism to overcome the poor long sentence performance encountered by Cho et al., achieving encouraging results. Correspondingly, Pouget-Abadie et al. endeavored to address the memory problems of Cho et al. by translating segments of the source sentence in a manner producing smooth translations, comparable to a phrase-based approach.  ","In a comparable way, Cho and colleagues utilized a recurrent neural network similar to an LSTM for encoding sentences into vectors and decoding them, even though their principal focus was integrating their neural network into a statistical machine translation system. Bahdanau et al. also tried direct translation using a neural network with an attention mechanism to conquer the poor long sentence performance seen by Cho et al., attaining promising results. In a parallel fashion, Pouget-Abadie et al. sought to address the memory issues of Cho et al. by translating pieces of the source sentence in a way that generates fluid translations, analogous to a phrase-based method.",A,1
1113,Going deeper with convolutions,"After further tuning of learning rate, hyperparameters and improved training methodology, we established that the resulting Inception architecture was especially useful in the context of localization and object detection as the base network for [6] and [5]. Interestingly, while most of the original architectural choices have been questioned and tested thoroughly, they turned out to be at least locally optimal. One must be cautious though: although the proposed architecture has become a success for computer vision, it is still questionable whether its quality can be attributed to the guiding principles that have lead to its construction.","Following additional adjustments of the learning rate, hyperparameters, and enhanced training approaches, we determined that the resulting Inception model was particularly beneficial for localization and object detection as the foundation for [6] and [5]. Remarkably, even though most of the original architectural selections were extensively questioned and tested, they proved to be at least locally ideal. However, one should be careful: while the suggested architecture has become a triumph for computer vision, it is still uncertain whether its quality can be credited to the guiding principles that led to its development.","After more refinement of the learning rate, hyperparameters, and improved training techniques, we found that the resulting Inception structure was especially useful for localization and object detection as the base network for [6] and [5]. Interestingly, even though the majority of the original architectural choices were thoroughly challenged and evaluated, they turned out to be at least locally optimal. However, one must be cautious: despite the fact that the proposed architecture has become a success for computer vision, it is still debatable whether its quality can be attributed to the guiding tenets that led to its design.  ","Following further adjustment of the learning rate, hyperparameters, and enhanced training methodologies, we established that the resulting Inception model was particularly beneficial in the context of localization and object detection as the foundation for [6] and [5]. Remarkably, even though most of the original architectural selections were extensively questioned and tested, they proved to be at least locally ideal. However, one should be wary: while the suggested architecture has become a triumph for computer vision, it is still uncertain whether its quality can be credited to the guiding principles that resulted in its construction.",A,1
1536,"You Only Look Once_Unified, Real-Time Object Detection","YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict. Our model struggles with small objects that appear in groups, such as flocks of birds. Since our model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the input image.","The YOLO model severely limits where bounding boxes can be predicted because each part of the image grid can only guess two boxes and assign one class. This strict space limit reduces how many close objects our model can detect. Our model has trouble with small things in groups, like bird flocks. Since our model learns from data how to predict bounding box locations, it struggles to generalize to objects in new or odd proportions or arrangements. Our model also uses relatively rough features for bounding box prediction because our design downsamples the input image multiple times.","YOLO puts very tight constraints on where bounding boxes can be anticipated because each image grid section is restricted to predicting two boxes and one class. This tight spatial control decreases the quantity of nearby items our system can identify. Our system has difficulty with little objects clustered together, like bird swarms. Because our system learns from data how to predict bounding box positions, it has trouble generalizing to objects in unfamiliar or peculiar shapes or layouts. Our system also utilizes relatively coarse features for bounding box forecasting since our architecture downsamples the input image multiple times.","The YOLO architecture imposes rigid limitations on where bounding boxes can be expected since each image grid portion is limited to guessing two boxes and one class. This strict spatial regulation reduces the number of adjacent objects our model can detect. Our model struggles with small objects grouped together, such as flocks of birds. Because our model is trained on data to predict bounding box locations, it has difficulty generalizing to objects in new or odd proportions or configurations. Our model also employs relatively rough features for bounding box prediction because our design downsamples the input image multiple times.",A,1
1010,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"This removes roughly half of the WSC training set, but the DPR data set adds about 1,000 pronoun resolution examples. Examples from DPR are annotated with the correct referent noun, making it easy to use this data set in the format listed above. The WNLI training and validation sets have a significant overlap with the WSC training set. To avoid leaking validation examples into our training data (a particular issue in the multi-task experiments of Section 3.5.2), we therefore never train on WNLI and never report results on the WNLI validation set. ","This takes away around half of the WSC training set, however the DPR data set provides about 1,000 pronoun resolution instances. Samples from DPR have the accurate referent noun annotated, making it straightforward to utilize this data set in the format described above. There is considerable overlap between the training and validation sets of WNLI and WSC. To prevent validation examples from entering our training data (a specific problem in the multi-task experiments of Section 3.5.2), we thus do not train on WNLI and do not document results on the WNLI validation set.","This eliminates roughly 50% of the examples in the WSC training set, but the DPR data set contributes around 1,000 pronoun resolution cases. DPR examples have the right referring noun marked, so it's easy to use this data set in the format shown above. The WNLI training and validation sets share a lot of overlap with the WSC training set. To avoid validation instances getting into our training data (an issue especially in the multi-task experiments of Section 3.5.2), we therefore do not train on WNLI and do not report on the WNLI validation set.  ","This takes out about half of the training examples from WSC, however the DPR data set provides approximately 1,000 instances of pronoun resolution. DPR samples have the correct reference noun labeled, which makes it simple to utilize this data set in the format described above. There is significant overlap between the training and validation sets of WNLI and those of WSC. In order to prevent validation examples from entering our training data (a particular problem in the multi-task experiments of Section 3.5.2), we thus do not use WNLI for training and do not show results on the WNLI validation set.",A,1
1481,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets. We also compared the net B with a shallow net with five 5 × 5 conv. layers, which was derived from B by replacing each pair of 3 × 3 conv. layers with a single 5 × 5 conv. layer (which has the same receptive field as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters.","This shows that while adding more nonlinearity helps (C performs better than B), capturing spatial context through using convolution filters with meaningful receptive fields is also important (D outdoes C). The error rate of our model plateaus when it reaches 19 layers, but deeper models could prove beneficial given larger datasets. We also pitted net B against a shallow net with five 5x5 convolution layers, created by substituting each pair of 3x3 convolution layers from B with one 5x5 convolution layer (having the same receptive field as described in Section 2.3). The top-1 error of the shallow net was 7% higher than B's (on a center crop), confirming that a deep net with small filters is superior to a shallow net with larger filters.","This demonstrates that although incorporating extra nonlinearity is advantageous (C surpasses B), capturing spatial relationships by utilizing convolution filters with meaningful receptive fields is also crucial (D beats C). The error rate of our architecture maxes out at 19 layers, but deeper models may be useful for larger datasets. We also compared net B to a shallow net with five 5x5 convolution layers, formed by replacing each pair of 3x3 convolution layers in B with a single 5x5 convolution layer (having the same receptive field as explained in Section 2.3). The top-1 error of the shallow net was 7% higher than B's (on a center crop), showing that a deep net with small filters is better than a shallow net with bigger filters.","This shows that while adding more non-linearity is helpful (C is superior to B), capturing spatial context through convolution filters with meaningful receptive fields is also important (D is better than C). The error rate of our model peaks at 19 layers, but deeper models may prove beneficial for larger datasets. We also tested net B against a shallow net with five 5x5 convolution layers, created by swapping each pair of 3x3 convolution layers in B for a single 5x5 convolution layer (having the same receptive field as described in Section 2.3). The top-1 error of the shallow net was 7% higher than B's (on a center crop), demonstrating that a deep net with small filters surpasses a shallow net with larger filters.",A,1
1216,Language Models are Few-Shot Learners,"When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.","Since the test data is not public, our system is frequently too large to run on the test platform. Thus, we present findings on the dev set. We did upload to the test platform on a few datasets (SuperGLUE, TriviaQA, PiQa) where we managed to get submission working, and we only submit the 200B few-shot outputs, and show dev set outputs for all other cases.","When the evaluation data is private, our algorithm is often too big to execute on the evaluation server, so we document performance on the development set. We did manage to submit to the evaluation server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we got submission functional, and we only submit the 200B few-shot results, and document development set results for everything else.","Since the test data remains confidential, our system is regularly too large to fit on the test computer, therefore we report metrics on the dev set. We did upload to the test computer for a few datasets (SuperGLUE, TriviaQA, PiQa) where we got submission working, and we only submit the 200B few-shot outputs, and show dev set outputs for all other experiments.",A,1
1096,GloVe_Global Vectors for Word Representation,"We present results on the word analogy task in Table 2. The GloVe model performs significantly better than the other baselines, often with smaller vector sizes and smaller corpora. Our results using the word2vec tool are somewhat better than most of the previously published results. This is due to a number of factors, including our choice to use negative sampling (which typically works better than the hierarchical softmax), the number of negative samples, and the choice of the corpus. We demonstrate that the model can easily be trained on a large 42 billion token corpus, with a substantial corresponding performance boost. We note that increasing the corpus size does not guarantee improved results for other models, as can be seen by the decreased performance of the SVD.","The outcomes of the word analogy task are shown in Table 2. The GloVe model carries out significantly superior to the other baseline models, frequently with smaller vector dimensions and smaller text collections. Our outcomes utilizing the word2vec tool are somewhat enhanced compared to most of the previously released results. This is due to numerous factors, including our decision to employ negative sampling (which usually works better than the hierarchical softmax), the quantity of negative samples, and the choice of the corpus. We prove that the model can be easily trained on a large 42 billion token corpus, with a significant corresponding performance improvement. We observe that expanding the corpus size does not ensure enhanced outcomes for other models, as can be seen by the decreased performance of the SVD.","We display the findings on the word analogy assignment in Table 2. The GloVe model executes noticeably better than the other reference models, often with smaller vector sizes and smaller datasets. Our outputs using the word2vec apparatus are somewhat superior to most of the previously published outputs. This is owing to various elements, comprising our preference to utilize negative sampling (which characteristically executes superior to the hierarchical softmax), the amount of negative exemplars, and the choice of the dataset. We demonstrate that the model can be effortlessly educated on a substantial 42 billion token dataset, with a significant related performance boost. We take note that expanding the dataset size does not assure enhanced outputs for other models, as can be perceived by the decreased execution of the SVD.  ","The conclusions on the word analogy exercise are exhibited in Table 2. The GloVe model acts essentially better compared to the other foundational models, frequently with more modest vector sizes and more modest corpora. Our consequences utilizing the word2vec instrument are fairly improved contrasted with a large portion of the recently distributed results. This is because of various components, including our decision to utilize negative sampling (which typically performs better compared to the progressive softmax), the quantity of negative tests, and the decision of the corpus. We show that the model can be effectively prepared on an enormous 42 billion token corpus, with a huge relating execution support. We take note of that expanding the corpus size doesn't ensure improved outcomes for other models, as can be found in the diminished execution of the SVD.",A,1
1083,GloVe_Global Vectors for Word Representation,"Next, we note that the arguments of F in Eqn. (2) are vectors while the right-hand side is a scalar. While F could be taken to be a complicated function parameterized by, e.g., a neural network, doing so would obfuscate the linear structure we are trying to capture. Next, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles. Our final model should be invariant under this relabeling, but Eqn. (3) is not. However, the symmetry can be restored in two steps.","Furthermore, we observe that the inputs to F in Equation 2 are vectors whereas the right side is a scalar. Although F could be represented as a complex function like a neural network, that would conceal the linear structure we want to model. Also, for word-word co-occurrence matrices, the differentiation between a word and a context word is arbitrary and we can swap the two roles. Our final model should be unchanged under this relabeling, but Equation 3 does not have this property. However, we can restore the symmetry in two steps.","In addition, we see that the arguments of F in Formula 2 are vectors while the right hand side is a scalar. Even though F could be a complicated function like a neural network, that would hide the linear structure we want to capture. Moreover, for word-word co-occurrence matrices, the distinction between a word and a context word is random and we can switch the two positions. Our final model should be the same after this relabeling, but Formula 3 does not have this characteristic. Still, we can reinstate the symmetry in two steps.  ","Moreover, we find that the inputs to F in Expression 2 are vectors while the right hand side is a scalar. Despite F could be a complex function like a neural network, that would conceal the linear structure we aim to model. Furthermore, for word-word co-occurrence matrices, the differentiation between a word and a context word is arbitrary and we can exchange the two roles. Our final model should be unaltered under this relabeling, but Expression 3 does not possess this property. However, we can restore the symmetry in two steps.",A,1
1394,Transformer-XL,"Due to the similarity, we simply adapt the best model and the same hyper-parameters on enwik8 to text8 without further tuning. The comparison with previous methods is summarized in Table 3. Again, Transformer-XL achieves the new SoTA result with a clear margin. term dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4. Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8.","Because the datasets are so alike, we just take the optimal model and identical hyperparameters from enwik8 and apply them to text8 without any further adjustment. The contrast with prior approaches is outlined in Table 3. Yet again, Transformer-XL produces the new state-of-the-art result by a wide margin. This dataset primarily examines the capacity for modeling brief term relationships since the sentences have been jumbled. Therefore, this dataset mainly evaluates the ability to model only short-term connections. The comparison of Transformer-XL with the other techniques is displayed in Table 4. Although Transformer-XL is principally intended to better learn longer-term connections, it dramatically improves the single-model state-of-the-art from 23.7 to 21.8.","Since the data is so similar, we simply take the best performing model and identical settings from enwik8 and use them on text8 without any additional tuning. The juxtaposition with earlier methodologies is summarized in Table 3. Transformer-XL once again achieves the new best result by a substantial difference. This dataset mostly tests the skill for modeling brief term associations because the sentences have been shuffled around. Thus, this dataset primarily assesses the skill to model only immediate dependencies. The contrast of Transformer-XL with the other approaches is exhibited in Table 4. While Transformer-XL is primarily created to better learn longer-term dependencies, it dramatically boosts the single-model state-of-the-art from 23.7 to 21.8.  ","Because of the similarities, we just take the optimal model and identical hyperparameters from enwik8 and apply them to text8 without any extra tuning. The comparison with previous techniques is outlined in Table 3. Transformer-XL once again accomplishes the new best result by a wide margin. This dataset mainly evaluates the ability to model short-term connections since the sentences have been reordered. Therefore, this dataset primarily tests the skill to model only immediate dependencies. The contrast of Transformer-XL with the other methods is presented in Table 4. Although Transformer-XL is primarily built to better capture longer-term dependencies, it substantially improves the single-model state-of-the-art from 23.7 to 21.8.",A,1
1417,U-Net_Convolutional Networks for Biomedical Image Segmentation,The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation. We demonstrate the application of the u-net to three different segmentation tasks. The first task is the segmentation of neuronal structures in electron microscopic recordings. An example of the data set and our obtained segmentation is displayed in Figure 2. We provide the full result as Supplementary Material. The data set is provided by the EM segmentation challenge [14] that was started at ISBI 2012 and is still open for new contributions.,The movements are taken from a normal distribution with a standard deviation of 10 pixels. The displacements for each pixel are then found using bicubic interpolation. Randomly removing layers at the end of the contracting path adds more implicit data augmentation. We show how the u-net can be used for 3 different segmentation jobs. The first job is identifying neuronal structures in electron microscope images. An example of the data and our segmentation result is shown in Figure 2. The full result is in the Supplementary Material. The data is from the EM segmentation challenge [14] which began at ISBI 2012 and is ongoing for new contributions.,"The shifts are sampled from a Gaussian with 10 pixel standard deviation. The displacements of individual pixels are then computed using bicubic interpolation. Omitting layers randomly at the end of the contracting path provides additional implicit data augmentation. We demonstrate using the u-net for three distinct segmentation tasks. The first task segments neuronal structures in electron microscopy images. An example of the dataset and our segmentation is in Figure 2. The complete result is provided as Supplementary Material. The dataset is from the EM segmentation challenge [14] started at ISBI 2012, which remains open for new contributions.  ","The movements are drawn from a normal distribution with a standard deviation of 10 pixels. The displacements of each pixel are then determined using bicubic interpolation. Randomly dropping out layers at the end of the contracting path provides further implicit data augmentation. We exhibit the application of the u-net to three different segmentation jobs. The first job segments neuronal structures in electron microscopic scans. An instance of the data and our segmentation is shown in Figure 2. The full result is given as Supplementary Material. The data is from the EM segmentation challenge [14] initiated at ISBI 2012, which continues to accept new contributions.",A,1
996,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"However, we opted to create a new data set because prior data sets use a more limited set of filtering heuristics, are not publicly available, and/or are different in scope (e.g. are limited to News data (Zellers et al., 2019; Liu et al., 2019c), comprise only Creative Commons content (Habernal et al., 2016), or are focused on parallel training data for machine translation (Smith et al., 2013)). This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text. We dub this data set the “Colossal Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets.8 We consider the impact of using various alternative versions of this data set in Section 3.4. ","However, we decided to make a new data set because previous data sets use more limited filtering rules, are not publicly available, and/or have a different scope (for example, are restricted to News data (Zellers et al., 2019; Liu et al., 2019c), only contain Creative Commons content (Habernal et al., 2016), or focus on parallel training data for machine translation (Smith et al., 2013)). This results in a collection of text that is not just orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also contains reasonably clean and natural English text. We call this data set the ""Enormous Clean Crawled Collection"" (or C4 for short) and release it as part of TensorFlow Datasets. We look at the impact of using different versions of this data set in Section 3.4.","However, we elected to construct a novel data set since earlier data sets utilize a more constrained set of filtering heuristics, are not publicly accessible, and/or have a different scope (for instance, are limited to News data (Zellers et al., 2019; Liu et al., 2019c), comprise only Creative Commons content (Habernal et al., 2016), or focus on parallel training data for machine translation (Smith et al., 2013)). This yields a collection of text that is not just orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also contains reasonably clean and natural English text. We name this data set the ""Gigantic Clean Crawled Collection"" (or C4 for short) and release it as part of TensorFlow Datasets. We examine the impact of utilizing various alternative versions of this data set in Section 3.4.","However, we chose to generate a new data set since previous data sets employ a more constrained set of filtering rules, are not publicly available, and/or differ in scope (for example, are limited to News data (Zellers et al., 2019; Liu et al., 2019c), include only Creative Commons content (Habernal et al., 2016), or focus on parallel training data for machine translation (Smith et al., 2013)). This produces a collection of text that is not just orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text. We entitle this data set the ""Massive Clean Crawled Collection"" (or C4 for short) and release it as part of TensorFlow Datasets. We analyze the impact of applying various alternative versions of this data set in Section 3.4.",A,1
1413,U-Net_Convolutional Networks for Biomedical Image Segmentation,"At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes.","In every downsampling step, we multiply the quantity of feature channels by two. Each step in the expansive path is made up of an upsampling of the feature map followed by a 2x2 convolution (""up-convolution"") which halves the number of feature channels, a joining together with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each one succeeded by a ReLU. The cropping is required because of the loss of border pixels in every convolution. At the final layer, a 1x1 convolution is utilized to map each 64-component feature vector to the wanted number of classes.","At every stage of downsampling, we double the amount of feature channels. Every part of the expansive path consists of an upsampling of the feature map, followed by a 2x2 convolution (""up-convolution"") which reduces the feature channels by half, a fusion with the suitably trimmed feature map from the contracting path, and two 3x3 convolutions, each accompanied by a ReLU. The trimming is necessary owing to the loss of edge pixels in each convolution. In the final layer, a 1x1 convolution is used to transform each 64-element feature vector into the desired quantity of classes.  ","In each downsampling step, we increase the feature channels twofold. Each part of the expansive path involves upsampling the feature map, then applying a 2x2 convolution (""up-convolution"") to halve the feature channels, concatenating with the appropriately cropped feature map from the contracting path, followed by two 3x3 convolutions with a ReLU after each one. The cropping is needed because border pixels are lost in every convolution. In the final layer, a 1x1 convolution maps each 64-component feature vector to the target number of classes.",A,1
1026,Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"Nevertheless, our method achieves bounding-box regression by a different manner from previous feature-map-based methods [7, 5]. In [7, 5], bounding-box regression is performed on features pooled from arbitrarily sized regions, and the regression weights are shared by all region sizes. In our formulation, the features used for regression are of the same spatial size (n × n) on the feature maps. To account for varying sizes, a set of k bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the k regressors do not share weights. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale.","However, our approach accomplishes bounding-box regression in a different way than previous methods based on feature maps [7, 5]. In [7, 5], bounding-box regression is done on features pooled from regions of arbitrary sizes, and the regression weights are shared across all region sizes. In our formulation, the features used for regression have the same spatial dimensions (n × n) on the feature maps. To handle varying sizes, a set of k bounding-box regressors are learned. Each regressor is responsible for one scale and aspect ratio, and the k regressors do not share weights. Therefore, it is still feasible to predict boxes of different sizes even though the features have a fixed size/scale.","Nonetheless, our technique achieves bounding-box regression differently compared to prior feature-map-based approaches [7, 5]. In [7, 5], bounding-box regression is carried out on features aggregated from arbitrarily sized areas, and the regression weights are common across all area sizes. In our formulation, the features utilized for regression possess the same spatial size (n × n) on the feature maps. To account for varying dimensions, a set of k bounding-box regressors are learned. Each regressor is accountable for one scale and aspect ratio, and the k regressors do not share weights. As such, it is still viable to predict boxes of different sizes despite the features having a fixed size/scale.  ","However, our approach performs bounding-box regression in a distinct manner from previous feature-map-dependent techniques [7, 5]. In [7, 5], bounding-box regression is executed on features pooled from arbitrarily proportioned regions, and the regression coefficients are shared among all region proportions. In our formulation, the features leveraged for regression possess the same spatial dimensions (n × n) on the feature maps. To accommodate varying proportions, a set of k bounding-box regressors are learned. Each regressor is responsible for one scale and aspect ratio, and the k regressors do not share coefficients. Therefore, it is still feasible to predict boxes of varying sizes despite the features maintaining a fixed size/scale.",A,1
1476,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B–E). Second, we observe that the classification error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which contains three 1 × 1 conv. layers), performs worse than the configuration D, which uses 3 × 3 conv. layers throughout the network.","Initially, we see that applying local response normalization (A-LRN network) does not enhance the model A without any normalization layers. Therefore, we do not use normalization in the deeper architectures (B–E). Next, we notice that the classification error reduces as the ConvNet depth increases: from 11 layers in A to 19 layers in E. Significantly, despite having the same depth, the configuration C (which has three 1 × 1 conv. layers), performs worse than the configuration D, which utilizes 3 × 3 conv. layers throughout the network.","To start, implementing local response normalization (A-LRN network) does not improve the model A without any normalization layers. As a result, we do not use normalization in the deeper architectures (B–E). Additionally, we find that the classification error decreases as the ConvNet becomes deeper: from 11 layers in A to 19 layers in E. Importantly, even though they have the same depth, the configuration C (which has three 1 × 1 conv. layers), does worse than the configuration D, which uses 3 × 3 conv. layers in the whole network.  ","First off, applying local response normalization (A-LRN network) does not enhance the model A without any normalization layers. Therefore, we do not employ normalization in the deeper architectures (B–E). Secondly, we see that the classification error reduces as the ConvNet becomes deeper: from 11 layers in A to 19 layers in E. Significantly, despite having the same depth, the configuration C (which contains three 1 × 1 conv. layers), performs worse than the configuration D, which utilizes 3 × 3 conv. layers throughout the network.",A,1
963,"DistilBERT, a distilled version of BERT","As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a 4We use jiant [Wang et al., 2019] to compute the baseline.","The results in Table 2 demonstrate that DistilBERT achieves test accuracy on the IMDb benchmark that is only 0.6 percentage points lower than BERT, even though DistilBERT is 40% smaller in size. On the SQuAD dataset, DistilBERT's performance is within 3.9 points of the full BERT model. We also investigated if an additional distillation step during fine-tuning could improve DistilBERT's performance on SQuAD by using a BERT model previously fine-tuned on SQuAD as the teacher.","As shown by the findings in Table 2, DistilBERT comes within 0.6 percent of BERT's test accuracy on the IMDb benchmark while having 40% fewer parameters. On the SQuAD task, DistilBERT is within 3.9 points of the complete BERT model. We also explored whether adding another round of distillation while adapting DistilBERT on SQuAD, by fine-tuning it with a BERT model previously fine-tuned on SQuAD as the teacher, could further improve performance. ","The results presented in Table 2 indicate that DistilBERT achieves test accuracy on IMDb that is only 0.6 percentage points lower than BERT, even though DistilBERT's size is 40% smaller. On SQuAD, DistilBERT's performance is within 3.9 points of the full BERT model. We also tested whether an extra distillation step during fine-tuning on SQuAD, by training DistilBERT using a BERT model previously fine-tuned on SQuAD as the teacher, could further improve DistilBERT's performance.",A,1
1308,RoBERTa_A Robustly Optimized BERT Pretraining Approach,"In the second setting (ensembles, test), we submit RoBERTa to the GLUE leaderboard and achieve state-of-the-art results on 4 out of 9 tasks and the highest average score to date. This is especially exciting because RoBERTa does not depend on multi-task finetuning, unlike most of the other top submissions. We expect future work may further improve these results by incorporating more sophisticated multi-task finetuning procedures.","In the second experiment (groups, evaluation), we enter RoBERTa into the GLUE benchmark and accomplish the best outcomes so far on 4 out of 9 assignments and the highest average mark up to this point. This is particularly thrilling because RoBERTa does not rely on multi-task tuning, unlike most of the other top entries. We anticipate future work may additionally refine these outcomes by joining more complex multi-task tuning techniques.","In the second analysis (collections, assessment), we submit RoBERTa to the GLUE ranking and attain state-of-the-art performances on 4 out of 9 tasks and the top mean score thus far. This is especially exciting since RoBERTa is not dependent on multi-task fine-tuning, contrary to most of the other highest submissions. We expect future efforts may further enhance these results by integrating more sophisticated multi-task fine-tuning processes.  ","In the second trial (assemblies, appraisal), we enter RoBERTa into the GLUE leaderboard and achieve best-in-class results on 4 out of 9 assignments and the peak average mark to this point. This is particularly thrilling as RoBERTa does not hinge on multi-task tuning, unlike most of the other premier entries. We anticipate future work may additionally improve these outcomes by combining more complex multi-task tuning techniques.",A,1
1115,Going deeper with convolutions,The main idea of the Inception architecture is based on finding out how an optimal local sparse structure in a convolutional vision network can be approximated and covered by readily available dense components. Note that assuming translation invariance means that our network will be built from convolutional building blocks. All we need is to find the optimal local construction and to repeat it spatially. Arora et al. [2] suggests a layer-by layer construction in which one should analyze the correlation statistics of the last layer and cluster them into groups of units with high correlation. These clusters form the units of the next layer and are connected to the units in the previous layer.,The primary concept behind the Inception model depends on determining how to approximate and implement an optimal local sparse structure in a convolutional visual network using existing dense elements. This assumes that translation invariance enables constructing our network from convolutional components. We just need to identify the ideal local design and duplicate it spatially. Arora et al. [2] recommends a layer-wise development in which we examine the correlation data of the last layer and group highly correlated units. These collections constitute the units of the subsequent layer and connect to the previous layer's units.,The essential thought in the Inception architecture revolves around ascertaining how to estimate and reproduce an ideal localized sparse configuration in a convolutional visual system using available dense constituents. Presuming translation invariance means our network originates from convolutional building blocks. We only need to pinpoint the best local pattern and replicate it everywhere. Arora et al. [2] puts forth a layer-by-layer approach that analyzes the correlation statistics of the final layer and clusters units with high correlation. These clusters become the units of the next layer and connect to the prior layer's units.,The fundamental notion of the Inception model involves determining how to approximate and implement an optimal sparse local structure in a convolutional visual network using existing dense components. Assuming translation invariance means constructing our network from convolutional blocks. We just need to identify the optimal local design and duplicate it everywhere. Arora et al. [2] proposes a layer-by-layer technique that examines the correlation data of the final layer and groups highly correlated units. These groups become the units of the next layer and connect to the previous layer's units.,A,1
950,Deep Residual Learning for Image Recognition,"As a result, the image-centric training of Fast R-CNN [7] generates samples of small variations, which may not be desired for stochastic training. Motivated by this, in our current experiment we use the original RCNN [8] that is RoI-centric, in place of Fast R-CNN. Our R-CNN implementation is as follows. We apply the per-class RPN trained as above on the training images to predict bounding boxes for the ground truth class. These predicted boxes play a role of class-dependent proposals. For each training image, the highest scored 200 proposals are extracted as training samples to train an R-CNN classifier.","Consequently, the image-focused training process of Fast R-CNN [7] produces samples with minor differences, which may not be optimal for stochastic learning. Driven by this observation, in our present experiment we utilize the original RCNN [8] which is region of interest-focused, rather than Fast R-CNN. Our R-CNN implementation is as follows. We use the per-class RPN trained as described above on the training images to predict bounding boxes for the true class. These predicted boxes serve as class-specific proposals. For each training image, the top 200 highest scored proposals are extracted as training samples to train an R-CNN classifier.","For this reason, the image-oriented training of Fast R-CNN [7] generates examples with small variations, which might not be useful for random training. Prompted by this, in our current test we employ the original RCNN [8] which focuses on regions of interest, instead of Fast R-CNN. Our R-CNN implementation is like this. We apply the per-class RPN trained as mentioned above on the training images to forecast bounding boxes for the actual class. These predicted boxes act as class-specific suggestions. For every training image, the top 200 proposals with the highest scores are extracted as training samples to train an R-CNN classifier.  ","As a result, the image-driven training of Fast R-CNN [7] produces samples with minor differences, which may not be optimal for stochastic learning. Spurred by this finding, in our present analysis we utilize the original RCNN [8] which concentrates on regions of interest, rather than Fast R-CNN. Our R-CNN implementation is as follows. We use the per-class RPN trained as stated above on the training images to predict bounding boxes for the genuine class. These predicted boxes function as class-dependent proposals. For every training image, the 200 top-scoring proposals are extracted as training samples to train an R-CNN classifier.",A,1
1035,Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks,"For this purpose, we train a Fast R-CNN model by using the 2k SS proposals and ZF net. We fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time. In these ablation experiments, the RPN does not share features with the detector. Replacing SS with 300 RPN proposals at test-time leads to an mAP of 56.8%. The loss in mAP is because of the inconsistency between the training/testing proposals. This result serves as the baseline for the following comparisons. On the other extreme, using the top-ranked 6k RPN proposals (without NMS) has a comparable mAP (55.2%), suggesting NMS does not harm the detection mAP and may reduce false alarms.","To do this, we develop a Fast R-CNN system by utilizing the 2k SS proposals and ZF net. We keep this detector constant and measure the detection mAP by modifying the proposal areas used during testing. In these analysis tests, the RPN does not share features with the detector. Substituting SS with 300 RPN proposals at test time leads to an mAP of 56.8%. The mAP loss is due to the inconsistency between the training/testing proposals. This outcome provides the baseline for the following comparisons. On the other extreme, utilizing the top-ranked 6k RPN proposals (without NMS) has a comparable mAP (55.2%), implying NMS does not damage the detection mAP and may decrease false alarms.","For this goal, we build a Fast R-CNN system utilizing the 2k SS proposals and ZF net. We stabilize this detector and evaluate the detection mAP by altering the proposal regions utilized at test time. In these examination experiments, the RPN does not share features with the detector. Swapping SS with 300 RPN proposals at test time results in an mAP of 56.8%. The mAP loss is because of the inconsistency between the training/testing proposals. This outcome serves as the baseline for the subsequent comparisons. On the other extreme, employing the top-ranked 6k RPN proposals (without NMS) has a similar mAP (55.2%), indicating NMS does not impair the detection mAP and may reduce false alarms.  ","To accomplish this, we develop a Fast R-CNN model using the 2k SS proposals and ZF net. We fix this detector and assess the detection mAP by modifying the proposal areas used during testing. In these analysis experiments, the RPN does not share features with the detector. Exchanging SS with 300 RPN proposals at test time leads to an mAP of 56.8%. The mAP loss is due to the inconsistency between the training/testing proposals. This result acts as the baseline for the following comparisons. On the other extreme, utilizing the top-ranked 6k RPN proposals (without NMS) has a comparable mAP (55.2%), suggesting NMS does not damage the detection mAP and may decrease false alarms.",A,1
1463,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"The ConvNet configurations, evaluated in this paper, are outlined in Table 1, one per column. In the following we will refer to the nets by their names (A–E). All configurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512.","The ConvNet designs tested in this paper are shown in Table 1, with one design per column. We will refer to the networks by their names (A-E) going forward. All of the configurations follow the general architecture described in Section 2.1, and only vary in their depth, ranging from 11 weighted layers in network A (8 convolutional and 3 fully connected layers) to 19 weighted layers in network E (16 convolutional and 3 fully connected layers). The width of the convolutional layers (the number of channels) starts small at 64 in the first layer, then doubles after each max pooling layer until reaching 512.","The ConvNet models evaluated in this paper can be found in Table 1, with each column representing one model. We will use the names A through E when discussing these models. Every model uses the generic architecture from Section 2.1, only differing in how deep they are: Model A has 11 layers with weights (8 convolutional and 3 fully connected) while Model E has 19 weighted layers (16 convolutional and 3 fully connected). The width of the convolutional layers (number of channels) begins at 64 in the first layer, then doubles after each max pooling layer until reaching 512.  ","The designs of the ConvNets tested in this paper are shown in Table 1, one per column. We will refer to the networks using their names A-E moving forward. All of the designs follow the general design presented in Section 2.1, and only vary in their depth, ranging from 11 layers with weights in Network A (8 convolutional and 3 fully connected layers) to 19 weighted layers in Network E (16 convolutional and 3 fully connected layers). The width of the convolutional layers (number of channels) starts small with 64 channels in the first layer, then doubles after every max pooling layer until reaching 512 channels.",A,1
861,Deep contextualized word representations,"Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. 1 Introduction Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models. ","Our word vectors are derived from the inner workings of a deep two-way language model (biLM), which is pre-trained on a large collection of text. We demonstrate that these representations can be seamlessly incorporated into present models and substantially enhance the state-of-the-art across six tough NLP tasks, including question answering, textual entailment and sentiment analysis. We also provide an analysis exhibiting that revealing the deep internals of the pre-trained network is vital, enabling downstream models to mix various kinds of semi-supervision signals. 1 Introduction Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models.","Our word vectors are obtained from the internal mechanisms of a deep bidirectional language model (biLM), which is pre-conditioned on a substantial text corpus. We illustrate that these representations can be easily added to current models and meaningfully improve the state-of-the-art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an examination showing that exposing the deep internals of the pre-conditioned network is critical, permitting downstream models to combine different types of semi-supervision cues. 1 Introduction Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models.  ","Our word vectors are derived from the internal workings of a deep two-way language model (biLM), which is pre-trained on a large text dataset. We show that these representations can be seamlessly incorporated into existing models and substantially improve the state-of-the-art across six difficult NLP tasks, including question answering, textual entailment and sentiment analysis. We also provide an analysis demonstrating that revealing the deep internals of the pre-trained network is vital, allowing downstream models to integrate different types of semi-supervision signals. 1 Introduction Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models.",A,1
1220,Language Models are Few-Shot Learners,"LAMBADA is also a demonstration of the flexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word filters [RWC+19] (which ban “continuation” words). The few-shot setting instead allows us to “frame” the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired.","LAMBADA also shows the adaptability of quick learning as it gives a way to tackle an issue that typically happens with this data. Even though the fill-in-the-blank in LAMBADA is constantly the final word in a sentence, a normal language model has no means of knowing this fact. So it assigns likelihood not just to the right ending but also to other valid ways to continue the section. This problem has been somewhat solved before with stop-word filters [RWC+19] (which prohibit ""continuation"" words). The few-shot framework instead enables us to ""present"" the task as a cloze test and allows the language model to deduce from instances that a completion of precisely one word is wanted.","LAMBADA also demonstrates the flexibility of rapid learning as it provides a method to address a dilemma that traditionally occurs with this dataset. Despite the fact that the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of being aware of this detail. It thus assigns probability not only to the correct ending but also to other valid progressions of the paragraph. This issue has been partially tackled in the past with stop-word filters [RWC+19] (which ban ""continuation"" words). The few-shot setting instead allows us to ""frame"" the task as a fill-in-the-blank test and allows the language model to infer from examples that a completion of exactly one word is desired.  ","LAMBADA also shows the adaptability of quick learning as it gives a technique to handle a problem that typically happens with this dataset. Even with the completion in LAMBADA always being the final word in a sentence, a normal language model has no means of knowing this fact. So it assigns likelihood not just to the accurate ending but also to other valid continuations of the section. This issue has been partially addressed before with stop-word filters [RWC+19] (which prohibit ""continuation"" words). The few-shot framework instead enables us to ""present"" the task as a fill-in-the-blank test and allows the language model to deduce from examples that a completion of precisely one word is wanted.",A,1
1155,ImageNet A Large_Scale Hierarchical Image Database,"The idea is to not only consider the classification score at a node such as “dog”, but also of its child synsets, such as “German shepherd”, “English terrier”, etc. The maximum of all the classifier responses in this subtree becomes the classification score of the query image. Fig. 9 illustrates the result of our experiment on the mammal subtree. Note that our algorithm is agnostic to any method used to learn image classifiers for each synset. In this case, we use an AdaBoost-based classifier proposed by [6].","The concept is to take into account not just the categorization result at a node like ""dog"", but also of its more specific descendant concepts, such as ""German shepherd"", ""English terrier"", and so on. The highest of all the classifier outputs in this sub-hierarchy becomes the categorization result of the query image. Fig. 9 shows the consequence of our test on the mammal sub-hierarchy. Note that our algorithm does not depend on any specific technique used to train image classifiers for each concept. In this situation, we utilize an AdaBoost-based classifier proposed by [6].","The plan is to examine not solely the classification mark at a node such as ""dog"", but also of its more detailed child ideas, like ""German shepherd"", ""English terrier"", and so forth. The maximum of all the classifier reactions in this sub-tree becomes the classification mark of the query image. Fig. 9 illustrates the outcome of our experiment on the mammal sub-tree. Note that our algorithm does not rely on any particular method used to learn image classifiers for each idea. In this case, we use an AdaBoost-based classifier proposed by [6].  ","The intention is to evaluate not just the categorization score at a node such as ""dog"", but also of its more granular descendant notions, like ""German shepherd"", ""English terrier"", and so on. The highest of all the classifier outputs in this sub-tree becomes the categorization score of the query image. Fig. 9 shows the result of our trial on the mammal sub-tree. Note that our algorithm does not depend on any specific technique used to develop image classifiers for each notion. In this instance, we utilize an AdaBoost-based classifier proposed by [6].",A,1
891,Deep Residual Learning for Image Recognition,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity.","Training deeper neural networks poses more challenges. We put forth a residual learning system to simplify training networks much deeper than before. We explicitly reframe the layers as acquiring residual functions relative to the layer inputs, rather than acquiring unreferenced functions. We offer extensive empirical proof that these residual networks are easier to optimize, and can gain accuracy from much greater depth. On the ImageNet dataset we assess residual nets up to 152 layers deep—8 times deeper than VGG nets [41] but still less complex.","More layers make neural networks harder to train. We provide a residual learning approach to make training networks with far more layers easier. We directly redefine the layers to learn residual functions compared to the inputs, instead of learning functions without reference. We have comprehensive evidence that these residual networks are more optimizable, and gain precision from substantially increased depth. We test residual nets up to 152 layers on ImageNet—8 times deeper than VGG nets [41] but still less complicated.  ","Neural networks with more layers pose greater training difficulties. We put forward a residual learning framework to facilitate training networks much deeper than before. We explicitly reconfigure the layers to acquire residual functions relative to the inputs, rather than acquiring functions lacking reference. We supply extensive proof that these residual networks are more readily optimized, and obtain accuracy from much increased depth. We evaluate residual nets up to 152 layers deep on ImageNet—8 times deeper than VGG nets [41] but still less complex.",A,1
901,Deep Residual Learning for Image Recognition,"In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD. Both of them are powerful shallow representations for image retrieval and classification [4, 48]. For vector quantization, encoding residual vectors [17] is shown to be more effective than encoding original vectors. In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.","In image identification, VLAD [18] is a depiction that encodes using the remaining vectors compared to a dictionary, and Fisher Vector [30] can be explained as a probabilistic form [18] of VLAD. Both are influential superficial depictions for image retrieval and sorting [4, 48]. For vector quantification, encoding remaining vectors [17] is demonstrated to be more effective than encoding original vectors. In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the extensively utilized Multigrid approach [3] reformulates the system as subproblems at various scales, where each subproblem is accountable for the residual solution between a coarser and a finer scale.","In image recognition, VLAD [18] is a representation that encodes by the residual vectors in relation to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD. Both are powerful superficial representations for image retrieval and classification [4, 48]. For vector quantization, encoding residual vectors [17] is shown to be more effective than encoding original vectors. In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the commonly used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.","In image recognition, VLAD [18] is a representation that encodes utilizing the residual vectors compared to a dictionary, and Fisher Vector [30] can be explained as a probabilistic form [18] of VLAD. Both are influential shallow representations for image retrieval and categorization [4, 48]. For vector quantization, encoding residual vectors [17] is exhibited to be more effective than encoding original vectors. In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely utilized Multigrid approach [3] reformulates the system as subproblems at multiple scales, where each subproblem is accountable for the residual solution between a coarser and a finer scale.",A,1
1398,Transformer-XL,"As shown in Table 7, using segment-level recurrence substantially improves performance even when long-term dependency is not needed, which is consistent with our previous discussion that the recurrence mechanism resolves the context fragmentation problem. Moreover, our relative positional encodings is also superior to Shaw et al. (2018) on short sequences. ECL is the longest length to which increasing the context span would lead to a gain more than a threshold. However, ECL ignores the fact that it is harder to get improvement when a model already achieves a lower perplexity using only a shorter context, and thus it is not suitable for fair comparison among multiple models.","The data in Table 7 demonstrates that utilizing segment-level recurrence significantly enhances performance even without requiring long-term dependency, aligning with our prior analysis showing recurrence mechanisms address context fragmentation issues. Furthermore, our relative positional encodings outperform Shaw et al. (2018) on short sequences. ECL is the maximum length where expanding context span leads to gains surpassing a threshold. However, ECL does not account for the greater difficulty in gaining improvements when a model already reaches lower perplexity using only a shorter context, so it cannot fairly compare multiple models.","As exhibited in Table 7, leveraging segment-level recurrence markedly boosts performance even lacking necessity for long-range dependency, consistent with our earlier discussion that recurrence systems resolve context fragmentation problems. Additionally, our relative positional encodings surpass Shaw et al. (2018) on short sequences. ECL represents the furthest length where increasing context span would produce a gain higher than a set threshold. But ECL overlooks that getting improvement is harder when a model already achieves lower perplexity using just a shorter context, so it cannot equitably compare multiple models.  ","The information in Table 7 shows utilizing segment-level recurrence greatly enhances performance even without needing long-distance dependency, aligning with our prior analysis demonstrating recurrence mechanisms address context fragmentation issues. Furthermore, our relative positional encodings are superior to Shaw et al. (2018) on short sequences. ECL is the maximum length at which expanding context span leads to gains above a set threshold. However, ECL does not consider that getting improvement is more difficult when a model already reaches lower perplexity using only a shorter context, therefore it cannot fairly compare multiple models.",A,1
879,Deep contextualized word representations,"The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation. As a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely character input. In contrast, traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary. After training for 10 epochs on the 1B Word Benchmark (Chelba et al., 2014), the average forward and backward perplexities is 39.7, compared to 30.0 for the forward CNN-BIG-LSTM. ","The type representation that does not consider context uses 2048 character n-gram convolutional filters and then two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation. Therefore, the biLM gives three layers of representations for each input token, even those not in the training set because of the purely character input. However, traditional word embedding techniques only give one layer of representation for tokens in a fixed vocabulary. After training for 10 epochs on the 1B Word Benchmark (Chelba et al., 2014), the average forward and backward perplexities is 39.7, compared to 30.0 for the forward CNN-BIG-LSTM.","The context insensitive type representation utilizes 2048 character n-gram convolutional filters followed by two highway networks (Srivastava et al., 2015) and a linear projection to a 512 representation. Consequently, the biLM provides three layers of representations for each input token, including those not in the training set due to the purely character input. In contrast, conventional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary. After 10 epochs of training on the 1B Word Benchmark (Chelba et al., 2014), the average forward and backward perplexities are 39.7, compared to 30.0 for the forward CNN-BIG-LSTM.","The type representation that is oblivious to context employs 2048 character n-gram convolutional filters succeeded by two highway layers (Srivastava et al., 2015) and a linear projection to a 512 representation. Thus, the biLM furnishes three layers of representations for every input token, encompassing those outside the training set owing to the purely character input. However, old-fashioned word embedding techniques only furnish one layer of representation for tokens in a fixed vocabulary. Subsequent to training for 10 epochs on the 1B Word Benchmark (Chelba et al., 2014), the average forward and backward perplexities are 39.7, compared to 30.0 for the forward CNN-BIG-LSTM.",A,1
1282,RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods. Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data.","We conducted a replication analysis of BERT pretraining (Devlin et al., 2019). Our study thoroughly evaluated the impacts of hyperparameter tuning and training dataset size. We determined BERT was considerably undertrained. We suggest an enhanced procedure for training BERT models, which we call RoBERTa, that can equal or surpass the performance of all post-BERT approaches. Our modifications are straightforward. They involve: (1) training the model longer, with larger batches, on more data; (2) eliminating the next sentence prediction goal; (3) training on extended sequences; and (4) dynamically altering the masking pattern used on the training data.","We performed a reproduction analysis of BERT pretraining (Devlin et al., 2019). Our examination carefully assessed the effects of hyperparameter adjustment and size of the training set. We found BERT was significantly undertrained. We propose an improved method for developing BERT models, which we call RoBERTa, that can match or exceed the capabilities of all post-BERT techniques. Our changes are simple. They include: (1) training the model for longer, with larger batches, using more data; (2) removing the next sentence forecasting objective; (3) training on longer sequences; and (4) dynamically modifying the masking pattern applied to the training data.  ","We conducted a replication study of BERT pretraining (Devlin et al., 2019). Our evaluation thoroughly examined the influences of hyperparameter tuning and amount of training data. We determined BERT was substantially undertrained. We present an enhanced procedure for constructing BERT models, called RoBERTa, that can equal or surpass all post-BERT methods. Our modifications are straightforward. They consist of: (1) training the model longer, with bigger batches, on increased data; (2) eliminating the next sentence prediction goal; (3) training on extended sequences; and (4) dynamically changing the masking pattern used on the training data.",A,1
876,Deep contextualized word representations,"For some tasks (e.g., SNLI, SQuAD), we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing hk with [hk; ELMotask k ]. As the remainder of the supervised model remains unchanged, these additions can happen within the context of more complex neural models. For example, see the SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs, or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs. ","Certain assignments (like SNLI and SQuAD) show additional progress when we also utilize ELMo at the output of the task RNN, introducing another collection of output particular linear weights and swapping hk with [hk; ELMotask k]. Since the rest of the monitored model stays the same, these supplements can develop within more intricate neural models. For instance, observe the SNLI trials in Section 4 where a bi-attention layer comes after the biLSTMs, or the coreference resolution trials where a clustering model is built on top of the biLSTMs.","For some jobs (such as SNLI and SQuAD), we see extra enhancements by incorporating ELMo at the end result of the task RNN too, presenting a different set of output specific linear coefficients and substituting hk with [hk; ELMotask k]. Because the remainder of the regulated model persists unchanged, these additions can transpire inside more complex neural networks. See the SNLI tests in Section 4, where a bi-attention layer follows the biLSTMs, or the coreference resolution tests where a clustering model is layered above the biLSTMs, for example.","Certain tasks (like SNLI and SQuAD) display further improvements when we also make use of ELMo at the output of the task RNN, bringing in another set of output particular linear weights and replacing hk with [hk; ELMotask k]. Since the rest of the supervised model stays unchanged, these additions can occur within more sophisticated neural networks. For instance, observe the SNLI experiments in Section 4 where a bi-attention layer comes after the biLSTMs, or the coreference resolution experiments where a clustering model is built on top of the biLSTMs.",A,1
1180,ImageNet Classification with Deep Convolutional Neural Networks,"We employ two distinct forms of data augmentation, both of which allow transformed images to be produced from the original images with very little computation, so the transformed images do not need to be stored on disk. In our implementation, the transformed images are generated in Python code on the CPU while the GPU is training on the previous batch of images. So these data augmentation schemes are, in effect, computationally free. The first form of data augmentation consists of generating image translations and horizontal reflections.","We make use of two different types of data augmentation. Both allow transformed images to be made from the original images with very little computing, so the transformed images don't need to be kept on disk. In our system, the transformed images are created in Python code on the CPU while the GPU is learning on the previous batch of images. So these data augmentation plans are, essentially, computationally free. The first type of data augmentation includes generating image shifts and horizontal flips.","We utilize two unique forms of data augmentation. Both permit altered images to be produced from the original images with very minimal processing, so the altered images don't require storage on disk. In our setup, the altered images are generated in Python code on the CPU while the GPU is practicing on the prior set of images. Thus these data augmentation techniques are, in effect, computationally gratis. The first form of data augmentation consists of creating image translations and horizontal reversals.","We make use of two distinct modes of data augmentation. Both enable transformed images to be formed from the original images with very small computing, so the transformed images don't require saving on disk. In our implementation, the transformed images are created in Python code on the CPU while the GPU is learning on the previous group of images. So these data augmentation plans are, essentially, computationally free of charge. The first mode of data augmentation includes generating image shifts and horizontal flips.",A,1
1380,Transformer-XL,"Existing works range from ones where context representations are manually defined (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) to others that rely on document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017). More broadly, in generic sequence modeling, how to capture long-term dependency has been a long-standing research problem. From this perspective, since the ubiquitous adaption of LSTM, many efforts have been spent on relieving the vanishing gradient problem, including better initialization (Le et al., 2015), additional loss signal (Trinh et al., 2018), augmented memory structure (Ke et al., 2018) and others that modify the internal architecture of RNNs to ease the optimization (Wu et al., 2016; Li et al., 2018).","Existing works include ones where context representations are manually defined (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) and others that use document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017). More generally, in generic sequence modeling, how to maintain long-term dependency has been an ongoing research issue. Since the widespread adoption of LSTM, many efforts have focused on addressing the vanishing gradient problem, including better initialization (Le et al., 2015), additional loss signal (Trinh et al., 2018), augmented memory structure (Ke et al., 2018) and others that alter the internal architecture of RNNs to facilitate optimization (Wu et al., 2016; Li et al., 2018).","Current works range from those where context representations are hand-crafted (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) to others relying on document-level topics derived from data (Dieng et al., 2016; Wang et al., 2017). Broadly, in generic sequence modeling, capturing long-term dependency has been a persistent research challenge. Since LSTM became ubiquitous, many efforts have targeted the vanishing gradient issue, including superior initialization (Le et al., 2015), extra loss signal (Trinh et al., 2018), expanded memory structure (Ke et al., 2018) and others modifying the internal RNN architecture to ease optimization (Wu et al., 2016; Li et al., 2018).  ","Existing research includes studies where context representations are manually engineered (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) and those leveraging document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017). In general sequence modeling, modeling long-term dependencies has been a standing research problem. Since LSTM became prevalent, much work has focused on the vanishing gradient issue, including better initialization (Le et al., 2015), supplementary loss signal (Trinh et al., 2018), augmented memory architectures (Ke et al., 2018) and other RNN architecture modifications to facilitate optimization (Wu et al., 2016; Li et al., 2018).",A,1
1494,XLNet_Generalized Autoregressive Pretraining for Language Understanding,"Technically, to construct a valid target-aware prediction distribution, XLNet incorporates the target position into the hidden state via two-stream attention while previous permutation-based AR models relied on implicit position awareness inherent to their MLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that “orderless” does not mean that the input sequence can be randomly permuted but that the model allows for different factorization orders of the distribution. Another related idea is to perform autoregressive denoising in the context of text generation [11], which only considers a fixed order though.","Fundamentally, XLNet uses two-stream attention to include the target position in the hidden state to build a correct target-aware prediction distribution, while previous permutation-based AR models depended on the implicit position awareness built into their MLP architectures. Additionally, we want to stress that ""orderless"" does not imply the input sequence can be arbitrarily reordered for both orderless NADE and XLNet, but rather that the model permits different factorization orders of the distribution. A related concept is to do autoregressive denoising for text generation [11], which only looks at a fixed order however.","In essence, XLNet utilizes two-stream attention to incorporate the target position into the hidden state in order to construct a valid target-aware prediction distribution, whereas earlier permutation-based AR models leveraged the implicit position awareness inherent in their MLP designs. Moreover, we would like to emphasize that ""orderless"" does not mean the input sequence can be randomly shuffled for either orderless NADE or XLNet, but rather that the model allows for different factorization orders of the distribution. Another associated idea is to perform autoregressive denoising for text generation [11], which only considers a fixed order though.  ","At its core, XLNet uses two-stream attention to integrate the target position into the hidden state to build a sound target-aware prediction distribution, while previous permutation-based AR models capitalized on the implicit position awareness built into their MLP architectures. Furthermore, we want to stress that ""orderless"" does not mean the input sequence can be arbitrarily reordered for either orderless NADE or XLNet, but rather that the model accommodates different factorization orders of the distribution. A related notion is to conduct autoregressive denoising for text generation [11], which only examines a fixed order however.",A,1
1235,Language Models are Unsupervised Multitask Learners,"Most prior work trained language models on a single domain of text, such as news articles (Jozefowicz et al., 2016), Wikipedia (Merity et al., 2016), or fiction books (Kiros et al., 2015). Our approach motivates building as large and diverse a dataset as possible in order to collect natural language demonstrations of tasks in as varied of domains and contexts as possible. A promising source of diverse and nearly unlimited text is web scrapes such as Common Crawl. While these archives are many orders of magnitude larger than current language modeling datasets, they have significant data quality issues.","The majority of previous research focused on teaching language models using text from a single area, like news stories (Jozefowicz et al., 2016), Wikipedia (Merity et al., 2016), or fiction novels (Kiros et al., 2015). Our method suggests constructing as large and varied a dataset as feasible to gather natural language examples of tasks across many domains and settings. A promising source of diverse and nearly limitless text is web scrapes like Common Crawl. Although these archives are much bigger than current language modeling datasets, they have considerable data quality problems.","Most earlier work trained language models using text from one field, for instance news reports (Jozefowicz et al., 2016), Wikipedia (Merity et al., 2016), or fiction books (Kiros et al., 2015). Our approach advocates building as extensive and diverse a dataset as possible to collect natural language demonstrations of tasks across the widest variety of domains and contexts. A promising source of varied and almost unlimited text is web scrapes such as Common Crawl. While these archives are orders of magnitude larger than current language modeling datasets, they have significant data quality challenges.  ","The majority of past research trained language models using text from a single area, such as news articles (Jozefowicz et al., 2016), Wikipedia (Merity et al., 2016), or fiction books (Kiros et al., 2015). Our approach promotes constructing as large and varied a dataset as feasible to gather natural language examples of tasks in the broadest range of domains and settings possible. A promising source of diverse and nearly boundless text is web scrapes like Common Crawl. Although these archives are many times bigger than current language modeling datasets, they have considerable data quality issues.",A,1
1240,Language Models are Unsupervised Multitask Learners,"We observed a similar performance gap in our own attempts to train standard byte-level LMs on WebText. Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a practical middle ground between character and word level language modeling which effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences. Despite its name, reference BPE implementations often operate on Unicode code points and not byte sequences. These implementations would require including the full space of Unicode symbols in order to model all Unicode strings.","In our own tries to teach normal byte-level LMs on WebText, we saw a comparable gap in performance. Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a useful compromise between character and word level language modeling which essentially combines word level inputs for common symbol sequences and character level inputs for rare symbol sequences. Although it's called Byte Pair Encoding, reference implementations of BPE often work on Unicode code points rather than byte sequences. These implementations would need to include the complete set of Unicode symbols to be able to model all Unicode strings.","When we attempted to train standard byte-level language models on WebText, we noticed a similar underperformance. Byte Pair Encoding (BPE) (Sennrich et al., 2015) strikes a balance between character and word level language modeling, effectively fusing word level inputs for frequent symbol sequences with character level inputs for less common symbol sequences. Despite the name Byte Pair Encoding, existing BPE implementations typically operate on Unicode code points rather than bytes. To model all Unicode strings, these implementations would need to incorporate the full range of Unicode symbols.  ","In training conventional byte-level language models on WebText, we saw a comparable deficiency in performance. Byte Pair Encoding (BPE) (Sennrich et al., 2015) combines the best of character and word level language modeling, using word level inputs for common symbol sequences and character level inputs for rare symbol sequences. Though called Byte Pair Encoding, current BPE implementations work with Unicode code points, not bytes. To model all Unicode strings, these implementations would require the complete Unicode symbol set.",A,1
798,Attention is All You Need,"Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].","Our model does not use recurrence or convolution, so to enable the model to utilize the sequence order, we need to provide details about the relative or absolute position of the tokens in the sequence. For this purpose, we append ""positional encodings"" to the input embeddings at the base of the encoder and decoder stacks. The positional encodings have the same dmodel dimension as the embeddings, allowing the two to be summed. There are many possible positional encodings, both learned and fixed [9].","Since our model has neither recurrence nor convolution, we must incorporate information about the tokens' relative or absolute position in the sequence to let the model exploit the order. We accomplish this by adding ""positional encodings"" to the input embeddings at the foundation of the encoder and decoder stacks. The positional encodings have the same dmodel dimension as the embeddings, permitting the two to be added together. There are numerous options for positional encodings, both learned and fixed [9].","Our model lacks recurrence and convolution, so to enable the model to leverage sequence order, we inject details about the tokens' relative or absolute position in the sequence. We do this by appending ""positional encodings"" to the input embeddings at the base of the encoder and decoder stacks. The positional encodings have the same dmodel size as the embeddings, allowing summation of the two. Many positional encoding options exist, both learned and fixed [9].",A,1
830,Bag of Tricks for Efficient Text Classification,"Although deep neural networks have in theory much higher representational power than shallow models, it is not clear if simple text classification problems such as sentiment analysis are the right ones to evaluate them. We will publish our code so that the research community can easily build on top of our work. Acknowledgement. We thank Gabriel Synnaeve, Herv´e G´egou, Jason Weston and L´eon Bottou for their help and comments. We also thank Alexis Conneau, Duyu Tang and Zichao Zhang for providing us with information about their methods.","While deep neural networks theoretically have more representational capacity compared to shallow models, it is uncertain if elementary text classification tasks like sentiment analysis are suitable for assessing them. We will release our code so other researchers can conveniently extend our work. Thanks. We appreciate Gabriel Synnaeve, Herv ́e G ́egou, Jason Weston and L ́eon Bottou for their assistance and feedback. We also thank Alexis Conneau, Duyu Tang and Zichao Zhang for giving us details about their methods.","Although deep neural networks hypothetically have greater representational ability versus shallow models, it's unclear if basic text classification jobs like sentiment analysis are appropriate to evaluate them. We'll make our code available so the research community can easily build on our work. Gratitude. We are grateful to Gabriel Synnaeve, Herv ́e G ́egou, Jason Weston and L ́eon Bottou for their help and opinions. We also appreciate Alexis Conneau, Duyu Tang and Zichao Zhang for providing information regarding their methods.  ","While deep neural networks theoretically have superior representational power compared to shallow models, it is ambiguous whether simple text classification tasks such as sentiment analysis are suitable to assess them. We will publish our code so other researchers can conveniently extend our work. Thanks. We are thankful to Gabriel Synnaeve, Herv ́e G ́egou, Jason Weston and L ́eon Bottou for their assistance and feedback. We also appreciate Alexis Conneau, Duyu Tang and Zichao Zhang for giving us details about their approaches.",A,1
829,Bag of Tricks for Efficient Text Classification,"The speedup of the test phase is even more significant (a 600× speedup). Table 4 shows some qualitative examples. 4 Discussion and conclusion In this work, we propose a simple baseline method for text classification. Unlike unsupervisedly trained word vectors from word2vec, our word features can be averaged together to form good sentence representations. In several tasks, fastText obtains performance on par with recently proposed methods inspired by deep learning, while being much faster.","The acceleration of the evaluation period is even more noteworthy (a 600 times acceleration). The table displays some qualitative instances. In this paper, we suggest a straightforward foundational technique for categorizing text. In contrast to word vectors trained without supervision from word2vec, our word characteristics can be combined to form good sentence representations. In several tasks, fastText achieves performance comparable to recently developed methods motivated by deep learning, while being much quicker.","The speeding up of the testing phase is even more impressive (a 600 fold speedup). The table shows some qualitative samples. In this work, we present a simple baseline approach for text classification. Unlike word embeddings trained in an unsupervised way from word2vec, our word features can be averaged to form good sentence representations. On several tasks, fastText attains performance on par with recently proposed methods inspired by deep learning, while being much faster. ","The hastening of the trial period is even more striking (a 600 times hastening). The table displays some qualitative examples. In this paper, we put forth a straightforward foundational technique for sorting text. In contrast to word vectors trained without supervision from word2vec, our word traits can be combined to form good sentence depictions. On several tasks, fastText accomplishes performance comparable to recently devised methods motivated by deep learning, while being much swifter.",A,1
954,"DistilBERT, a distilled version of BERT","While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.","Though previous research examined using distillation for creating task-specific models, we apply knowledge distillation during pre-training and demonstrate that a BERT model can be reduced in size by 40% while keeping 97% of its language comprehension abilities and being 60% quicker. To take advantage of the inductive biases learned by larger models during pre-training, we present a triple loss joining together language modeling, distillation, and cosine-distance losses. Our smaller, faster, and lighter model costs less to pre-train and we exhibit its capabilities for on-device computations in a proof-of-concept test and a comparative on-device analysis.","Despite earlier work looking at utilizing distillation for constructing task-specific models, we leverage knowledge distillation during the pre-training phase and exhibit that it's feasible to decrease the size of a BERT model by 40%, while maintaining 97% of its language understanding capabilities and being 60% faster. To utilize the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation, and cosine-distance losses. Our smaller, quicker, and lighter model is less expensive to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.  ","While previous studies focused on using distillation for developing task-specific models, we apply knowledge distillation during pre-training and show it's possible to shrink a BERT model by 40%, while keeping 97% of its natural language comprehension abilities and being 60% faster. To take advantage of the inductive biases learned by bigger models during pre-training, we present a triple loss fusing language modeling, distillation, and cosine-distance losses. Our smaller, faster, and lighter model costs less to pre-train and we display its capabilities for on-device computations in a proof-of-concept test and a comparative on-device analysis.",A,1
1456,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,"With ConvNets becoming more of a commodity in the computer vision field, a number of attempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer. Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (Sermanet et al., 2014; Howard, 2014).","As ConvNets have become more commonplace in computer vision, there have been numerous efforts to enhance the original ConvNet architecture from Krizhevsky et al. (2012) to get better results. For example, the top-scoring models for ILSVRC2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014) used smaller receptive field size and stride for the first conv layer. Other advancements involved dense training and testing of networks across the full image and at multiple scales (Sermanet et al., 2014; Howard, 2014).","With convolutional neural networks turning into a standard tool in the computer vision domain, many tries have been made to improve on the original ConvNet design by Krizhevsky et al. (2012) to achieve superior accuracy. The best ILSVRC2013 submissions (Zeiler & Fergus, 2013; Sermanet et al., 2014) for instance used smaller receptive field dimensions and stride in the first conv layer. Other enhancements focused on dense training and testing of networks over the whole image and at multiple scales (Sermanet et al., 2014; Howard, 2014).  ","As convolutional neural networks have become more mainstream in computer vision, there have been numerous attempts to enhance the seminal ConvNet architecture from Krizhevsky et al. (2012) in hopes of obtaining better performance. For example, the top-scoring ILSVRC2013 entries (Zeiler & Fergus, 2013; Sermanet et al., 2014) employed smaller receptive field size and stride for the initial convolutional layer. Other improvements involved dense training and evaluation of networks across the entire image and across multiple scales (Sermanet et al., 2014; Howard, 2014).",A,1
1374,Transformer-XL,"Empirically, previous work has found that LSTM language models use 200 context words on average (Khandelwal et al., 2018), indicating room for further improvement. On the other hand, the direct connections between long-distance word pairs baked in attention mechanisms might ease optimization and enable the learning of long-term dependency (Bahdanau et al., 2014; Vaswani et al., 2017). Recently, Al-Rfou et al. (2018) designed a set of auxiliary losses to train deep Transformer networks for character-level language modeling, which outperform LSTMs by a large margin. Despite the success, the LM training in Al-Rfou et al. (2018) is performed on separated fixed-length segments of a few hundred characters, without any information flow across segments.","Past studies have discovered that LSTM language models typically utilize around 200 context words on average (Khandelwal et al., 2018), showing there is still room for enhancement. However, the direct links between distant word pairs inherent in attention mechanisms may facilitate optimization and learning of long-term dependencies (Bahdanau et al., 2014; Vaswani et al., 2017). More recently, Al-Rfou et al. (2018) created a set of auxiliary losses to train deep Transformer networks for character-level language modeling, substantially outperforming LSTMs. Though successful, the LM training in Al-Rfou et al. (2018) is done on fixed-length segments of a few hundred characters, without any cross-segment information flow.","Previous research has found LSTM language models employ about 200 context words on average (Khandelwal et al., 2018), indicating potential for further progress. Conversely, the direct connections between distant word pairs built into attention mechanisms may ease learning and enable capturing long-term dependencies (Bahdanau et al., 2014; Vaswani et al., 2017). Al-Rfou et al. (2018) recently designed auxiliary losses to train deep Transformer networks for character-level language modeling, substantially surpassing LSTMs. However, their LM training operates on separate fixed-length segments of several hundred characters, without cross-segment information transfer.  ","Studies have shown LSTM language models utilize around 200 context words on average (Khandelwal et al., 2018), suggesting room for improvement. In contrast, the direct links between far-apart word pairs inherent in attention mechanisms may facilitate learning and capturing long-term dependencies (Bahdanau et al., 2014; Vaswani et al., 2017). Al-Rfou et al. (2018) recently engineered auxiliary losses to train deep Transformer networks for character-level language modeling, greatly outperforming LSTMs. But their LM training works on isolated fixed-length segments of a few hundred characters, with no information flow between segments.",A,1
1162,ImageNet Classification with Deep Convolutional Neural Networks,"Current approaches to object recognition make essential use of machine learning methods. To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting. Until recently, datasets of labeled images were relatively small — on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations. For example, the current best error rate on the MNIST digit-recognition task (<0.3%) approaches human performance [4].","The way objects are currently recognized relies heavily on machine learning techniques. To make these techniques better, larger datasets can be collected, more powerful models can be learned, and better ways to prevent overfitting can be used. Until recently, datasets containing labeled images were fairly small - tens of thousands of images (like NORB, Caltech-101/256, and CIFAR-10/100). Basic recognition tasks can be handled well with datasets this size, particularly if label-preserving transformations are added. For instance, the lowest error rate on the MNIST digit recognition task (<0.3%) is close to human performance.","Modern object recognition utilizes machine learning extensively. Performance can be improved by gathering more data, developing stronger models, and using techniques to reduce overfitting. In the past, labeled image datasets were limited - only tens of thousands of images (NORB, Caltech-101/256, CIFAR-10/100). Even with small datasets, simple recognition tasks can be accomplished well, especially when label-preserving changes are incorporated. The current best error rate for MNIST digit recognition (<0.3%) is nearing human-level performance.  ","Current object recognition relies heavily on machine learning. Larger datasets, more powerful models, and better overfitting prevention can enhance performance. Until recently, labeled image datasets were small - tens of thousands of images (like NORB, Caltech-101/256, CIFAR-10/100). Even so, basic recognition tasks can be handled well, particularly when label-preserving transformations are used. For example, the lowest error rate for MNIST digit recognition (<0.3%) is approaching human performance.",A,1
965,"DistilBERT, a distilled version of BERT","We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization).","We investigated if DistilBERT was suitable for mobile apps by creating a question answering app for smartphones. We looked at the typical inference time on a current phone (iPhone 7 Plus) versus our past QA model using BERT-base. Leaving out tokenization, DistilBERT was 71% quicker than BERT, and the full model was 207 MB (which could be further reduced through quantization).","We studied whether DistilBERT could be utilized for apps on phones by making a question answering mobile app. We compared the mean inference speed on a new smartphone (iPhone 7 Plus) compared to our earlier trained QA model using BERT-base. Not including tokenization, DistilBERT was 71% faster than BERT, and the whole model was 207 MB (which could be additionally decreased with quantization).  ","We examined if DistilBERT was suitable for on-device apps by building a question answering mobile application. We benchmarked the average inference latency on a current smartphone (iPhone 7 Plus) against our previous QA model built on BERT-base. Omitting tokenization, DistilBERT was 71% faster than BERT, and the full model weighed 207 MB (which could be further shrunk with quantization).",A,1
1288,RoBERTa_A Robustly Optimized BERT Pretraining Approach,"The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) is a collection of 9 datasets for evaluating natural language understanding systems.6 Tasks are framed as either single-sentence classification or sentence-pair classification tasks. The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data.","The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) is a set of 9 data collections used to assess natural language processing systems. The tasks are structured as either classifying a single sentence or classifying a pair of sentences. The GLUE organizers give training and development data splits and also a submission server and leaderboard which lets participants evaluate and contrast their systems using private held-out test data that is not released publicly.","The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) consists of 9 datasets for testing natural language understanding systems. The tasks are designed as either categorizing one sentence or categorizing two sentences. The GLUE organizers provide splits of training data and development data, as well as a submission server and leaderboard that enables participants to measure and compare their systems using unseen test data that is kept private. ","The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) contains 9 collections of data for assessing natural language processing systems. The tasks are set up as classifying either a single sentence or a pair of sentences. The GLUE organizers make available splits of training data and development data, and also provide a submission server and leaderboard which allows participants to evaluate and benchmark their systems using held-out test data that is kept confidential.",A,1
1301,RoBERTa_A Robustly Optimized BERT Pretraining Approach,"For example, the recently proposed XLNet architecture (Yang et al., 2019) is pretrained using nearly 10 times more data than the original BERT (Devlin et al., 2019). It is also trained with a batch size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to BERT. To help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective), we begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K steps over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in Devlin et al. (2019).","As an illustration, the newly suggested XLNet design (Yang and colleagues, 2019) is pre-trained utilizing nearly 10 times additional information compared to the first BERT (Devlin and others, 2019). It is also educated with a batch dimension eight times bigger for half as numerous enhancement steps, consequently seeing four times as numerous sequences in pretraining contrasted with BERT. To assist disentangle the significance of these variables from other modeling decisions (for instance, the pretraining goal), we start by preparing RoBERTa following the BERTLARGE design (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K stages over a comparable BOOKCORPUS in addition to WIKIPEDIA dataset as was utilized in Devlin and others (2019).","For instance, the recently proposed XLNet model (Yang et al., 2019) is pre-trained using almost 10 times more data than the original BERT model (Devlin et al., 2019). It is also trained with a batch size eight times larger for half as many optimization iterations, thus seeing four times as many sequences during pretraining compared to BERT. To help isolate the importance of these factors from other modeling choices (such as the pretraining objective), we start by training RoBERTa using the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K iterations over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in Devlin et al. (2019).  ","As an example, the newly suggested XLNet neural network (Yang and coauthors, 2019) is pre-trained using close to 10 times more training data than the original BERT neural network (Devlin and colleagues, 2019). It is also trained with a batch size eight times larger for half as many training iterations, thus being exposed to four times as many sequences during pretraining compared to BERT. To help determine the importance of these factors apart from other modeling choices (such as the pretraining objective), we begin by training RoBERTa using the BERTLARGE neural network architecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K training iterations over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in Devlin et al. (2019).",A,1
964,"DistilBERT, a distilled version of BERT","To further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT.","To further examine the relationship between speed and model size for DistilBERT, we contrast in Table 3 the quantity of parameters for each model and the time required on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) to run inference on the entire STSB development set using a batch size of 1. DistilBERT has 40% less parameters than BERT and runs 60% faster than BERT.","To additionally investigate the speed/size compromise of DistilBERT, we compare in Table 3 the number of weights of each model along with the inference time necessary to fully process the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) utilizing a batch size of 1. DistilBERT possesses 40% fewer weights than BERT and executes 60% quicker than BERT. ","To further analyze the speed versus size tradeoff of DistilBERT, we juxtapose in Table 3 the parameter tally for each model plus the inference duration essential to complete a full cycle on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) applying a batch dimension of 1. DistilBERT contains 40% less parameters than BERT and operates 60% faster than BERT.",A,1
1315,Sentence Embeddings using Siamese BERT-Networks,"As we will show, this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings (Pennington et al., 2014). To alleviate this issue, we developed SBERT. The siamese network architecture enables that fixed-sized vectors for input sentences can be derived. Using a similarity measure like cosine similarity or Manhattan / Euclidean distance, semantically similar sentences can be found. These similarity measures can be performed extremely efficient on modern hardware, allowing SBERT to be used for semantic similarity search as well as for clustering.","Our analysis demonstrates that this widespread technique results in quite poor sentence vectors, frequently inferior to averaging GloVe vectors (Pennington et al., 2014). To address this problem, we created SBERT. The siamese architecture means fixed-length embeddings can be produced for input sentences. Using a similarity function such as cosine similarity or Manhattan/Euclidean distance, semantically related sentences can be identified. These similarities can be computed very efficiently on modern hardware, enabling SBERT to be utilized for semantic search and clustering.","As we show, this common practice leads to rather unsuitable sentence representations, often worse than taking the mean of GloVe embeddings (Pennington et al., 2014). To fix this issue, we built SBERT. The siamese network design means fixed-size vectors can be generated for input sentences. Utilizing a similarity metric like cosine similarity or Manhattan/Euclidean distance, semantically equivalent sentences can be detected. These similarities can be calculated extremely fast on current hardware, allowing SBERT to be leveraged for semantic search and clustering.","Our analysis proves that this prevalent approach results in quite poor sentence embeddings, often inferior to averaging GloVe embeddings (Pennington et al., 2014). To resolve this problem, we invented SBERT. The siamese architecture permits fixed-length vectors to be derived for input sentences. Employing a similarity function such as cosine similarity or Manhattan/Euclidean distance, semantically close sentences can be identified. These similarities can be performed very efficiently on modern hardware, enabling SBERT to be used for semantic similarity search and clustering.",A,1
1322,Sentence Embeddings using Siamese BERT-Networks,"Conneau et al. showed, that InferSent consistently outperforms unsupervised methods like SkipThought. Universal Sentence Encoder (Cer et al., 2018) trains a transformer network and augments unsupervised learning with training on SNLI. Hill et al. (2016) showed, that the task on which sentence embeddings are trained significantly impacts their quality. Previous work (Conneau et al., 2017; Cer et al., 2018) found that the SNLI datasets are suitable for training sentence embeddings.","Conneau and colleagues demonstrated that InferSent persistently surpasses unsupervised techniques such as SkipThought. The Universal Sentence Encoder (Cer and others, 2018) educates a transformer system and complements unsupervised learning with preparation on SNLI. Hill and colleagues (2016) exhibited that the undertaking on which sentence embeddings are prepared essentially affects their quality. Earlier work (Conneau and others, 2017; Cer and others, 2018) discovered that the SNLI informational collections are appropriate for preparing sentence embeddings.","Conneau and co-authors showed that InferSent consistently exceeds unsupervised approaches such as SkipThought. The Universal Sentence Encoder (Cer et al., 2018) develops a transformer framework and supplements unsupervised learning with preparation on SNLI. Hill et al. (2016) demonstrated that the assignment on which sentence embeddings are prepared essentially impacts their quality. Past work (Conneau et al., 2017; Cer et al., 2018) found that the SNLI datasets are reasonable for preparing sentence embeddings.","Conneau and colleagues exhibited that InferSent persistently beats unsupervised techniques like SkipThought. The Universal Sentence Encoder (Cer et al., 2018) trains a transformer network and augments unsupervised learning with training on SNLI. Hill and co-authors (2016) showed that the task on which sentence embeddings are trained significantly affects their quality. Prior work (Conneau et al., 2017; Cer et al., 2018) found that the SNLI data sets are suitable for training sentence embeddings.",A,1
1044,Generative Adversarial Nets,"Deep generative models have had less of an impact, due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear units in the generative context. We propose a new generative model estimation procedure that sidesteps these difficulties. 1 In the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution.","Advanced generative models have not made as much of an impact, because approximating the many challenging probabilistic calculations that come up with maximum likelihood estimation and related techniques is difficult. Also, it's hard to take advantage of the benefits of piecewise linear units in generative models. We suggest a new way to estimate generative models that avoids these problems. With adversarial networks, the generative model competes against a discriminator: a model that learns to tell whether a sample is from the model or the real data.","Sophisticated generative models have not had as much influence, since approximating numerous complex probabilistic computations from maximum likelihood estimation and related methods is challenging. Furthermore, capitalizing on the strengths of piecewise linear units in generative settings is difficult. We put forth a novel generative model estimation approach that circumvents these challenges. In adversarial networks, the generative model is opposed by a discriminator: a discriminative model that learns to discern whether a sample is from the model distribution or actual data distribution.","Advanced generative models have not made as big an impact, because it's difficult to approximate the many intractable probabilistic calculations from maximum likelihood estimation and related techniques. Also, it's hard to take full advantage of the benefits of piecewise linear units in generative contexts. We present a new way to estimate generative models that avoids these issues. With adversarial nets, the generative model competes against a discriminator: a discriminative model that learns to identify whether a sample comes from the model distribution or real data distribution.",A,1
1443,Universal Language Model Fine-tuning for Text Classification,"Similar to existing work (Peters et al., 2017, 2018), we are not limited to fine-tuning a unidirectional language model. For all our experiments, we pretrain both a forward and a backward LM. We fine-tune a classifier for each LM independently using BPT3C and average the classifier predictions. While our approach is equally applicable to sequence labeling tasks, we focus on text classification tasks in this work due to their important real world applications.","Like previous research (Peters et al., 2017, 2018), we are not constrained to fine-tuning a one-directional language model. For all of our experiments, we pre-train both a forward and a backward LM. We fine-tune a classifier for each LM separately using BPT3C and take the average of the classifier predictions. Although our approach can also be applied to sequence labeling tasks, we concentrate on text classification tasks in this work because of their significant real world uses.","Similar to existing studies (Peters et al., 2017, 2018), we are not limited to adapting a unidirectional language model. For all our tests, we pretrain both a forward and backward LM. We customize a classifier for each LM independently utilizing BPT3C and average the classifier forecasts. While our method is just as relevant to sequence labeling tasks, we focus on text classification tasks in this work due to their important practical applications. ","Like previous work (Peters et al., 2017, 2018), we are not constrained to fine-tuning a one-way language model. For all our experiments, we pre-train both a forward and backward LM. We adapt a classifier for each LM separately using BPT3C and take the mean of the classifier predictions. Although our approach can also apply to sequence labeling tasks, we concentrate on text classification tasks in this work because of their significant real world purposes.",A,1
1221,Language Models are Few-Shot Learners,"When presented with examples formatted this way, GPT-3 achieves 86.4% accuracy in the few-shot setting, an increase of over 18% from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost 20%, for GPT-3 it improves accuracy by 10%. Finally, the fill-in-blank method is not effective one-shot, where it always performs worse than the zero-shot setting. Perhaps this is because all models still require several examples to recognize the pattern.","After being shown instances arranged in this style, GPT-3 has 86.4% precision with minimal preparation, an enhancement of over 18% compared to the preceding best result. We see that capability with little groundwork rises markedly with model extent. Although this configuration drops the smallest model's performance by practically 20%, for GPT-3 it lifts accuracy by 10%. Lastly, the fill-in-the-blank approach is ineffective with one instance, where it always does worse than with no examples. This could be because all models still need multiple illustrations to identify the pattern.","When given examples in this format, GPT-3 reaches 86.4% accuracy with a small number of examples, an improvement of over 18% over the previous highest result. We find that performance with few examples increases substantially as model size grows. While this setting lowers the smallest model's performance by nearly 20%, for GPT-3 it boosts accuracy by 10%. Finally, the fill-in-the-blank method does not work well with just one example, where it always underperforms compared to having zero examples. This is likely because all models still need several examples to learn the pattern.","Presented with instances structured in this way, GPT-3 attains 86.4% precision with minimal preparation, a rise of over 18% from the earlier best. We discern that capability with scarce groundwork ascends markedly with model extent. Although this configuration diminishes the smallest model's performance by almost 20%, for GPT-3 it elevates accuracy by 10%. Lastly, the fill-in-the-blank technique is ineffective with a single case, where it always fares worse than with no precedents. This could be owing to all models still necessitating numerous precedents to ascertain the pattern.",A,1
802,Attention is All You Need,"As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.","The table indicates that a self-attention layer links all positions with a fixed number of sequential operations, while a recurrent layer needs O(n) sequential operations. Regarding computational intricacy, self-attention layers are quicker than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is frequently the case with sentence representations utilized by cutting-edge models in machine translations, like word-piece [38] and byte-pair [31] representations. To enhance computational efficiency for tasks with very long sequences, self-attention could be constrained to considering only a vicinity of size r in the input sequence focused around the particular output position. This would raise the maximum path length to O(n/r). We intend to further explore this method in future work.","As shown in the table, a self-attention layer connects all positions with a constant quantity of sequentially executed operations, while a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is less than the representation dimensionality d, which is often true for sentence representations used by state-of-the-art models in machine translation, like word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks with very long sequences, self-attention could be limited to considering only a neighborhood of size r in the input sequence around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.","As indicated in the table, a self-attention layer links all positions with a fixed number of sequentially executed operations, whereas a recurrent layer necessitates O(n) sequential operations. Regarding computational complexity, self-attention layers are quicker than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is frequently the case with sentence representations utilized by cutting-edge models in machine translation, such as word-piece [38] and byte-pair [31] representations. To boost computational efficiency for tasks involving very long sequences, self-attention could be constrained to considering only a vicinity of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We intend to explore this approach further in future work.",A,1
1305,RoBERTa_A Robustly Optimized BERT Pretraining Approach,"We explore a slightly wider hyperparameter space, described in the Appendix, and ensemble between 5 and 7 models per task. Recent submissions on the GLUE leaderboard adopt a pairwise ranking formulation for the QNLI task, in which candidate answers are mined from the training set and compared to one another, and a single (question, candidate) pair is classified as positive (Liu et al., 2019b,a; Yang et al., 2019). This formulation significantly simplifies the task, but is not directly comparable to BERT (Devlin et al., 2019). Following recent work, we adopt the ranking approach for our test submission, but for direct comparison with BERT we report development set results based on a pure classification approach.","We examine a somewhat broader hyperparameter space, outlined in the Appendix, and combine between 5 and 7 models per task. Recent entries on the GLUE leaderboard use a pairwise ranking formulation for the QNLI task, where possible answers are extracted from the training set and contrasted with one another, and a single (question, candidate) pair is categorized as positive (Liu et al., 2019b,a; Yang et al., 2019). This formulation greatly simplifies the task, but is not directly comparable to BERT (Devlin et al., 2019). Following recent work, we utilize the ranking approach for our test submission, but for direct comparison with BERT we present development set results based on a pure classification method.","We investigate a slightly larger hyperparameter space, described in the Appendix, and ensemble between 5 and 7 models for each task. Recent submissions on the GLUE leaderboard adopt a pairwise ranking formulation for the QNLI task, in which candidate answers are sourced from the training set and juxtaposed with one another, and a single (question, candidate) pair is labeled as positive (Liu et al., 2019b,a; Yang et al., 2019). This formulation significantly streamlines the task, but is not directly comparable to BERT (Devlin et al., 2019). Following recent work, we use the ranking approach for our test submission, but for direct comparison with BERT we report development set results based on a pure classification methodology.  ","We explore a marginally broader hyperparameter space, outlined in the Appendix, and combine between 5 and 7 models per task. Recent entries on the GLUE leaderboard utilize a pairwise ranking formulation for the QNLI task, where possible answers are extracted from the training set and compared against one another, and a single (question, candidate) pair is identified as positive (Liu et al., 2019b,a; Yang et al., 2019). This formulation greatly simplifies the task, but is not directly comparable to BERT (Devlin et al., 2019). Following recent work, we employ the ranking approach for our test submission, but for direct comparison with BERT we present development set results based on a pure classification technique.",A,1
1043,Generative Adversarial Nets,"The promise of deep learning is to discover rich, hierarchical models [2] that represent probability distributions over the kinds of data encountered in artificial intelligence applications, such as natural images, audio waveforms containing speech, and symbols in natural language corpora. So far, the most striking successes in deep learning have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label [14, 20]. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units [17, 8, 9] which have a particularly well-behaved gradient .","The potential of deep learning is to find complex, layered models [2] that characterize probability spreads over the types of information seen in artificial intelligence uses, like natural pictures, audio waveforms with speech, and symbols in collections of natural language. Up to this point, the most remarkable victories in deep learning have concerned discriminative models, usually those that map a high-dimensional, information-rich sensory input to a class tag [14, 20]. These remarkable victories have mostly depended on the backpropagation and dropout algorithms, utilizing piecewise linear units [17, 8, 9] which have an especially well-behaved slope.","The promise of deep learning is to uncover intricate, hierarchical models [2] that describe probability distributions over the kinds of information found in AI applications, including natural images, audio waveforms with vocalizations, and symbols in bodies of natural language. Thus far, the most striking triumphs in deep learning have been about discriminative models, often ones that map a high-dimensional, rich sensory input to a class label [14, 20]. These striking triumphs have largely relied on the backpropagation and dropout algorithms, employing piecewise linear units [17, 8, 9] which have a gradient that is particularly well-behaved.","The potential of deep learning is to develop complex, layered models [2] that characterize probability spreads over the types of data seen in artificial intelligence uses, such as natural images, audio waveforms containing speech, and symbols in collections of natural language. To this point, the most remarkable successes in deep learning have been about discriminative models, usually ones that map a high-dimensional, rich sensory input to a class tag [14, 20]. These remarkable successes have primarily depended on the backpropagation and dropout algorithms, using piecewise linear units [17, 8, 9] which have an especially well-behaved slope.",A,1
1123,Going deeper with convolutions,"Here, the most successful particular instance (named GoogLeNet) is described in Table 1 for demonstrational purposes. The exact same topology (trained with different sampling methods) was used for 6 out of the 7 models in our ensemble. All the convolutions, including those inside the Inception modules, use rectified linear activation. The size of the receptive field in our network is 224×224 taking RGB color channels with mean subtraction. “#3×3 reduce” and “#5×5 reduce” stands for the number of 1×1 filters in the reduction layer used before the 3×3 and 5×5 convolutions. One can see the number of 1×1 filters in the projection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers use rectified linear activation as well.","The most successful specific case (called GoogLeNet) is illustrated in Table 1 for example purposes. The very same design (trained with various sampling techniques) was utilized for 6 out of the 7 models in our collection. All the convolutions, including those inside the Inception modules, employ rectified linear activation. The size of the receptive field in our network is 224×224 taking RGB color channels with mean subtraction. “#3×3 reduce” and “#5×5 reduce” represents the number of 1×1 filters in the reduction layer used before the 3×3 and 5×5 convolutions. One can observe the number of 1×1 filters in the projection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers utilize rectified linear activation too.","The most effective individual instance (dubbed GoogLeNet) is shown in Table 1 as an example. The precise same architecture (trained with different sampling procedures) was employed for 6 out of the 7 models in our group. All the convolutions, including those inside the Inception modules, use rectified linear activation. The size of the receptive field in our network is 224×224 taking RGB color channels with mean subtraction. “#3×3 reduce” and “#5×5 reduce” denotes the number of 1×1 filters in the reduction layer used before the 3×3 and 5×5 convolutions. One can discern the number of 1×1 filters in the projection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers utilize rectified linear activation too.","The most triumphant specific case (termed GoogLeNet) is delineated in Table 1 for illustrative purposes. The very same design (trained with diverse sampling techniques) was employed for 6 out of the 7 models in our collection. All the convolutions, including those inside the Inception modules, utilize rectified linear activation. The size of the receptive field in our network is 224×224 taking RGB color channels with mean subtraction. “#3×3 reduce” and “#5×5 reduce” signifies the number of 1×1 filters in the reduction layer used before the 3×3 and 5×5 convolutions. One can discern the number of 1×1 filters in the projection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers use rectified linear activation too.",A,1
1142,ImageNet A Large_Scale Hierarchical Image Database,"ImageNet, however, demonstrates a much more balanced distribution of images across the semantic hierarchy. Another critical difference between ESP and ImageNet is sense disambiguation. When human players input the word “bank”, it is unclear whether it means “a river bank” or a “financial institution”. At this large scale, disambiguation becomes a nontrivial task. Without it, the accuracy and usefulness of the ESP data could be affected. ImageNet, on the other hand, does not have this problem by construction. See section 3.2 for more details. Lastly, most of the ESP dataset is not publicly available. Only 60K images and their labels can be accessed [1].","In contrast, ImageNet has a more even distribution of pictures across semantic categories. Another major difference between ESP and ImageNet is clarifying the meaning of words. When human participants enter ""bank"", it's ambiguous if it refers to ""a river bank"" or ""a financial institution"". At this large size, making this distinction becomes a challenging job. Without it, the precision and usefulness of the ESP information could suffer. ImageNet, however, does not have this issue since each image depicts a specific meaning. See section 3.2 for more details. Additionally, most of the ESP data set cannot be publicly accessed. Only 60K photos and their tags are available [1].","On the other hand, ImageNet has a much more balanced spread of images across the semantic taxonomy. A further critical distinction between ESP and ImageNet is word sense disambiguation. When human users input the word ""bank"", it's unclear if it means ""a river bank"" or ""a financial institution"". At this large scale, making this distinction becomes a difficult task. Without it, the accuracy and utility of the ESP information could be negatively impacted. ImageNet, however, does not have this issue since each image illustrates a specific meaning. Refer to section 3.2 for more information. Furthermore, most of the ESP dataset cannot be publicly accessed. Only 60K images and their labels are available [1].  ","In contrast, ImageNet exhibits a far more balanced distribution of images across semantic categories. Another key difference between ESP and ImageNet is resolving the meaning of ambiguous words. When human users enter ""bank"", it's unclear whether it refers to ""a river bank"" or ""a financial institution"". At this large scale, resolving this ambiguity becomes a challenging task. Without doing so, the precision and usefulness of the ESP data could suffer. ImageNet, however, does not have this issue since each image depicts a specific meaning. See section 3.2 for further details. Additionally, most of the ESP dataset is not publicly accessible. Only 60K images and their labels can be accessed [1].",A,1
1526,"You Only Look Once_Unified, Real-Time Object Detection","The (x, y) coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the whole image. Finally the confidence prediction represents the IOU between the predicted box and any ground truth box. We implement this model as a convolutional neural network and evaluate it on the PASCAL VOC detection dataset [9]. The initial convolutional layers of the network extract features from the image while the fully connected layers predict the output probabilities and coordinates.","The (x, y) values denote the middle of the box in relation to the edges of the grid unit. The width and height are anticipated proportional to the full image. Lastly the confidence forecast represents the IOU between the expected box and any actual box. We build this model as a convolutional neural system and assess it on the PASCAL VOC detection data set [9]. The early convolutional tiers of the system draw out features from the image while the completely linked tiers predict the production probabilities and locations.","The (x, y) points symbolize the center of the box with regards to the boundaries of the grid segment. The width and height are foreseen relative to the entire image. Finally, the confidence prediction characterizes the IOU between the expected box and any factual box. We construct this model as a convolutional neural network and evaluate it on the PASCAL VOC detection dataset [9]. The initial convolutional layers of the network derive features from the image while the fully connected layers anticipate the output probabilities and coordinates.  ","The (x, y) coordinates depict the middle of the box in context to the confines of the grid unit. The width and height are projected proportional to the whole image. Ultimately, the confidence forecast represents the IOU between the anticipated box and any real box. We build this model as a convolutional neural network and assess it on the PASCAL VOC detection dataset [9]. The early convolutional tiers of the network derive features from the image while the fully linked tiers predict the output probabilities and locations.",A,1
1227,Language Models are Unsupervised Multitask Learners,"Recent work reports modest performance improvements (Yogatama et al., 2019) and the two most ambitious efforts to date have trained on a total of 10 and 17 (dataset, objective) pairs respectively (McCann et al., 2018) (Bowman et al., 2018). From a meta-learning perspective, each (dataset, objective) pair is a single training example sampled from the distribution of datasets and objectives. Current ML systems need hundreds to thousands of examples to induce functions which generalize well. This suggests that multitask training many need just as many effective training pairs to realize its promise with current approaches. It will be very difficult to continue to scale the creation of datasets and the design of objectives to the degree that may be required to brute force our way there with current techniques.","The latest research shows small gains in performance (Yogatama et al., 2019) and the two most ambitious attempts so far have practiced on a sum of 10 and 17 (dataset, goal) pairs respectively (McCann et al., 2018) (Bowman et al., 2018). From a meta-learning view, each (dataset, goal) pair is a single practice case taken from the distribution of data and aims. Current ML systems need hundreds to thousands of examples to learn functions that generalize well. This implies that multitask training may need just as many effective training pairs to achieve its potential with current methods. It will be very tough to keep increasing the creation of data and the design of goals to the degree that may be required to get there by brute force with current techniques.","Recent studies demonstrate modest enhancements in results (Yogatama et al., 2019) and the two most ambitious tries thus far have practiced on a total of 10 and 17 (dataset, purpose) pairs respectively (McCann et al., 2018) (Bowman et al., 2018). From a meta-learning angle, each (dataset, purpose) pair is a single practice example sampled from the distribution of data and purposes. Current ML systems require hundreds to thousands of examples to induce functions that generalize well. This hints that multitask training may need just as many effective training pairs to realize its promise with current methods. It will be very difficult to continue scaling the creation of data and the design of purposes to the degree that may be required to get there through brute force with current techniques.","The latest work shows small gains in performance (Yogatama et al., 2019) and the two most ambitious attempts so far have trained on a total of 10 and 17 (dataset, goal) pairs respectively (McCann et al., 2018) (Bowman et al., 2018). From a meta-learning view, each (dataset, goal) pair is a single training case taken from the distribution of data and goals. Current ML systems need hundreds to thousands of examples to learn functions that generalize well. This implies that multitask training may need just as many effective training pairs to realize its potential with current approaches. It will be very tough to keep increasing the creation of data and the design of goals to the degree that may be required to get there through brute force with current techniques.",A,1
